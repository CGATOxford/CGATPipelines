##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Mike Morgan
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline GWAS
===========================

:Author: Mike Morgan
:Release: $Id$
:Date: |today|
:Tags: Python

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_gwas.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* gcta >= 1.25.0
* plink >= 1.9
* bolt-lmm >= 2.1
* smartpca >= 1.0
* goshifter >= 1.0
* gcta >= 1.25

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *
from ruffus.combinatorics import *

import sys
import os
import re
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh

# ---------------------------------------------------
# load the UKBiobank phenotype data into an SQLite DB
# use this as the main accessor of phenotype data
# for the report and non-genetic analyses


@follows(mkdir("phenotypes.dir"),
         mkdir("%s" % PARAMS['plots_dir']))
@transform("%s/%s*.tab" % (PARAMS['data_dir'],
                           PARAMS['data_prefix']),
           regex("%s/(.+).tab" % PARAMS['data_dir']),
           r"phenotypes.dir/\1.tsv")
def formatPhenotypeData(infiles, outfile):
    '''
    Use the UKBiobank encoding dictionary/R script to
    set the factor levels for phenotype data.
    Output is in plink covariate file format
    '''

    pheno_file = infiles

    statement = '''
    cgat pheno2pheno
    --task=plink_format
    --id-variable=%(data_id_var)s
    --log=%(outfile)s.log
    %(pheno_file)s
    > %(outfile)s
    '''

    P.run()


@follows(formatPhenotypeData)
@transform(formatPhenotypeData,
           suffix(".tsv"),
           ".load")
def loadPhenotypes(infile, outfile):
    '''
    load all phenotype data in to an SQLite DB
    '''

    P.load(infile, outfile)


@follows(loadPhenotypes)
@transform(formatPhenotypeData,
           regex("phenotypes.dir/(.+).tsv"),
           r"phenotypes.dir/\1_British.tsv")
def selectBritish(infile, outfile):
    '''
    Select only those individuals with a white British
    ethnicity
    '''

    statement = '''
    cgat pheno2pheno
    --task=select_ethnicity
    --ethnicity-id=%(format_ethnicity_var)s
    --ethnicity-label=%(format_ethnicity)s
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(selectBritish)
@transform(selectBritish,
           suffix("_British.tsv"),
           ".keep")
def makeKeepFile(infile, outfile):
    '''
    make a samples.keep file for filtering
    on individuals
    '''

    statement = '''
    cat %(infile)s | awk '{if(NR > 1) {printf("%%s\\t%%s\\n", $1, $2)}}'
    > %(outfile)s
    '''

    P.run()


@follows(loadPhenotypes,
         selectBritish)
@transform(selectBritish,
           regex("phenotypes.dir/(.+)_British.tsv"),
           r"phenotypes.dir/\1.pheno")
def dichotimisePhenotype(infile, outfile):
    '''
    Dichotomise a phenotype for association testing
    '''

    statement = '''
    cgat pheno2pheno
    --task=dichotimise_phenotype
    --pheno-id=%(data_dichot_var)s
    --reference-variable=%(data_reference_value)s
    --missing-var-label=%(data_missing_label)s
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(loadPhenotypes,
         mkdir("plots.dir"))
@transform(formatPhenotypeData,
           regex("phenotypes.dir/(.+).tsv"),
           r"plots.dir/1\_phenotype.png")
def plotPhenotypeData(infile, outfile):
    '''
    Generare plots of phenotype distributions
    for CGATReport document
    '''

    pass


@follows(loadPhenotypes,
         plotPhenotypeData)
@transform(formatPhenotypeData,
           regex("phenotypes.dir/(.+).tsv"),
           r"plots.dir/\1_map_%s.png" % PARAMS['phenotype_map_overlay'])
def plotPhenotypeMap(infile, outfile):
    '''
    Plot an overlay of a phenotype by geographical distribution
    onto a map of the UK
    '''

    job_memory = "4G"

    statement = '''
    cgat pheno2plot
    --plot-type=map
    -x %(phenotype_map_overlay)s
    --coordinate-file=%(phenotype_coord_file)s
    --coords-id-col=%(phenotype_id_coords)s
    --lattitude-column=%(phenotype_lat)s
    --longitude-column=%(phenotype_long)s
    --xvar-labels=%(phenotype_xlabels)s
    --reference-value=%(phenotype_ref_value)s
    --var-type=categorical
    --log=%(outfile)s.log
    --output-file=%(outfile)s
    %(infile)s
    '''

    P.run()

# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Specific pipeline tasks
# need to allow for extra characters after chromsome ID


@follows(mkdir("plink.dir"),
         dichotimisePhenotype)
@collate("%s/*.%s" % (PARAMS['data_dir'],
                      PARAMS['data_suffix']),
         regex("%s/(.+)(\d+)(.+)\.%s" % (PARAMS['data_dir'],
                                         PARAMS['data_suffix'])),
         add_inputs(r"%s/\1\2\3.%s" % (PARAMS['data_dir'],
                                       PARAMS['data_aux'])),
         r"plink.dir/\1\2.bed")
def convertToPlink(infiles, outfiles):
    '''
    Convert from other format
    to Plink binary format.  One bed file
    per chromosome - keep the fam files the same
    '''

    job_memory = "60G"
    infiles = ",".join([x for x in infiles[0]])

    log_out = ".".join(outfiles.split(".")[:-1])
    out_pattern = ".".join(outfiles.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=%(data_format)s
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --update-sample-attribute=gender
    --format-parameter=%(format_gender)s
    --method=format
    --memory="60G"
    --format-method=change_format
    --reformat-type=plink_binary
    --output-file-pattern=%(out_pattern)s
    --log=%(log_out)s.log
    %(infiles)s
    '''

    P.run()

# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# some variants have missing ID names - change these with the following
# structure:
# chr_bp_A1_A2


@follows(convertToPlink)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim"]),
           r"plink.dir/\1.exclude")
def nameVariants(infiles, outfile):
    '''
    Some variants have missing file convert these to
    have ID structure: chr_bp_A1_A2 instead - update the
    relevant bim file and generate a list of variants
    to exclude - triallelic, duplicates and overlapping
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    # temporary file name
    temp_file = P.getTempFilename(shared=True)

    job_memory = "2G"

    # use awk on the .bim file to generate replacement IDs

    state0 = '''
    cat %(bim_file)s | awk '{if($2 == ".") {printf("%%s\\t%%s_%%s_%%s_%%s\\t%%s\\t%%s\\t%%s\\t%%s\\n",
    $1,$1,$4,$5,$6,$3,$4,$5,$6)} else{print $0}}' > %(temp_file)s.bim;
    mv %(temp_file)s.bim %(bim_file)s
    '''

    # create files to remove triallelic variants, overlapping variants
    # and duplicates
    state1 = '''
    cgat geno2geno
    --task=detect_duplicates
    --outfile-pattern=%(temp_file)s
    --log=%(outfile)s.log
    %(bim_file)s;
    cat %(temp_file)s.triallelic %(temp_file)s.duplicates
    %(temp_file)s.overlapping | sort | uniq >> %(outfile)s
    '''

    state2 = '''
    touch %(outfile)s
    '''

    statement = ";".join([state0, state1, state2])
    P.run()

# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# QC tasks on genotype and samples

# genotype filtering tasks:
# snp genotyping rate
# individual missingness
# individual heterozygosity
# individual relatedness - IBD
# gender check - reported vs genetic gender
# PCA on set of LD pruned SNPs - compare to self-reported ethnicity
# perform each task independently and create exclusion lists. Remove
# all individuals and SNPs at the end.

# GRM will give relatedness and inbreeding at the same time

# First step is to produce a complete LD pruned list of SNPs
# this is slow for lots of SNPs and samples!


@follows(mkdir("QC.dir"),
         nameVariants)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude"]),
           r"QC.dir/\1_round1.prune.in")
def ldPruneSNPsRound1(infiles, outfile):
    '''
    LD prune SNPs to create a list of independent SNPs
    genome-wide.  To be used for PCA, GRM, inbreeding,
    and heterozygosity estimation
    '''

    # this needs to be performed multiple times to get
    # a small enough set of SNPS ~ 10k-50k

    # this selects entirely INDELS if not MAF cut-off
    # is used.  Evidently LD is low between indels,
    # but not between SNPs and indels.  How many
    # INDELs are on the Affy array, and how many
    # are imputed?   What is their imputation accuracy?
    # this is causing relatedness to be massively
    # overestimated!!
    # devel version allows --keep and --remove
    # use plinkdev for development version

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    exclude_file = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "6G"
    job_threads = 10

    statement = '''
    cgat geno2qc
    --program=plinkdev
    --input-file-format=plink_binary
    --exclude-snps=%(exclude_file)s
    --method=ld_prune
    --keep=%(gwas_keep)s
    --use-kb
    --memory=60G
    --ignore-indels
    --genotype-rate=0.1
    --hardy-weinberg=1e-5
    --min-allele-frequency=0.01
    --prune-method=%(ld_prune_method)s
    --step-size=%(ld_prune_step)s
    --window-size=%(ld_prune_window)s
    --threshold=%(ld_prune_threshold)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mkdir("QC.dir"),
         nameVariants,
         ldPruneSNPsRound1)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude",
                       r"QC.dir/\1_round1.prune.in"]),
           r"QC.dir/\1_round2.prune.in")
def ldPruneSNPsRound2(infiles, outfile):
    '''
    LD prune SNPs to create a list of independent SNPs
    genome-wide.  To be used for PCA, GRM, inbreeding,
    and heterozygosity estimation
    '''

    # this needs to be performed multiple times to get
    # a small enough set of SNPS ~ 10k-50k

    # this selects entirely INDELS if not MAF cut-off
    # is used.  Evidently LD is low between indels,
    # but not between SNPs and indels.  How many
    # INDELs are on the Affy array, and how many
    # are imputed?   What is their imputation accuracy?
    # this is causing relatedness to be massively
    # overestimated!!
    # devel version allows --keep and --remove
    # use plinkdev for development version

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    exclude_file = infiles[1][2]
    include_file = infiles[1][3]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "6G"
    job_threads = 10
    prune_threshold = PARAMS['ld_prune_threshold'] - 0.10

    statement = '''
    cgat geno2qc
    --program=plinkdev
    --input-file-format=plink_binary
    --exclude-snps=%(exclude_file)s
    --extract-snps=%(include_file)s
    --method=ld_prune
    --keep=%(gwas_keep)s
    --use-kb
    --memory=60G
    --ignore-indels
    --min-allele-frequency=0.01
    --genotype-rate=0.1
    --hardy-weinberg=1e-5
    --prune-method=%(ld_prune_method)s
    --step-size=%(ld_prune_step)s
    --window-size=%(ld_prune_window)s
    --threshold=%(prune_threshold)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mkdir("QC.dir"),
         nameVariants,
         ldPruneSNPsRound2)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude",
                       r"QC.dir/\1_round2.prune.in"]),
           r"QC.dir/\1.prune.in")
def ldPruneSNPsRound3(infiles, outfile):
    '''
    LD prune SNPs to create a list of independent SNPs
    genome-wide.  To be used for PCA, GRM, inbreeding,
    and heterozygosity estimation
    '''

    # this needs to be performed multiple times to get
    # a small enough set of SNPS ~ 10k-50k

    # this selects entirely INDELS if not MAF cut-off
    # is used.  Evidently LD is low between indels,
    # but not between SNPs and indels.  How many
    # INDELs are on the Affy array, and how many
    # are imputed?   What is their imputation accuracy?
    # this is causing relatedness to be massively
    # overestimated!!
    # devel version allows --keep and --remove
    # use plinkdev for development version

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    exclude_file = infiles[1][2]
    include_file = infiles[1][3]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "6G"
    job_threads = 10
    prune_threshold = PARAMS['ld_prune_threshold'] - 0.18

    statement = '''
    cgat geno2qc
    --program=plinkdev
    --input-file-format=plink_binary
    --exclude-snps=%(exclude_file)s
    --extract-snps=%(include_file)s
    --method=ld_prune
    --keep=%(gwas_keep)s
    --use-kb
    --memory=60G
    --ignore-indels
    --min-allele-frequency=0.01
    --genotype-rate=0.1
    --hardy-weinberg=1e-5
    --prune-method=%(ld_prune_method)s
    --step-size=%(ld_prune_step)s
    --window-size=%(ld_prune_window)s
    --threshold=%(prune_threshold)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(ldPruneSNPsRound3,
         mkdir("genome.dir"))
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"QC.dir/\1.prune.in"]),
           r"genome.dir/\1_sparse.bed")
def makeTrimmedData(infiles, outfile):
    '''
    Filter on the LD pruned set of SNPs to
    create smaller, sparser genotype files
    Remove duplicates first
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    snps_file = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    outpattern = ".".join(outfile.split(".")[:-1])
    job_memory = "60G"
    job_threads = 1

    tmpfile = P.getTempFilename(shared=True)

    # find duplicates
    state1 = '''
    cgat geno2geno
    --log=%(outfile)s.exclude.log
    --task=detect_duplicates
    --outfile-pattern=%(tmpfile)s
    %(bim_file)s
    '''

    exclude_file = tmpfile + ".exclude"

    state2 = '''
    cat %(tmpfile)s.triallelic %(tmpfile)s.duplicates
    %(tmpfile)s.overlapping | sort | uniq >> %(exclude_file)s
    '''

    state3 = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=format
    --format-method=change_format
    --reformat-type=plink_binary
    --extract-snps=%(snps_file)s
    --exclude-snps=%(exclude_file)s
    --memory=%(job_memory)s
    --log=%(outfile)s.log
    --output-file-pattern=%(outpattern)s
    --threads=%(job_threads)i
    %(plink_files)s
    '''

    statement = ";".join([state1, state2, state3])

    P.run()


@follows(makeTrimmedData)
@collate(makeTrimmedData,
         regex("genome.dir/(.+)_sparse.bed"),
         add_inputs([r"genome.dir/\1_sparse.fam",
                     r"genome.dir/\1_sparse.bim"]),
         r"genome.dir/WholeGenome.bed")
def mergePlinkFiles(infiles, outfile):
    '''
    Merge all of the LD pruned files together
    to form a set of LD-independent genome-wide
    genotyping files for downstream sample QC
    '''

    # generate text file that contains the names of the files
    # to be merged
    temp_file = P.getTempFilename(shared=True)

    # not all multi-allelic SNPs have been removed properly
    # include the *.missnp file as an exclusion

    outpattern = ".".join(outfile.split(".")[:-1])
    job_memory = "64G"
    job_threads = 1
    with open(temp_file, "w") as ofile:
        for ifile in infiles:
            ifile_bed = ifile[0]
            ifile_fam = ifile[1][0]
            ifile_bim = ifile[1][1]
            ofile.write("%s\t%s\t%s\n" % (ifile_bed, ifile_bim, ifile_fam))

    statement = '''
    plink2
    --merge-list %(temp_file)s
    --threads %(job_threads)i
    --out %(outpattern)s;
    rm -rf %(temp_file)s
    '''

    P.run()


@follows(mergePlinkFiles)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"QC.dir/\1.het.gz")
def calcInbreeding(infiles, outfile):
    '''
    Detect individuals with excess of heterozygosity - indicative
    of population outbreeding/admixture and/or genotyping
    errors
    Also relevant to inbreeding, i.e. depletion of heterozygosity
    indicates individuals more related to themselves than expected
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    temp_file = P.getTempFilename(shared=True)

    job_memory = "4G"
    statement = '''
    cgat geno2qc
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --keep-individuals=%(gwas_keep)s
    --summary-method=inbreeding
    --summary-parameter=gz
    --output-file-pattern=%(temp_file)s
    --log=%(outfile)s.log
    %(plink_files)s;
    zcat %(temp_file)s.het.gz | tr -s ' ' '\\t' |
    sed -E 's/^[[:space:]]|[[:space:]]$//g' | gzip > %(outfile)s
    '''

    P.run()


@follows(calcInbreeding)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"QC.dir/\1.ibc")
def findExcessHomozygotes(infiles, outfile):
    '''
    Use the Plink2/GCTA to calculate inbreeding coefficients
    across all individuals expected, i.e. inbred
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-1])

    job_memory = "4G"
    statement = '''
    cgat geno2qc
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --summary-method=inbreeding_coef
    --keep-individuals=%(gwas_keep)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    '''

    P.run()


@follows(mkdir("QC.dir"))
@transform("%s/chrX*" % PARAMS['data_dir'],
           regex("%s/(.+).bed" % PARAMS['data_dir']),
           add_inputs([r"%s/\1.fam" % PARAMS['data_dir'],
                       r"%s/\1.bim" % PARAMS['data_dir'],
                       r"%s" % PARAMS['qc_pseudo_autosomal']]),
           r"QC.dir/\1.sexcheck")
def genderChecker(infiles, outfile):
    '''
    Check self-reported gender against X-chromosome
    inferred gender.

    Input data are plink binary format with the XY
    pseudoautosomal region removed.  This is required
    for the Plink1.9 gender checking function to work
    properly.

    Output a list of discordant individuals
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    pseudoautosome = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-1])
    job_memory = "6G"
    job_threads = 1

    tmp_file = P.getTempFilename(shared=True)

    state1 = '''
    cgat geno2qc
    --program=plink2
    --input-file-format=plink_binary
    --method=check_gender
    --memory=%(job_memory)s
    --exclude-snps=%(pseudoautosome)s
    --keep-individuals=%(gwas_keep)s
    --output-file-pattern=%(tmp_file)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink2.log
    '''

    # swap all spaces for a single tab and remove
    # leading and trailing spaces
    state2 = '''
    cat %(tmp_file)s.sexcheck | tr -s ' ' '\t' |
    sed 's/^[[:space:]]*//g' | sed 's/*[[:space:]]$//g'
    > %(outfile)s
    '''

    statement = ";".join([state1, state2])

    P.run()


@follows(mergePlinkFiles,
         mkdir("grm.dir"))
@transform("genome.dir/WholeGenome.*",
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"grm.dir/\1.grm.N.bin")
def makeGRM(infiles, outfiles):
    '''
    Calculate the realised GRM across all LD trimmed
    variants
    Use parallelisation
    '''

    job_threads = PARAMS['grm_threads']
    # memory per thread
    job_memory = "12G"

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfiles.split(".")[:-3])

    # why does GCTA keep throwing memory errors??
    statement = '''
    cgat geno2assoc
    --program=plink2
    --parallel=%(job_threads)s
    --input-file-format=plink_binary
    --keep-individuals=%(gwas_keep)s
    --memory="120G"
    --method=matrix
    --matrix-compression=bin
    --matrix-form=grm
    --output-file-pattern=%(out_pattern)s
    --log=%(outfiles)s.log
    %(plink_files)s
    > %(outfiles)s.gcta.log
    '''

    P.run()


@follows(mergePlinkFiles)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"QC.dir/\1.genome.gz")
def calculateIdentityByDescent(infiles, outfile):
    '''
    Calculate the pair-wise estimates of IBS/IBD
    between individuals.  This will be used to
    flag related individuals
    '''

    job_memory = "20G"
    job_threads = 12

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2qc
    --program=plink2
    --input-file-format=plink_binary
    --keep-individuals=%(gwas_keep)s
    --parallel=%(job_threads)s
    --memory="240G"
    --threads=%(job_threads)s
    --method=IBD
    --IBD-parameter=norm
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mergePlinkFiles,
         mkdir("pca.dir"))
@transform("genome.dir/WholeGenome.*",
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.bim",
                       r"genome.dir/\1.fam"]),
           r"pca.dir/\1_naive.pcs")
def runNaivePCA(infiles, outfile):
    '''
    flashPCA breaks with too many SNPs.
    '''

    job_threads = PARAMS['pca_threads']
    job_memory = PARAMS['pca_memory']

    bed_file = infiles[0]
    bim_file = infiles[1][0]
    fam_file = infiles[1][1]
    plink_root = ".".join(bed_file.split(".")[:-1])
    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    flashpca --numthreads %(job_threads)i
             --bfile %(plink_root)s
             --ndim 20
             --mem low
             --outpc %(out_pattern)s.pcs
             --outvec %(out_pattern)s.eigenvec
             --outload %(out_pattern)s.loadings
             --outval %(out_pattern)s.eigenval
             --outpve %(out_pattern)s.pve
    '''

    P.run()


@follows(runNaivePCA,
         mkdir("pca.dir"))
@transform("genome.dir/WholeGenome.*",
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"pca.dir/\1_filtered.pcs")
def runFilteredPCA(infiles, outfile):
    '''
    Pre-filter samples for ethnicity.

    flashPCA must be in the PATH variable
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])
    temp_out = P.getTempFilename(shared=True)

    statement1 = '''
    cgat geno2assoc
    --program=plinkdev
    --input-file-format=plink_binary
    --method=format
    --keep-individuals=%(gwas_keep)s
    --format-method=change_format
    --reformat-type=plink_binary
    --output-file-pattern=%(temp_out)s
    --log=%(outfile)s.log
    %(plink_files)s
    '''

    out_pattern = ".".join(outfile.split(".")[:-1])
    job_threads = PARAMS['pca_threads']

    statement2 = '''
    flashpca --numthreads %(job_threads)i
             --bfile %(temp_out)s
             --ndim 20
             --mem low
             --outpc %(out_pattern)s.pcs
             --outvec %(out_pattern)s.eigenvec
             --outload %(out_pattern)s.loadings
             --outval %(out_pattern)s.eigenval
             --outpve %(out_pattern)s.pve
    '''

    statement = " ; ".join([statement1,
                            statement2])

    P.run()


@follows(runNaivePCA,
         runFilteredPCA)
@transform([runNaivePCA,
            runFilteredPCA],
           regex("pca.dir/(.+)_(.+).pcs"),
           add_inputs([PARAMS['data_pheno_all'],
                       r"genome.dir/\1.fam"]),
           r"plots.dir/\1_\2-PC1vsPC2.png")
def plotPcaResults(infiles, outfile):
    '''
    Generate pairwise plot of the first
    2 principal components
    '''

    pcs_file = infiles[0]
    phenotypes = infiles[1][0]
    fam_file = infiles[1][1]

    statement = '''
    cgat pheno2plot
    --plot-type=pca
    --plot-n-pc=2
    --metadata-file=%(phenotypes)s
    --fam-file=%(fam_file)s
    --group-labels=%(format_ethnicity_var)s
    --log=%(outfile)s.log
    --output-file=%(outfile)s
    %(pcs_file)s
    '''

    P.run()


###############################################
# Parse QC files to get a list of individuals #
# to exclude from analyses                    #
###############################################


@follows(genderChecker,
         mkdir("exclusions.dir"))
@transform(genderChecker,
           regex("QC.dir/(.+).sexcheck"),
           r"exclusions.dir/gender_exclusion.txt")
def excludeDiscordantGender(infile, outfile):
    '''
    Make a list of individuals with discordant gender
    from X chromsome data vs. self-reported gender
    '''

    job_memory = "2G"
    statement = '''
    cgat qcs2qc
    --task=discordant_gender
    --gender-check-file=%(infile)s
    --plotting-path=%(plots_dir)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(findExcessHomozygotes,
         excludeDiscordantGender)
@transform(findExcessHomozygotes,
           regex("QC.dir/(.+).ibc"),
           r"exclusions.dir/\1.inbred")
def excludeInbred(infile, outfile):
    '''
    Flag up individuals with high inbreeding
    coefficients based on a threshold.

    Most human populations have F < 0.05, but
    this may be greatly affect by Ne.

    Use GCTA's Fhat3 as an unbiased estimator
    of F.
    '''

    job_memory = "2G"
    statement = '''
    cgat qcs2qc
    --task=find_inbreds
    --inbreeding-coef-file=%(infile)s
    --inbreeding-coefficient=Fhat3
    --inbred-cutoff=%(qc_inbreed_threshold)0.3f
    --plotting-path=%(plots_dir)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(calcInbreeding)
@transform(calcInbreeding,
           regex("QC.dir/(.+).het.gz"),
           r"exclusions.dir/\1.het_exclude")
def findExcessHeterozygotes(infile, outfile):
    '''
    Calculate the heterozygosity rate and flag individuals
    with excess heterozygosity indicative of population
    admixture or genotyping errors/contamination.

    Also flag individuals with high inbreeding coefficient.
    Plot these - if there are multiple clusters - indicates
    additional populations of individuals - see UKBiobank
    documentation for details
    '''

    job_memory = "2G"
    statement = '''
    cgat qcs2qc
    --task=flag_hets
    --heterozygotes-file=%(infile)s
    --plotting-path=%(plots_dir)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(mergePlinkFiles)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"exclusions.dir/\1.rel.id")
def excludeRelated(infiles, outfile):
    '''
    Find and exclude related individuals with IBD
    >= a threshold. Recommend IBD <= 3rd cousins
    (IBD >= 0.03125) - this can be parallelised

    Plink2 recommend using the binary genotype
    files as input, not the previously computed
    GRM. See here for details:
    https://www.cog-genomics.org/plink2/distance#rel_cutoff

    # Plink seems to be over doing the trimming of
    # individuals based on --rel-cutoff <- needs
    # further investigation.
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    job_memory = "10G"
    job_threads = 24
    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])

    statement = '''
    cgat geno2qc
    --program=plink2
    --input-file-format=plink_binary
    --method=remove_relations
    --threshold=%(relationship_cutoff)s
    --memory=240G
    --threads=%(job_threads)s
    --keep-individuals=%(gwas_keep)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(excludeRelated)
@transform(excludeRelated,
           regex("exclusions.dir/(.+).rel.id"),
           add_inputs(r"genome.dir/\1.fam"),
           r"exclusions.dir/\1.related")
def flagRelated(infiles, outfile):
    '''
    Diff the input .fam and the unrelated individuals
    from the --rel-cutoff to get a set of related
    individuals to exclude
    '''

    unrelated = infiles[0]
    fam_file = infiles[1]

    job_memory = "1G"
    to_cluster = False
    statement = '''
    cat %(fam_file)s | tr -s ' ' '\\t' | cut -f 1,2
    | diff %(unrelated)s - | grep ">" | sed 's/>//g'
    > %(outfile)s
    '''

    P.run()


@follows(calculateIdentityByDescent)
@transform(calculateIdentityByDescent,
           regex("QC.dir/(.+).genome.gz"),
           r"plots.dir/IBD-hist.png")
def plotIbdHistogram(infile, outfile):
    '''
    plot the distribution of IBD estimates
    '''

    job_memory = "300G"

    statement = '''
    cgat qcs2qc
    --task=flag_relations
    --relationship-file=%(infile)s
    --ibs-cutoff=%(relationship_cutoff)s
    --plotting-path=plots.dir
    --log=%(outfile)s.log
    '''

    P.run()

# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Association testing and other statistical testing tasks


if PARAMS['candidate_region']:
    @follows(convertToPlink,
             mkdir("candidate.dir"))
    @transform(convertToPlink,
               regex("plink.dir/%s(.+).bed" % PARAMS['candidate_chromosome']),
               add_inputs([r"plink.dir/%s\1.fam" % PARAMS['candidate_chromosome'],
                           r"plink.dir/%s\1.bim" % PARAMS[
                               'candidate_chromosome'],
                           r"plink.dir/%s\1.exclude" % PARAMS[
                               'candidate_chromosome'],
                           r"exclusions.dir/WholeGenome.gwas_exclude"]),
               r"candidate.dir/%s\1-candidate_region.bed" % PARAMS['candidate_chromosome'])
    def getCandidateRegion(infiles, outfile):
        '''
        Pull out genotyping data on individuals over a
        candidate region for testing.
        '''

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        exclude = infiles[1][2]
        gwas_exclude = infiles[1][3]
        region = ",".join(PARAMS['candidate_region'].split(":")[-1].split("-"))
        out_pattern = ".".join(outfile.split(".")[:-1])

        statement = '''
        cgat geno2assoc
        --program=plink2
        --input-file-format=plink_binary
        --method=format
        --keep-individuals=%(gwas_keep)s
        --remove-individuals=%(gwas_exclude)s
        --restrict-chromosome=%(candidate_chromosome)s
        --snp-range=%(region)s
        --exclude-snps=%(exclude)s
        --format-method=change_format
        --format-parameter=%(format_gender)s
        --update-sample-attribute=gender
        --reformat-type=plink_binary
        --output-file-pattern=%(out_pattern)s
        --log=%(outfile)s.log
        %(plink_files)s
        '''

        P.run()

    # make a GRM from the candidate region for MLM analysis
    # need to remove duplicates first

    @follows(getCandidateRegion)
    @transform("candidate.dir/*.bed",
               regex("candidate.dir/(.+).bed"),
               add_inputs([r"candidate.dir/\1.fam",
                           r"candidate.dir/\1.bim"]),
               r"grm.dir/\1.grm.N.bin")
    def makeCandidateGRM(infiles, outfiles):
        '''
        Calculate the realised GRM across all candidate region
        variants.   Use parallelisation.
        '''

        job_threads = PARAMS['grm_threads']
        # memory per thread
        job_memory = "12G"

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]

        plink_files = ",".join([bed_file, fam_file, bim_file])
        out_pattern = ".".join(outfiles.split(".")[:-3])

        statement = '''
        cgat geno2assoc
        --program=gcta
        --threads=%(job_threads)s
        --input-file-format=plink_binary
        --keep-individuals=%(gwas_keep)s
        --method=matrix
        --matrix-compression=bin
        --matrix-form=grm
        --output-file-pattern=%(out_pattern)s
        --log=%(outfiles)s.log
        %(plink_files)s
        > %(outfiles)s.gcta.log
        '''

        P.run()

    @follows(getCandidateRegion)
    @transform(getCandidateRegion,
               regex("candidate.dir/(.+).bed"),
               add_inputs([r"candidate.dir/\1.fam",
                           r"candidate.dir/\1.bim"]),
               r"candidate.dir/\1_assoc.assoc")
    def testCandidateRegion(infiles, outfile):
        '''
        Test the candidate region for association using
        Plink basic association - i.e. not a model-specific
        analysis
        '''

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        out_pattern = ".".join(outfile.split(".")[:-1])

        job_threads = PARAMS['candidate_threads']
        job_memory = PARAMS['candidate_memory']

        statement = '''
        cgat geno2assoc
        --program=plink2
        --input-file-format=plink_binary
        --method=association
        --keep-individuals=%(gwas_keep)s
        --association-method=assoc
        --genotype-rate=0.01
        --indiv-missing=0.01
        --hardy-weinberg=0.0001
        --min-allele-frequency=0.001
        --output-file-pattern=%(out_pattern)s
        --threads=%(candidate_threads)s
        --log=%(outfile)s.log
        -v 5
        %(plink_files)s
        '''

        P.run()

    @follows(testCandidateRegion)
    @transform(getCandidateRegion,
               regex("candidate.dir/(.+).bed"),
               add_inputs([r"candidate.dir/\1.fam",
                           r"candidate.dir/\1.bim"]),
               r"candidate.dir/\1_conditional.%s" % PARAMS['conditional_model'])
    def conditionalTestRegion(infiles, outfile):
        '''
        Perform association analysis conditional on
        top SNP from previous analysis
        '''

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        out_pattern = ".".join(outfile.split(".")[:-1])

        statement = '''
        cgat geno2assoc
        --program=plink2
        --input-file-format=plink_binary
        --method=association
        --association-method=logistic
        --keep-individuals=%(gwas_keep)s
        --genotype-rate=0.1
        --indiv-missing=0.1
        --hardy-weinberg=0.0001
        --conditional-snp=rs12924101
        --min-allele-frequency=0.01
        --output-file-pattern=%(out_pattern)s
        --threads=%(pca_threads)s
        --log=%(outfile)s.log
        -v 5
        %(plink_files)s
        '''

        P.run()
else:
    pass


@follows(excludeDiscordantGender,
         findExcessHeterozygotes,
         findExcessHomozygotes,
         excludeInbred,
         flagRelated)
@transform("exclusions.dir/*.het_exclude",
           regex("exclusions.dir/(.+).het_exclude"),
           add_inputs([r"exclusions.dir/\1.inbred",
                       r"exclusions.dir/\1.related",
                       excludeDiscordantGender]),
           r"exclusions.dir/\1.gwas_exclude")
def mergeExclusions(infiles, outfile):
    '''
    Merge together files and drop duplicates
    for individuals that fail QC steps
    '''

    hets = infiles[0]
    inbreds = infiles[1][0]
    related = infiles[1][1]
    gender = infiles[1][2]

    statement = '''
    cgat qcs2qc
    --task=merge_exclusions
    --gender-check-file=%(gender)s
    --relationship-file=%(related)s
    --inbreeding-coef-file=%(inbreds)s
    --heterozygotes-file=%(hets)s
    --auxillary-file=%(gwas_remove)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()

if PARAMS['gwas_model'] == "linear":
    @follows(convertToPlink,
             mergeExclusions,
             mkdir("gwas.dir"))
    @transform("plink.dir/chr*",
               regex("plink.dir/(.+).bed"),
               add_inputs([r"plink.dir/\1.fam",
                           r"plink.dir/\1.bim",
                           r"exclusions.dir/WholeGenome.gwas_exclude"]),
               r"gwas.dir/\1_assoc.qassoc")
    def unadjustedAssociation(infiles, outfile):
        '''
        Run an unadjusted association analysis on SNPs MAF >= 1%
        Need to condition on array batch - field 22000
        '''

        job_memory = "36G"

        mem = int(job_memory.strip("G"))
        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]

        gwas_exclude = infiles[1][2]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        out_pattern = ".".join(outfile.split(".")[:-1])

        statement = '''
        cgat geno2assoc
        --program=plink2
        --input-file-format=plink_binary
        --phenotypes-file=%(data_phenotypes)s
        --pheno=%(format_pheno)s
        --method=association
        --keep=%(gwas_keep)s
        --remove-individuals=%(gwas_exclude)s
        --association-method=assoc
        --genotype-rate=0.1
        --hardy-weinberg=0.000000000000001
        --min-allele-frequency=0.001
        --output-file-pattern=%(out_pattern)s
        --memory=%(mem)s
        -v 5
        %(plink_files)s
        > %(outfile)s.plink.log
        '''

        P.run()

else:
    @follows(convertToPlink,
             mergeExclusions,
             mkdir("gwas.dir"))
    @transform("plink.dir/chr*",
               regex("plink.dir/(.+).bed"),
               add_inputs([r"plink.dir/\1.fam",
                           r"plink.dir/\1.bim",
                           r"exclusions.dir/WholeGenome.gwas_exclude"]),
               r"gwas.dir/\1_assoc.assoc")
    def unadjustedAssociation(infiles, outfile):
        '''
        Run an unadjusted association analysis on SNPs MAF >= 1%
        Need to condition on array batch - field 22000
        '''

        job_memory = "36G"

        mem = int(job_memory.strip("G"))
        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]

        gwas_exclude = infiles[1][2]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        out_pattern = ".".join(outfile.split(".")[:-1])

        statement = '''
        cgat geno2assoc
        --program=plink2
        --input-file-format=plink_binary
        --phenotypes-file=%(data_phenotypes)s
        --pheno=%(format_pheno)s
        --method=association
        --keep=%(gwas_keep)s
        --remove-individuals=%(gwas_exclude)s
        --association-method=assoc
        --genotype-rate=%(gwas_geno)s
        --hardy-weinberg=%(gwas_hwe)s
        --min-allele-frequency=%(gwas_maf)s
        --indiv-missing=%(gwas_mind)s
        --output-file-pattern=%(out_pattern)s
        --memory=%(mem)s
        --subset-filter=%(gwas_filter)s
        -v 5
        %(plink_files)s
        > %(outfile)s.plink.log
        '''

        P.run()


@follows(mkdir("covariates.dir"),
         mergeExclusions,
         unadjustedAssociation)
@transform("covariates.dir/*",
           regex("covariates.dir/(.+).batch"),
           r"covariates.dir/\1.covar")
def mergeCovariates(infiles, outfile):
    '''
    Merge together all covariates for inclusion
    in the adjusted GWA
    '''

    covar_file = infiles
    pca_file = PARAMS['gwas_pca']
    covars = ",".join([covar_file, pca_file])
    job_memory = "4G"

    statement = '''
    cgat pheno2pheno
    --task=merge_covariates
    --covariate-file=%(covars)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(convertToPlink,
         mergeCovariates,
         mergeExclusions,
         mkdir("gwas.dir"))
@transform("plink.dir/chr*",
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"exclusions.dir/WholeGenome.gwas_exclude",
                       mergeCovariates]),
           r"gwas.dir/\1_adj.assoc.%s" % PARAMS['gwas_model'])
def pcAdjustedAssociation(infiles, outfile):
    '''
    Run an association analysis on SNPs MAF >= 1%
    adjusted for principal components
    Need to condition on array batch - field 22000
    '''

    job_memory = "36G"

    mem = int(job_memory.strip("G"))
    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    remove = infiles[1][2]
    covariate_file = infiles[1][3]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-2])

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=association
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --covariates-file=%(covariate_file)s
    --covariate-column=%(gwas_covars)s
    --keep=%(gwas_keep)s
    --remove-individuals=%(remove)s
    --association-method=%(gwas_model)s
    --genotype-rate=%(gwas_geno)s
    --hardy-weinberg=%(gwas_hwe)s
    --min-allele-frequency=%(gwas_maf)s
    --indiv-missing=%(gwas_mind)s
    --output-file-pattern=%(out_pattern)s
    --memory=%(mem)s
    --subset-filter=%(gwas_filter)s
    -v 5
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mkdir("plots.dir"),
         unadjustedAssociation)
@collate(unadjustedAssociation,
         regex("gwas.dir/(.+)_assoc.(.+)"),
         r"plots.dir/WholeGenome_manhattan.png")
def plotUnadjustedManhattan(infiles, outfile):
    '''
    Generate a manhattan plot for the unadjusted
    association analysis
    '''
    job_memory = "16G"

    res_files = ",".join(infiles)
    out_file = outfile.split("/")[-1]
    out_file = out_file.split("-")[0]
    out_file = "gwas.dir/" + out_file + ".results"

    statement = '''
    cgat assoc2plot
    --plot-type=manhattan
    --resolution=genome_wide
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(res_files)s
    > %(out_file)s
    '''

    P.run()


@follows(mkdir("plots.dir"),
         unadjustedAssociation,
         plotUnadjustedManhattan,
         pcAdjustedAssociation)
@collate(pcAdjustedAssociation,
         regex("gwas.dir/(.+)_adj.assoc.%s" % PARAMS['gwas_model']),
         r"plots.dir/WholeGenome_adj-manhattan.png")
def plotGenomeManhattan(infiles, outfile):
    '''
    Generate a manhattan plot across all chromosomes
    from the covariate adjusted analysis
    '''

    job_memory = "16G"

    res_files = ",".join(infiles)
    out_file = outfile.split("/")[-1]
    out_file = out_file.split("-")[0]
    out_file = "gwas.dir/" + out_file + ".results"
    statement = '''
    cgat assoc2plot
    --plot-type=manhattan
    --resolution=genome_wide
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(res_files)s
    > %(out_file)s
    '''

    P.run()


@follows(mkdir("plots.dir"),
         unadjustedAssociation,
         pcAdjustedAssociation,
         plotUnadjustedManhattan,
         plotGenomeManhattan)
@collate(pcAdjustedAssociation,
         regex("gwas.dir/(.+)_adj.assoc.%s" % PARAMS['gwas_model']),
         r"plots.dir/WholeGenome_adj-qqplot.png")
def plotGenomeQQ(infiles, outfile):
    '''
    Generate a QQ plot across all chromosome
    from the covariate adjusted analysis
    '''

    job_memory = "32G"

    res_files = ",".join(infiles)
    statement = '''
    cgat assoc2plot
    --plot-type=qqplot
    --resolution=genome_wide
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(res_files)s
    '''

    P.run()


# testing conditional analysis of multiple regions
# make a load of dummy files first
@follows(pcAdjustedAssociation,
         mkdir("conditional.dir"))
@transform(PARAMS['gwas_hit_regions'],
           regex("(.+)/(.+)-(.+).tsv"),
           r"conditional.dir/\2.tsv")
def splitRegionsFile(infile, outfile):
    '''
    Split a file containing genome intervals
    to pull out for conditional analyses
    '''

    # I LOVE AWK!!!!

    statement = '''
    cat %(infile)s | awk '{if(NR > 1) {printf("conditional.dir/chr%%i-%%i-%%i_%%s.tsv\\n",
    $1, $2, $3, $4)}}' | awk '{system("touch " $0)}';
    '''

    P.run()
    P.touch(outfile)


@follows(splitRegionsFile)
@transform("conditional.dir/*.tsv",
           regex("conditional.dir/(.+)-(.+)-(.+)_(.+).tsv"),
           add_inputs([r"plink.dir/\1.bed",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.fam",
                       r"plink.dir/\1.exclude"]),
           r"conditional.dir/\1-\2-\3_\4.bed")
def getConditionalRegions(infiles, outfile):
    '''
    Pull out the regions of interest for downstream
    conditional analyses
    '''

    conditional_file = infiles[0]
    bed_file = infiles[1][0]
    bim_file = infiles[1][1]
    fam_file = infiles[1][2]
    exclude_snps = infiles[1][3]

    chrom = conditional_file.split("/")[-1].split("-")[0]
    start = conditional_file.split("/")[-1].split("-")[1]
    end = conditional_file.split("/")[-1].split("-")[2]
    end = end.split("_")[0]

    snp_range = ",".join([start, end])
    plink_files = ",".join([bed_file, bim_file, fam_file])

    out_pattern = outfile.strip(".bed")

    job_memory = "40G"
    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=format
    --restrict-chromosome=%(chrom)s
    --snp-range=%(snp_range)s
    --exclude-snps=%(exclude_snps)s
    --format-method=change_format
    --reformat-type=plink_binary
    --keep=%(gwas_keep)s
    --output-file-pattern=%(out_pattern)s
    --memory=%(job_memory)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(getConditionalRegions)
@transform(getConditionalRegions,
           regex("conditional.dir/(.+).bed"),
           add_inputs([r"conditional.dir/\1.fam",
                       r"conditional.dir/\1.bim",
                       r"exclusions.dir/WholeGenome.gwas_exclude",
                       mergeCovariates]),
           r"conditional.dir/\1_conditional.assoc.%s" % PARAMS['conditional_model'])
def conditionalAssociation(infiles, outfile):
    '''
    Perform association analysis conditional on
    top SNP from previous analysis
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    condition_snp = bed_file.split("/")[-1].split("_")[-1]
    conditional_snp = condition_snp.rstrip(".bed")
    remove = infiles[1][2]
    covariate_file = infiles[1][3]

    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "40G"

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=association
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --association-method=%(conditional_model)s
    --covariates-file=%(covariate_file)s
    --covariate-column=%(gwas_covars)s
    --keep-individuals=%(gwas_keep)s
    --remove-individuals=%(remove)s
    --genotype-rate=0.1
    --hardy-weinberg=1e-50
    --memory=%(job_memory)s
    --conditional-snp=%(conditional_snp)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    -v 5
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# test for epistasis between GWAS regions on the phenotype of interest.  Use gwas hit
# regions


@follows(getConditionalRegions,
         mkdir("epistasis.dir"))
@collate(getConditionalRegions,
         regex("conditional.dir/(.+).bed"),
         add_inputs([r"conditional.dir/\1.fam",
                     r"conditional.dir/\1.bim"]),
         r"epistasis.dir/GwasHits.bed")
def mergeGwasHits(infiles, outfile):
    '''
    Take the GWAS hit regions and merge
    into a single set of Plink files to test
    epistatic interactions
    '''

    # generate text file that contains the names of the files
    # to be merged
    temp_file = P.getTempFilename(shared=True)
    outpattern = ".".join(outfile.split(".")[:-1])
    job_memory = "3G"
    job_threads = 10

    with open(temp_file, "w") as ofile:
        for ifile in infiles:
            ifile_bed = ifile[0]
            ifile_fam = ifile[1][0]
            ifile_bim = ifile[1][1]
            ofile.write("%s\t%s\t%s\n" % (ifile_bed, ifile_bim, ifile_fam))

    statement = '''
    plink2
    --merge-list %(temp_file)s
    --threads %(job_threads)i
    --out %(outpattern)s;
    rm -rf %(temp_file)s
    '''

    P.run()


@follows(mergeGwasHits)
@transform(mergeGwasHits,
           regex("epistasis.dir/(.+).bed"),
           add_inputs([r"epistasis.dir/\1.fam",
                       r"epistasis.dir/\1.bim"]),
           r"epistasis.dir/\1_VsRegion.epi.cc")
def testEpistasisVsRegion(infiles, outfile):
    '''
    Test for epistasis with a set of specific variants
    of interest provided in a Plink .set file

    Only test SNPs with MAF >= 0.5%
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "1G"
    job_threads = 11

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --method=epistasis
    --epistasis-method=epistasis
    --set-file=%(epistasis_set)s
    --set-method="set-by-all"
    --epistasis-threshold=%(epistasis_threshold)s
    --epistasis-report-threshold=%(epistasis_reporting)s
    --min-allele-freq=0.005
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    --threads=%(job_threads)s
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()

############################################
# We want to test for epistasis explicitly between target region variants
# and all others whilst adjusting for covariates and removing sources of
# type I error.
# This means for each target region variant any SNPs in LD must be removed.
# So first create lists of variants, for each target variant, that are NOT
# in LD.  These will be selected using Plink to generate the relevant
# files for analysis

# get the list of region SNPs first and create a dummy file for each


@follows(testEpistasisVsRegion,
         mkdir("target_snps.dir"))
@subdivide("%s" % PARAMS['epistasis_set'],
           regex("epistasis.dir/(.+).set"),
           r"epistasis.dir/split_snps.text")
def splitTargetVariants(infile, outfile):
    '''
    Generate a dummy file for each variant in the
    set file
    '''

    job_memory = "0.5G"
    statement = '''
    cat %(infile)s | grep -v "END" | awk '{if(NR > 1) {printf("target_snps.dir/%%s.target\\n", $1)}}'
    | awk '{system("touch " $0)}';
    '''

    P.run()
    P.touch(outfile)


@follows(splitTargetVariants)
@transform("target_snps.dir/*",
           regex("target_snps.dir/(.+).target"),
           r"target_snps.dir/\1.exclude")
def excludeLdVariants(infile, outfile):
    '''
    Extract the variants in LD with target variant
    to exclude from epistasis analysis
    '''

    contig = PARAMS['candidate_chromosome'].lstrip("chr")
    ld_dir = os.path.join(os.getcwd(), "ld.dir")
    ld_files = [lf for lf in os.listdir(ld_dir) if re.search(contig, lf)]
    ld_fle = [os.path.join(ld_dir, bg)
              for bg in ld_files if re.search("bgz$", bg)][0]

    snp = infile.split("/")[-1].split(".")[0]
    job_memory = "2G"

    statement = '''tabix %(ld_fle)s %(contig)s:87500000-90354753 |
    grep "%(snp)s" | awk '{ print } END {if (!NR) {print "Empty"} else {if($7 > 0.1) { print }}}'
    | cut -f 3,6 | tr -s '\\t' '\\n' | tr -s ';' '\\n' | sort | uniq | grep -v %(snp)s
    > %(outfile)s
    '''

    P.run()


@follows(splitTargetVariants,
         excludeLdVariants)
@transform(mergeGwasHits,
           regex("epistasis.dir/(.+).bed"),
           add_inputs([r"epistasis.dir/\1.fam",
                       r"epistasis.dir/\1.bim",
                       r"exclusions.dir/WholeGenome.gwas_exclude"]),
           r"epistasis.dir/\1.raw")
def convertToRawFormat(infiles, outfile):
    '''
    Extract the target SNP genotypes in raw format
    to merge with the covariates file
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])
    exclusions = infiles[1][2]

    out_pattern = ".".join(outfile.split(".")[:-1])
    job_memory = "30G"

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=format
    --format-method=change_format
    --reformat-type=raw
    --keep-individuals=%(gwas_keep)s
    --remove-individuals=%(exclusions)s
    --extract-snps=%(epistasis_set)s
    --output-file-pattern=%(out_pattern)s
    --memory=%(job_memory)s
    %(plink_files)s
    '''

    P.run()


@follows(convertToRawFormat)
@transform(convertToRawFormat,
           regex("epistasis.dir/(.+).raw"),
           add_inputs("covariates.dir/WholeGenome.covar"),
           r"epistasis.dir/\1.covar")
def mergeGenotypeAndCovariates(infiles, outfile):
    '''
    Merge covariates and target SNPs into a single
    file
    '''

    snp_file = infiles[0]
    covar_file = infiles[1]
    covars = ",".join([covar_file, snp_file])
    job_memory = "4G"

    statement = '''
    cgat pheno2pheno
    --task=merge_covariates
    --adjustment=snp
    --covariate-file=%(covars)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(excludeLdVariants)
@transform(excludeLdVariants,
           regex("target_snps.dir/(.+).exclude"),
           add_inputs([r"epistasis.dir/GwasHits.bed",
                       r"epistasis.dir/GwasHits.fam",
                       r"epistasis.dir/GwasHits.bim"]),
           r"epistasis.dir/single_\1.epi.cc")
def ldExcludedEpistasisVsGwasLead(infiles, outfile):
    '''
    Test for epistasis between a given variant derived
    from a set file, and only the lead SNPs from
    a genome-wide analysis
    '''

    job_memory = "40G"
    job_threads = 1

    snp = infiles[0].split("/")[-1].split(".")[0]
    ld_exclude = infiles[0]

    bed_file = infiles[1][0]
    fam_file = infiles[1][1]
    bim_file = infiles[1][2]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-2])
    out_pattern = out_pattern

    # write the SNP id to a dummy set file
    tmpf = P.getTempFilename(shared=True)
    with open(tmpf, "w") as tfile:
        tfile.write("VAR\n{}\nEND".format(snp))

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --method=epistasis
    --exclude-snps=%(ld_exclude)s
    --epistasis-method=epistasis
    --extract-snps=%(epistasis_hit_region)s
    --set-file=%(tmpf)s
    --set-method="set-by-all"
    --epistasis-threshold=%(epistasis_threshold)s
    --epistasis-report-threshold=%(epistasis_reporting)s
    --min-allele-freq=0.001
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    --threads=%(job_threads)s
    --memory=%(job_memory)s
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()

    statement = '''rm -rf %(tmpf)s'''

    P.run()


@follows(excludeLdVariants,
         ldExcludedEpistasisVsGwasLead)
@transform(excludeLdVariants,
           regex("target_snps.dir/(.+).exclude"),
           add_inputs([r"epistasis.dir/GwasHits.bed",
                       r"epistasis.dir/GwasHits.fam",
                       r"epistasis.dir/GwasHits.bim"]),
           r"epistasis.dir/\1.epi.cc")
def ldExcludedEpistasis(infiles, outfile):
    '''
    Test for epistasis with a given variant derived
    from a set file, exlcuding all variants
    in LD.
    Only test SNPs with MAF >= 0.5%
    '''

    job_memory = "80G"
    job_threads = 1

    snp = infiles[0].split("/")[-1].split(".")[0]
    ld_exclude = infiles[0]

    bed_file = infiles[1][0]
    fam_file = infiles[1][1]
    bim_file = infiles[1][2]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-2])

    # write the SNP id to a dummy set file
    tmpf = P.getTempFilename(shared=True)
    with open(tmpf, "w") as tfile:
        tfile.write("VAR\n{}\nEND".format(snp))

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --method=epistasis
    --exclude-snps=%(ld_exclude)s
    --epistasis-method=epistasis
    --set-file=%(tmpf)s
    --set-method="set-by-all"
    --epistasis-threshold=%(epistasis_threshold)s
    --epistasis-report-threshold=%(epistasis_reporting)s
    --min-allele-freq=0.005
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    --threads=%(job_threads)s
    --memory=%(job_memory)s
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()

    statement = '''rm -rf %(tmpf)s'''

    P.run()


@follows(ldExcludedEpistasis)
@transform(ldExcludedEpistasis,
           regex("epistasis.dir/rs(.+).epi.cc"),
           r"plots.dir/rs\1-naive_epistasis_manhattan.png")
def plotLdExcludedEpistasis(infile, outfile):
    '''
    Generate a manhattan plot and QQplot
    of the adjusted epistasis analysis
    '''

    job_memory = "8G"

    plot_path = "_".join(outfile.split("_")[:-1])

    statement = '''
    cgat assoc2plot
    --plot-type=epistasis
    --resolution=chromosome
    --log=%(outfile)s.log
    --save-path=%(plot_path)s
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@jobs_limit(6)
@follows(mergeGenotypeAndCovariates,
         excludeLdVariants,
         plotLdExcludedEpistasis)
@transform(excludeLdVariants,
           regex("target_snps.dir/(.+).exclude"),
           add_inputs([r"epistasis.dir/GwasHits.bed",
                       r"epistasis.dir/GwasHits.fam",
                       r"epistasis.dir/GwasHits.bim",
                       mergeGenotypeAndCovariates]),
           r"epistasis.dir/\1.auto.R")
def adjustedEpistasis(infiles, outfile):
    '''
    Test for epistasis between target SNPs
    and SNPs of interest, whilst adjusting for
    covariates.
    '''

    job_memory = "75G"
    job_threads = 1

    snp = infiles[0].split("/")[-1].split(".")[0]
    ld_exclude = infiles[0]

    bed_file = infiles[1][0]
    fam_file = infiles[1][1]
    bim_file = infiles[1][2]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    covar_file = infiles[1][3]
    covars = PARAMS['gwas_covars']
    all_covars = ",".join([covars, snp])

    out_pattern = ".".join(outfile.split(".")[:-2])

    statement = '''
    R CMD Rserve --vanilla; checkpoint;
    cgat geno2assoc
    --program=plinkdev
    --input-file-format=plink_binary
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --method=epistasis
    --epistasis-method=adjusted
    --epistasis-parameter=%(epistasis_plugin)s
    --covariates-file=%(covar_file)s
    --covariate-column=%(all_covars)s
    --exclude-snps=%(ld_exclude)s
    --min-allele-freq=0.005
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    --memory=%(job_memory)s
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(adjustedEpistasis)
@transform(adjustedEpistasis,
           regex("epistasis.dir/(.+).auto.R"),
           r"plots.dir/\1-epistasis_manhattan.png")
def plotAdjustedEpistasis(infile, outfile):
    '''
    Generate a manhattan plot and QQplot
    of the adjusted epistasis analysis
    '''

    job_memory = "4G"

    plot_path = "_".join(outfile.split("_")[:-1])

    statement = '''
    cgat assoc2plot
    --plot-type=epistasis
    --resolution=chromosome
    --log=%(outfile)s.log
    --save-path=%(plot_path)s
    %(infile)s
    > %(outfile)s
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Analysis of pleiotropy using the Pleiotropy Estimation and Testing method               #
# of Zhang et al Genetic Epidemiology 2014                                                #
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Requires SNPs in raw genotype (human readable) format, phenotypes and
# covariates

@follows(convertToRawFormat,
         mkdir("pleiotropy.dir"))
@transform(convertToRawFormat,
           regex("epistasis.dir/(.+).raw"),
           add_inputs(["covariates.dir/WholeGenome.merged",
                       selectBritish]),
           r"pleiotropy.dir/\1.merged")
def mergeForPleiotropy(infiles, outfile):
    '''
    Merge covariates, phenotype and target SNPs into a single
    file
    '''

    snp_file = infiles[0]
    covar_file = infiles[1][0]
    pheno_file = infiles[1][1]
    covars = ",".join([covar_file, snp_file, pheno_file])
    job_memory = "4G"

    statement = '''
    cgat pheno2pheno
    --task=merge_covariates
    --adjustment=snp
    --covariate-file=%(covars)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(mergeForPleiotropy)
@transform(mergeForPleiotropy,
           suffix(".merged"),
           ".pleiotropy")
def calcPleiotropyTest(infile, outfile):
    '''
    Apply the PET-B method to genotype data for
    two traits.
    '''

    job_memory = "40G"
    job_threads = 1

    statement = '''
    cgat testPleiotropy
    --R-scripts=%(r_scripts)s
    --trait1=%(pleiotropy_trait1)s
    --trait2=%(pleiotropy_trait2)s
    --covariates=%(gwas_covars)s
    --resamples=%(pleiotropy_resamples)i
    --trait1-model=%(pleiotropy_trait1_model)s
    --trait2-model=%(pleiotropy_trait2_model)s
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# mixed model analysis on a subsample of the data. Compute GRM from specific SNP set
# take SNP set across limited region of MC1R??


@follows(convertToPlink,
         mkdir("mlm.dir"))
@transform("%s/%s*" % (PARAMS['mlm_genotypes'],
                       PARAMS['mlm_grm_region'].split("-")[0]),
           regex("%s/(.+).bed" % PARAMS['mlm_genotypes']),
           add_inputs([r"%s/\1.fam" % PARAMS['mlm_genotypes'],
                       r"%s/\1.bim" % PARAMS['mlm_genotypes'],
                       r"%s/\1.exclude" % PARAMS['mlm_genotypes'],
                       mergeExclusions]),
           r"mlm.dir/\1_region.bed")
def getGrmRegion(infiles, outfile):
    '''
    Get a specific region to generate the GRM for
    the linear mixed model
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    exclude = infiles[1][2]
    gwas_exclude = infiles[1][3]
    out_pattern = ".".join(outfile.split(".")[:-1])
    chromosome = PARAMS['mlm_grm_region'].split("-")[0]
    region = ",".join(PARAMS['mlm_grm_region'].split("-")[1:])

    job_memory = "30G"

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=format
    --keep-individuals=%(gwas_keep)s
    --remove-individuals=%(gwas_exclude)s
    --restrict-chromosome=%(chromosome)s
    --snp-range=%(region)s
    --exclude-snps=%(exclude)s
    --format-method=change_format
    --format-parameter=%(format_gender)s
    --update-sample-attribute=gender
    --reformat-type=plink_binary
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    '''

    P.run()


@follows(getGrmRegion)
@transform("mlm.dir/*.fam",
           regex("mlm.dir/(.+).fam"),
           add_inputs("%s" % PARAMS['gwas_keep']),
           r"mlm.dir/\1.keep")
def subSampleIndividuals(infiles, outfile):
    '''
    Subsample from the total population
    to get ~n individuals for linear
    model analysis.
    Output with combined list of ethnically
    selected individuals
    '''

    fam_file = infiles[0]
    keep_file = infiles[1]
    keep_temp = P.getTempFilename(shared=True)
    fam_temp = P.getTempFilename(shared=True)

    statement = '''
    cat %(keep_file)s | cut -f1,2 | sort | grep -v "FID" > %(keep_temp)s;
    cat %(fam_file)s | tr " " "\\t" | cut -f1,2 | sort > %(fam_temp)s;
    comm -12 %(fam_temp)s %(keep_temp)s | shuf -n %(mlm_subsample)s
    > %(outfile)s;
    rm -rf %(keep_temp)s %(fam_temp)s
    '''

    P.run()


@follows(getGrmRegion,
         subSampleIndividuals)
@transform("mlm.dir/*.bed",
           regex("mlm.dir/(.+).bed"),
           add_inputs([r"mlm.dir/\1.fam",
                       r"mlm.dir/\1.bim",
                       subSampleIndividuals]),
           r"mlm.dir/\1.grm.N.bin")
def calcRegionGrm(infiles, outfiles):
    '''
    Calculate the realised GRM across all region
    variants. Use multi-threading, no parallelisation
    required.
    '''

    job_threads = PARAMS['grm_threads']
    # memory per thread
    job_memory = "8G"

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    grm_keep = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfiles.split(".")[:-3])

    statement = '''
    cgat geno2assoc
    --program=gcta
    --threads=%(job_threads)s
    --input-file-format=plink_binary
    --keep-individuals=%(grm_keep)s
    --method=matrix
    --matrix-compression=bin
    --matrix-form=grm
    --output-file-pattern=%(out_pattern)s
    --log=%(outfiles)s.log
    %(plink_files)s
    > %(outfiles)s.gcta.log
    '''

    P.run()


@follows(subSampleIndividuals)
@transform("mlm.dir/*.fam",
           regex("mlm.dir/(.+).fam"),
           r"mlm.dir/\1.pheno")
def subsetPhenotype(infile, outfile):
    '''
    Generate a phenotype file of the subset
    individuals from the .fam file.

    GCTA doesn't like having extraneous phenotype
    data when the sample size is large
    '''

    job_memory = "1G"

    statement = '''
    cat %(infile)s | tr " " "\\t" | cut -f 1,2,6 |
    awk 'BEGIN {printf("FID\\tIID\\tPHENO\\n")} {print $0}'
    > %(outfile)s'''

    P.run()


@follows(subsetPhenotype)
@transform("plink.dir/*.bed",
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       mergeExclusions,
                       subsetPhenotype,
                       r"covariates.dir/WholeGenome.batch"]),
           r"mlm.dir/\1.mlma")
def runMixedModel(infiles, outfile):
    '''
    Run a linear mixed-model association analysis
    on the subset individuals.
    '''

    job_threads = PARAMS['grm_threads']
    # memory per thread
    job_memory = "7G"

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    mlm_exclude = infiles[1][2]
    mlm_pheno = infiles[1][3]
    mlm_covar = infiles[1][4]
    grm_prefix = ".".join(mlm_pheno.split(".")[:-1])

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --program=gcta
    --threads=%(job_threads)s
    --input-file-format=plink_binary
    --method=lmm
    --lmm-method=standard
    --min-allele-frequency=0.01
    --grm-prefix=%(grm_prefix)s
    --remove-individuals=%(mlm_exclude)s
    --phenotypes-file=%(mlm_pheno)s
    --pheno=1
    --covariates-file=%(gwas_pca)s
    --discrete-covariates-file=%(mlm_covar)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.gcta.log
    '''

    P.run()


@follows(mkdir("plots.dir"),
         runMixedModel)
@collate(runMixedModel,
         regex("mlm.dir/(.+).mlma"),
         r"plots.dir/WholeGenome_mlm-manhattan.png")
def plotMixedModelManhattan(infiles, outfile):
    '''
    Generate a manhattan plot across each chromosome
    from the unadjusted analysis
    '''

    job_memory = "16G"

    res_files = ",".join(infiles)
    out_file = outfile.split("/")[-1]
    out_file = out_file.split("-")[0]
    out_file = "gwas.dir/" + out_file + ".results"
    statement = '''
    cgat assoc2plot
    --plot-type=manhattan
    --resolution=genome_wide
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(res_files)s
    > %(out_file)s
    '''

    P.run()


@follows(mkdir("plots.dir"),
         runMixedModel)
@collate(runMixedModel,
         regex("mlm.dir/(.+).mlma"),
         r"plots.dir/WholeGenome_mlm-qqplot.png")
def plotMixedModelQQ(infiles, outfile):
    '''
    Generate a QQ plot across all MLM results
    '''

    job_memory = "16G"

    res_files = ",".join(infiles)
    statement = '''
    cgat assoc2plot
    --plot-type=qqplot
    --resolution=genome_wide
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(res_files)s
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# these next tasks aren't strictly GWA, but they do use genome-wide
# genotying nonetheless


@follows(mergePlinkFiles,
         mkdir("fst.dir"))
@transform("genome.dir/WholeGenome.*",
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"fst.dir/\1.fst")
def getGenomeWideFst(infiles, outfile):
    '''
    Calculate Wright's F-statistic for population
    differentiation, Fst, for all SNPs.

    Use approx. independent SNP set following
    LD pruning.
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])
    job_memory = "20G"

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --summary-method=case_control_fst
    --summary-parameter=case-control
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    -v 5
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mergePlinkFiles,
         getGenomeWideFst,
         mkdir("fst.dir"))
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude"]),
           r"fst.dir/\1.fst")
def getFstByChromosome(infiles, outfile):
    '''
    Calculate Wright's F-statistic for population
    differentiation, Fst, for all SNPs.

    Use all SNPs across all chromosomes, minus
    exclusions
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    exclude_file = infiles[1][2]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --exclude-snps=%(exclude_file)s
    --summary-method=case_control_fst
    --summary-parameter="case-control"
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    -v 5
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Calculate region-wide LD using a reference population


@follows(mkdir("reference.dir"))
@transform("%s" % PARAMS['reference_pops'],
           regex("(.+)/(.+).ALL.panel"),
           r"reference.dir/\2.pop")
def selectRefPopulation(infile, outfile):
    '''
    Select the reference population from
    the panel demographics file
    '''

    job_memory = "1G"

    statement = '''
    cat %(infile)s |
    awk '{if($3 == "%(reference_select)s") {printf("%%s\\t%%s\\n", $1, $1)}}'
    > %(outfile)s
    '''

    P.run()


@follows(convertToPlink,
         mkdir("haplotypes.dir"))
@transform("plink.dir/*.bed",
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"exclusions.dir/WholeGenome.gwas_exclude"]),
           r"haplotypes.dir/\1.blocks.det")
def defineHaplotypeBlocks(infiles, outfile):
    '''
    Assign SNPs to haplotype blocks and get LD
    block positions
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    remove = infiles[1][2]

    out_pattern = ".".join(outfile.split(".")[:-2])

    job_memory = "40G"

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=estimate_haplotypes
    --memory="40G"
    --keep=%(gwas_keep)s
    --remove-individuals=%(remove)s
    --genotype-rate=0.1
    --min-allele-frequency=0.001
    --hardy-weinberg=0.00000001
    --haplotype-frequency=0.001
    --haplotype-size=1000
    --log=%(outfile)s.log
    --output-file-pattern=%(out_pattern)s
    -v 5
    %(plink_files)s
    > %(outfile)s.plink.log
    '''
    P.run()


# LD calculations need to be on defined regions,
# otherwise files are huge and a pain the bumhole to
# manipulate, process and store
# expect file names format - "ALL.chrN.*.vcf.gz"

@follows(mkdir("reference.dir"))
@transform("%s/*.vcf.gz" % PARAMS['reference_vcf'],
           regex("(.+)/ALL_chr(.+)_phase(.+)_(.+)_(.+).vcf.gz"),
           add_inputs(selectRefPopulation),
           r"reference.dir/chr\2_ref.bed")
def convertRefVcf(infiles, outfile):
    '''
    Convert a reference panel VCF file to
    plink format for LD calculation.
    Only keep variants with MAF >= 0.1%
    '''

    job_memory = "60G"

    vcf = infiles[0]
    keep = infiles[1]
    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=vcf
    --method=format
    --memory="60G"
    --format-method=change_format
    --reformat-type=plink_binary
    --min-allele-frequency=0.001
    --keep-individuals=%(keep)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(vcf)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(convertRefVcf,
         mkdir("ld.dir"))
@transform(convertRefVcf,
           regex("reference.dir/chr(.+)_ref.bed"),
           add_inputs([r"reference.dir/chr\1_ref.fam",
                       r"reference.dir/chr\1_ref.bim"]),
           r"ld.dir/chr\1.ld.gz")
def calcLd(infiles, outfile):
    '''
    Calculate LD region wide for SNPs within
    1Mb of eachother from a reference panel;
    recommend 1000 Genomes project.
    This needs to go into a database
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "5G"
    job_threads = 12

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=ld
    --ld-statistic=r2
    --ld-min=0.05
    --ld-format-output=table
    --memory="60G"
    --threads=%(job_threads)s
    --log=%(outfile)s.log
    --output-file-pattern=%(out_pattern)s
    %(plink_files)s
    > %(outfile)s.plink.log;
    '''

    P.run()


@follows(calcLd)
@transform(calcLd,
           suffix(".ld.gz"),
           ".ld.bgz")
def bgzipLdFiles(infile, outfile):
    '''
    Tabix requires block Gzipped files
    to index - zap input files to
    save on storage.
    '''

    job_memory = "1G"
    tmp_file = P.getTempFilename()
    statements = []

    statements.append('''
    zcat %(infile)s | tr -s ' ' '\\t' |
    sed 's/^[[:space:]]*//g' |
    sed 's/*[[:space:]]$//g' |
    bgzip > %(outfile)s
    ''')

    # statements.append('''
    # touch -r %(infile)s %(tmp_file)s
    # ''')

    # statements.append('''
    # touch -r %(tmp_file)s %(infile)s
    # ''')

    # statements.append('''
    # rm -rf %(tmp_file)s
    # ''')

    P.run()


@follows(bgzipLdFiles)
@transform(bgzipLdFiles,
           suffix(".ld.bgz"),
           ".ld.bgz.tbi")
def tabixIndexLd(infile, outfile):
    '''
    Use tabix to index pair-wise LD
    values on BP_B column
    '''

    job_memory = "4G"

    statement = '''
    tabix -S 1 -b 2 -e 2 %(infile)s
    '''

    P.run()


@follows(calcLd)
@transform(calcLd,
           suffix(".ld.gz"),
           ".load")
def loadLd(infile, outfile):
    '''
    Load all LD values into separate tables,
    use BP_A as the index
    '''

    job_memory = "60G"
    # ld output files need whitespace substituting for tabs
    temp_file = P.getTempFilename(shared=True)
    statement = '''
    zcat %(infile)s | tr -s " " "\\t" |
     sed 's/^[[:space:]]*//g' |
     sed 's/[[:space:]]$//g' > %(temp_file)s
    '''

    P.run()

    P.load(temp_file, outfile,
           job_memory=job_memory,
           options="--add-index=BP_A")

# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# SNP prioritisation methods <- trying to identify most likely causal SNPs for each
# association signal

# need a list of SNPs that represent independent signals <- conditional and original
# need: P-value, ORs, SE, LD, functional annotation(?)


@follows(tabixIndexLd,
         plotGenomeManhattan,
         mkdir("hit_regions.dir"))
@subdivide("gwas.dir/*_adj.results",
           regex("gwas.dir/(.+)_adj.results"),
           r"hit_regions.dir/res_sig.tsv")
def splitGwasRegions(infile, outfile):
    '''
    Split top GWAS hits into separate files
    for candidate causal SNP prioritisation
    '''

    job_memory = "5G"
    out_dir = "/".join(outfile.split("/")[:-1])

    statement = '''
    cgat assoc2assoc
    --task=get_hits
    --p-threshold=0.00000005
    --output-directory=%(out_dir)s
    --log=%(outfile)s.log
    %(infile)s
    '''

    P.run()
    P.touch(outfile)


@follows(tabixIndexLd,
         conditionalAssociation,
         splitGwasRegions)
@transform("conditional.dir/*.assoc.logistic",
           regex("conditional.dir/(.+)-(.+)-(.+).assoc.logistic"),
           r"hit_regions.dir/cond_sig.tsv")
def splitConditionalRegions(infile, outfile):
    '''
    Split top conditional analysos hits into separate files
    for candidate causal SNP prioritisation
    '''

    job_memory = "5G"
    out_dir = "/".join(outfile.split("/")[:-1])

    statement = '''
    cgat assoc2assoc
    --task=get_hits
    --p-threshold=0.000001
    --output-directory=%(out_dir)s
    --log=%(outfile)s.log
    %(infile)s
    '''

    P.run()
    P.touch(outfile)


@follows(splitGwasRegions,
         splitConditionalRegions,
         mkdir("locuszoom.dir"))
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/(.+)_(.+)_significant.tsv"),
           r"locuszoom.dir/\1_\2.metal")
def transformMetalForLocuszoom(infile, outfile):
    '''
    Transform regional results file into METAL format
    for plotting in LocusZoom
    '''

    statement = '''
    cat %(infile)s | cut -f 6,7 |
    awk 'BEGIN {printf("MarkerName\\tP\\n")}
    {if(NR > 1)
    {printf("%%s\\t%%s\\n", $2, $1)}}' |
    awk '{if($2 == 0) {printf("%%s\\t2.25074E-308\\n", $1)}
    else {print $0}}'
    > %(outfile)s
    '''

    P.run()


@follows(transformMetalForLocuszoom)
@transform("locuszoom.dir/*.metal",
           regex("locuszoom.dir/(.+)_(.+)_(.+).metal"),
           r"locuszoom.dir/\1_\2_\3.pdf")
def plotLocusZoom(infile, outfile):
    '''
    Generate high-resolution Manhattan plots
    with locuszoom.

    `locuszoom` must be in your PATH variable
    '''

    components = infile.split("/")[-1].split("_")
    snp = components[2].split(".")[0]
    start = components[1]
    chrome = components[0]
    outpattern = "_".join([chrome, start])

    # need the absolute path for locuszoom to read
    # the input file
    inpath = os.path.abspath(infile)

    job_memory = "4G"

    statement = '''
    cd  locuszoom.dir/; checkpoint;
    locuszoom
    --metal %(inpath)s
    --pvalcol P
    --build hg19
    --source 1000G_March2012
    --pop EUR
    --plotonly
    --no-date
    --flank 1500kb
    --refsnp %(snp)s
    --prefix %(outpattern)s
    > %(outpattern)s.log;
    cd ../ ;
    '''

    P.run()


@follows(splitGwasRegions,
         splitConditionalRegions,
         mkdir("snpsets.dir"))
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/(.+)_(.+)_significant.tsv"),
           r"snpsets.dir/\1_\2.snpset")
def makeSnpSets(infile, outfile):
    '''
    Write file of SNP Ids for downstream task - use
    results files
    '''

    statement = '''
    cat %(infile)s | cut -f 7 > %(outfile)s
    '''

    P.run()


@follows(makeSnpSets,
         mkdir("scores.dir"))
@transform(makeSnpSets,
           regex("snpsets.dir/chr(\d+)_(\d+)_(.+).snpset"),
           add_inputs(r"%s/chr\1.bim" % PARAMS['functional_bim_dir']),
           r"scores.dir/chr\1_\2_\3_scores.tsv")
def getSnpFunctionalScores(infiles, outfile):
    '''
    Retrieve functional scores associated with
    each SNP - used to calculate prior
    probabilities for SNP prioritisation methods
    '''

    snp_set = infiles[0]
    bim_file = infiles[1]

    job_memory = "2G"
    statement = '''
    cgat snpPriority
    --score-method=get_eigen
    --eigen-score-directory=%(functional_score_dir)s
    --snp-set=%(snp_set)s
    --log=%(outfile)s.log
    %(bim_file)s
    > %(outfile)s
    '''

    P.run()


@follows(splitGwasRegions,
         splitConditionalRegions,
         mkdir("candidate_snps.dir"))
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/(.+)_significant.tsv"),
           r"candidate_snps.dir/\1_PICS.tsv")
def calcPicsScores(infiles, outfile):
    '''
    Calculate the probabilisitc inference of causal SNPs
    score for all association signals

    Output SNPs which explain ~99% of posterior probability
    of P(B^causal | A^lead)
    '''

    snp_file = infiles
    chrome = snp_file.split("/")[-1].split("_")[0]
    chrome = chrome.lstrip("chr")

    statement = '''
    cgat snpPriority
    --score-method=PICS
    --chromosome=%(chrome)s
    --distribution=normal
    --flat-prior
    --ld-dir=%(ld_dir)s
    --ld-threshold=0.5
    --log=%(outfile)s.log
    %(snp_file)s
    > %(outfile)s
    '''

    P.run()


@follows(calcPicsScores,
         mkdir("credible_sets.dir"))
@transform(calcPicsScores,
           regex("candidate_snps.dir/(.+)_PICS.tsv"),
           r"credible_sets.dir/\1_PICS.tsv")
def makePicsCredibleSet(infile, outfile):
    '''
    Create an 80% credible set from the PICS
    posterior probabilities
    '''

    job_memory = "1G"

    statement = '''
    cgat snpPriority
    --score-method=credible_set
    --credible-interval=0.95
    --lead-snp-id=2
    --filename-separator="_"
    --snp-column=0
    --probability-column=1
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(makePicsCredibleSet)
@collate(makePicsCredibleSet,
         regex("credible_sets.dir/(.+)_PICS.tsv"),
         r"credible_sets.dir/PICS.table")
def summarisePicsResults(infiles, outfile):
    '''
    Summarise and tabulate the results from
    PICS prioritisation
    '''

    infiles = ",".join(infiles)

    statement = '''
    cgat snpPriority
    --score-method=summarise
    --log=%(outfile)s.log
    %(infiles)s
    > %(outfile)s
    '''

    P.run()


###########################
# Take this out for the moment - it isn't mature yet

# @follows(splitGwasRegions,
#          splitConditionalRegions,
#          calcPicsScores)
# @transform("hit_regions.dir/*_significant.tsv",
#            regex("hit_regions.dir/(.+)_significant.tsv"),
#            r"candidate_snps.dir/\1_LDscore.tsv")
# def calcLdScores(infile, outfile):
#     '''
#     Calculate the SE weighted effects * LDscore for all
#     association signals.

#     Select top 1% ranked variants
#     '''

#     table = outfile.split("/")[-1].split("_")[0]
#     chrome = table.strip("chr")

#     statement = '''
#     python /ifs/devel/projects/proj045/gwas_pipeline/snpPriority.py
#     --score-method=LDscore
#     --ld-dir=%(ld_dir)s
#     --ld-threshold=0.5
#     --chromosome=%(chrome)s
#     --log=%(outfile)s.log
#     %(infile)s
#     > %(outfile)s
#     '''

#     P.run()


@follows(splitGwasRegions,
         splitConditionalRegions,
         calcPicsScores)
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/(.+)_significant.tsv"),
           r"candidate_snps.dir/\1_LDranks-1pc.tsv")
def calcTop1pcLdRanks(infile, outfile):
    '''
    Take the top 1% SNPs in LD with the index
    SNP with r2 > 0.8
    '''

    table = outfile.split("/")[-1].split("_")[0]
    chrome = table.strip("chr")

    statement = '''
    cgat snpPriority
    --score-method=R2_rank
    --ld-dir=%(ld_dir)s
    --chromosome=%(chrome)s
    --rank-threshold=0.01
    --ld-threshold=0.8
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(splitGwasRegions,
         splitConditionalRegions,
         calcPicsScores,
         calcTop1pcLdRanks)
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/(.+)_significant.tsv"),
           r"candidate_snps.dir/\1_ABF.tsv")
def calcApproxBayesFactorScore(infile, outfile):
    '''
    Calculate the approximate Bayes Factor for fine-mapped
    region SNPs - returns the
    '''

    table = outfile.split("/")[-1].split("_")[0]
    chrome = table.strip("chr")

    statement = '''
    cgat snpPriority
    --score-method=ABF
    --chromosome=%(chrome)s
    --flat-prior
    --prior-variance=0.04
    --fine-map-window=1500000
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(calcApproxBayesFactorScore,
         mkdir("credible_sets.dir"))
@transform(calcApproxBayesFactorScore,
           regex("candidate_snps.dir/(.+)_ABF.tsv"),
           r"credible_sets.dir/\1_ABF.tsv")
def makeAbfCredibleSet(infile, outfile):
    '''
    Create an 95% credible set from the ABF
    posterior probabilities
    '''

    job_memory = "1G"

    statement = '''
    cgat snpPriority
    --score-method=credible_set
    --credible-interval=0.95
    --lead-snp-id=2
    --filename-separator="_"
    --snp-column=0
    --probability-column=2
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(makeAbfCredibleSet,
         summarisePicsResults)
@collate(makeAbfCredibleSet,
         regex("credible_sets.dir/(.+)_ABF.tsv"),
         r"credible_sets.dir/ABF.table")
def summariseAbfResults(infiles, outfile):
    '''
    Summarise and tabulate the results from
    approximate Bayes factor prioritisation
    '''

    infiles = ",".join(infiles)

    statement = '''
    cgat snpPriority
    --score-method=summarise
    --log=%(outfile)s.log
    %(infiles)s
    > %(outfile)s
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Perform colocalization testing between association summary statistics and additional
# summary statistics from an eQTL analysis e.g. GTex
# inputs needed: gwas summary, MAF file, trait summary
@follows(splitConditionalRegions,
         mkdir("coloc.dir"))
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/chr([0-9]+)_(\d+)_(\S+)_significant.tsv"),
           add_inputs(r"%s/chr\1_genes.tsv" % PARAMS['eqtl_dir']),
           r"coloc.dir/chr\1_\2_genes.tsv")
def selectRegionGenes(infiles, outfile):
    '''
    Select the gene symbols that lie within
    each association region to be tested for
    colocalisation.

    The input gene file needs to contain start and end
    co-ordinates.

    Genes are selected who's TSS and TSE lie within the
    boundary of the fine mapping interval, generally
    1.5Mb +/- lead SNP position.  The gene file should be
    in BED4 format, with column 4 containing gene symbols
    '''

    resfile = infiles[0]
    gene_file = infiles[1]
    start = resfile.split("/")[-1].split("_")[1]

    statement = '''
    cat %(gene_file)s |
    awk '{if(($2 >= %(start)s - 1500000) && ($3 <= %(start)s + 1500000))
    {print $0}}' | cut -f 4 | sort | uniq > %(outfile)s
    '''

    P.run()


# trait summary statistic results files need to contain
# just the variants in LD with each other, say r^2 >=0.2
# there needs to be balance so that some SNPs are still
# retained for analysis
@follows(selectRegionGenes)
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/chr([0-9]+)_(.+)_(.+)significant.tsv"),
           add_inputs(r"ld.dir/chr\1.ld.bgz"),
           r"coloc.dir/chr\1_\2_\3ldextract.tsv")
def ldExtractResults(infile, outfile):
    '''
    Extract only those variants in LD r^2 >= 0.5
    to make sure only independent association
    signals are used in the colocalisation
    analysis
    '''

    components = infile[0].split("/")[-1].rstrip("_significant.tsv")
    lead_snp = "_".join(components.split("_")[2:])
    chrome = int(components.split("_")[0].strip("chr"))
    ld_file = infile[1]
    snpos = int(components.split("_")[1])
    start = snpos - 1500000
    end = snpos + 1500000
    if start < 1:
        start = 1
    else:
        pass

    statement = '''
    tabix %(ld_file)s %(chrome)i:%(start)i-%(end)i
    | awk '{if($2 == %(snpos)i || $5 == %(snpos)i) {print $0}}'
    | awk '$7 >= %(coloc_ldthresh)s {print $0}'
    | cut -f 3,6 | sed 's/\\t/\\n/' | sort | uniq
    > %(outfile)s
    '''

    P.run()


@follows(selectRegionGenes,
         ldExtractResults)
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/chr([0-9]+)_(\d+)_(\S+)_significant.tsv"),
           add_inputs([r"%s/chr\1_eQTL.txt.gz" % PARAMS['eqtl_dir'],
                       r"coloc.dir/chr\1_\2_genes.tsv",
                       r"coloc.dir/chr\1_\2_\3_ldextract.tsv",
                       r"%s/chr\1.frq" % PARAMS['coloc_mafdir']]),
           r"coloc.dir/chr\1_\2_\3.coloc")
def colocTesteQTL(infiles, outfile):
    '''
    Perform a colocalisation test between trait summary statistics
    and eQTL result summary statistics
    '''

    trait1_results = infiles[0]
    trait2_results = infiles[1][0]
    gene_list = infiles[1][1]
    ld_extract = infiles[1][2]
    maf_table = infiles[1][3]

    job_memory = "8G"

    statement = '''
    cgat assoc2coloc
    --trait1-results=%(trait1_results)s
    --trait2-results=%(trait2_results)s
    --trait2-p-column=%(eqtl_pcol)s
    --trait1-prevalence=%(coloc_prevalence)s
    --trait2-size=%(eqtl_nsize)i
    --trait1-snplist=%(ld_extract)s
    --R-script=%(r_scripts)s
    --gene-list=%(gene_list)s
    --maf-table=%(maf_table)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Variance components analysis on GWAS hit regions and observed epistatic interactions
# Detect genetic overlap with additional traits by estimating h2 additive from GWAS and
# epistasis regions.
# get the top SNPs first


@follows(mergeGwasHits,
         plotGenomeManhattan,
         splitGwasRegions,
         mkdir("reml.dir"))
@collate("hit_regions.dir/*significant.tsv",
         regex("hit_regions.dir/(.+)_(.+)_(.+)_significant.tsv"),
         r"reml.dir/GwasHits.snps")
def getGwasTopHits(infiles, outfile):
    '''
    Take the top SNPs with association p-values
    below a given threshold.
    '''

    job_memory = "10G"
    infiles = " ".join(infiles)

    statement = '''
    cat %(infiles)s | grep -v "SNP" | cut -f 7
    > %(outfile)s
    '''

    P.run()


@follows(getGrmRegion)
@transform("epistasis.dir/*.fam",
           regex("epistasis.dir/(.+).fam"),
           add_inputs("%s" % PARAMS['gwas_keep']),
           r"reml.dir/\1.keep")
def subSampleIndividualsForReml(infiles, outfile):
    '''
    Subsample from the total population
    to get ~n individuals for linear
    model analysis.
    Output with combined list of ethnically
    selected individuals
    '''

    fam_file = infiles[0]
    keep_file = infiles[1]
    keep_temp = P.getTempFilename(shared=True)
    fam_temp = P.getTempFilename(shared=True)

    statement = '''
    cat %(keep_file)s | tr -s " " "\\t" | cut -f1,2 | sort | grep -v "FID" > %(keep_temp)s;
    cat %(fam_file)s | tr " " "\\t" | cut -f1,2 | sort > %(fam_temp)s;
    comm -12 %(fam_temp)s %(keep_temp)s | shuf -n %(mlm_subsample)s
    > %(outfile)s;
    rm -rf %(keep_temp)s %(fam_temp)s
    '''

    P.run()


@follows(mergeGwasHits,
         getGwasTopHits,
         subSampleIndividualsForReml,
         mkdir("reml.dir"))
@transform(mergeGwasHits,
           regex("epistasis.dir/(.+).bed"),
           add_inputs([r"epistasis.dir/\1.fam",
                       r"epistasis.dir/\1.bim",
                       subSampleIndividualsForReml,
                       getGwasTopHits]),
           r"reml.dir/REML_Gwas.bed")
def subsetCohortForReml(infiles, outfile):
    '''
    Subset the data prior to GRM calculation
    and REML estimation of variance components
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    keep_file = infiles[1][2]
    snp_file = infiles[1][3]
    job_memory = "40G"

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=format
    --keep-individuals=%(keep_file)s
    --extract-snps=%(snp_file)s
    --min-allele-frequency=0.001
    --format-method=change_format
    --format-parameter=%(format_gender)s
    --update-sample-attribute=gender
    --reformat-type=plink_binary
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(subsetCohortForReml)
@transform(subsetCohortForReml,
           regex("reml.dir/(.+).bed"),
           add_inputs([r"reml.dir/\1.fam",
                       r"reml.dir/\1.bim"]),
           r"reml.dir/\1.grm.N.bin")
def makeRemlGrm(infiles, outfile):
    '''
    Generate the GRM across GWAS regions for
    variance components analysis
    '''

    job_threads = PARAMS['grm_threads']
    # memory per thread
    job_memory = "10G"

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-3])

    statement = '''
    cgat geno2assoc
    --program=gcta
    --threads=%(job_threads)s
    --input-file-format=plink_binary
    --method=matrix
    --matrix-compression=bin
    --matrix-form=grm
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.gcta.log
    '''

    P.run()


@follows(makeRemlGrm)
@transform("reml.dir/*.fam",
           regex("reml.dir/(.+).fam"),
           r"reml.dir/\1.pheno")
def subsetRemlPhenotype(infile, outfile):
    '''
    Generate a phenotype file from the subset
    .fam file
    '''

    job_memory = "1G"

    statement = '''
    cat %(infile)s | tr " " "\\t" | cut -f1,2,6
    > %(outfile)s
    '''

    P.run()


@follows(makeRemlGrm,
         subsetRemlPhenotype)
@transform(makeRemlGrm,
           regex("reml.dir/(.+).grm.N.bin"),
           add_inputs([r"reml.dir/\1.grm.bin",
                       r"reml.dir/\1.grm.id",
                       r"reml.dir/\1.pheno"]),
           r"reml.dir/\1.hsq")
def calcHeritabilityReml(infiles, outfile):
    '''
    Use REML to perform variance components analysis
    and estimate genetic contribution to trait
    of interest
    '''

    n_file = infiles[0]
    bin_file = infiles[1][0]
    id_file = infiles[1][1]
    grm_files = ",".join([n_file, bin_file, id_file])

    pheno_file = infiles[1][2]

    job_threads = PARAMS['grm_threads']
    job_memory = "10G"
    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --input-file-format=GRM_binary
    --program=gcta
    --phenotypes-file=%(pheno_file)s
    --pheno=1
    --covariates-file=%(mlm_cont_covarfile)s
    --discrete-covariates-file=%(mlm_discrete_covarfile)s
    --method=reml
    --reml-method=BLUP_EBV
    --prevalence=%(reml_prevalence)f
    --threads=%(job_threads)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(grm_files)s
    > %(outfile)s.gcta.log
    '''

    P.run()


@follows(calcHeritabilityReml)
@transform("reml.dir/*.indi.blp",
           regex("reml.dir/(.+).indi.blp"),
           add_inputs([r"reml.dir/\1.bed",
                       r"reml.dir/\1.fam",
                       r"reml.dir/\1.bim"]),
           r"reml.dir/\1.snp.blp")
def snpBlup(infiles, outfile):
    '''
    SNP BLUP <- phenotypic variance explained
    by each SNP that went into the GRM
    '''

    blup_file = infiles[0]
    bed_file = infiles[1][0]
    fam_file = infiles[1][1]
    bim_file = infiles[1][2]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-2])

    job_memory = "20G"

    statement = '''
    cgat geno2assoc
    --program=gcta
    --input-file-format=plink_binary
    --method=reml
    --reml-method=snpBLUP
    --reml-parameters=%(blup_file)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.gcta.log
    '''

    P.run()


@follows(makeRemlGrm)
@transform("reml.dir/*.fam",
           regex("reml.dir/(.+).fam"),
           add_inputs("%s" % PARAMS['reml_phenos']),
           r"reml.dir/\1_All.pheno")
def subsetAllPhenotypes(infile, outfile):
    '''
    Subset the phenotypes file based
    on the Plink .fam file
    '''

    job_memory = "5G"

    fam_file = infiles[0]
    pheno_file = infiles[1]

    statement = '''
    cgat pheno2pheno
    --task=subset_phenotypes
    --fam-file=%(fam_file)s
    --log=%(outfile)s.log
    %(pheno_file)s
    > %(outfile)s
    '''

    P.run()


@follows(makeRemlGrm,
         subsetAllPhenotypes)
@transform(makeRemlGrm,
           regex("reml.dir/(.+).grm.N.bin"),
           add_inputs([r"reml.dir/\1.grm.bin",
                       r"reml.dir/\1.grm.id",
                       "%s" % PARAMS['reml_phenos']]),
           r"reml.dir/\1.hsq")
def calcBivariateReml(infiles, outfile):
    '''
    Use REML to perform variance components analysis
    and estimate genetic contribution to two
    traits of interest
    '''

    n_file = infiles[0]
    bin_file = infiles[1][0]
    id_file = infiles[1][1]
    grm_files = ",".join([n_file, bin_file, id_file])

    pheno_file = infiles[1][2]

    job_threads = PARAMS['grm_threads']
    job_memory = "10G"
    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --input-file-format=GRM_binary
    --program=gcta
    --phenotypes-file=%(pheno_file)s
    --pheno=1
    --covariates-file=%(mlm_cont_covarfile)s
    --discrete-covariates-file=%(mlm_discrete_covarfile)s
    --method=reml
    --reml-method=BLUP_EBV
    --prevalence=0.05
    --threads=%(job_threads)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(grm_files)s
    > %(outfile)s.gcta.log
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
############################
# Joint analysis of traits #
############################
# Need to finish this or put it in a separate pipeline
@follows(plotGenomeManhattan,
         mkdir("joint_analysis.dir"))
@transform("gwas.dir/*.results",
           regex("gwas.dir/(.+)_adj.results"),
           r"joint_analysis.dir/\1.cojo")
def transformGwasToCojo(infile, outfile):
    '''
    Process output from Plink GWAS into the required format
    for a conditional and joint analysis using GCTA
    '''

    job_memory = "75G"
    job_threads = 1

    statement = '''
    cgat assoc2assoc
    --task=merge_freq
    --frequency-directory=%(joint_freq_dir)s
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#

# DISSECT testing tasks


@follows(mkdir("dissect.dir"),
         mkdir("grm.dir"))
@transform("data.dir/subsample_small.bed",
           regex("data.dir/(.+)_small.bed"),
           r"grm.dir/\1.dat")
def test_dissectGrmSingleFile(infiles, outfile):
    '''
    Test DISSECT for parallelised analysis
    Make a GRM, then do PCA
    MUST have a separate phenotypes file with no header, but in plink format
    for GWAS
    '''

    # memory errors with 3G - bump it up to 12G
    # using more processes may also relieve some of the
    # MEMORY ERRORS WITH 12G!!! WTF?? - more processes?? > 300?
    # memory requirements - try it with 192
    # if memory requirements are too high, jobs fail
    # increase the number of processes to spread the job out
    job_memory = "8G"
    infiles = ",".join(infiles)
    job_threads = 128
    # job_queue = "mpi.q"

    job_queue = ",".join(["all.q@cgat001", "all.q@cgat002", "all.q@cgat003", "all.q@cgat004",
                          "all.q@cgat005", "all.q@cgat006", "all.q@cgat007", "all.q@cgat008",
                          "all.q@cgat009", "all.q@cgat010", "all.q@cgat011", "all.q@cgat012",
                          "all.q@cgat013", "all.q@cgat014", "all.q@cgat101", "all.q@cgat102",
                          "all.q@cgat103", "all.q@cgat104", "all.q@cgat105", "all.q@cgat106",
                          "all.q@cgat107", "all.q@cgat108", "all.q@cgat109", "all.q@cgat110",
                          "all.q@cgat111", "all.q@cgat112", "all.q@cgat113", "all.q@cgat114",
                          "all.q@cgat115", "all.q@cgat116", "all.q@cgatgpu1", "all.q@cgatsmp1"])

    cluster_parallel_environment = " mpi "
    statement = '''
    mpirun
    dissect
    --bfile data.dir/subsample
    --make-grm
    --out grm.dir/subsample
    > dissect_test.log
    '''

    P.run()


@follows(mkdir("dissect.dir"),
         mkdir("grm.dir"))
@transform("data.dir/subsample_list.tsv",
           regex("data.dir/(.+)_list.tsv"),
           r"grm.dir/\1.dat")
def test_dissectGrmManyFiles(infile, outfile):
    '''
    Test DISSECT for parallelised analysis
    Make a GRM, then do PCA
    MUST have a separate phenotypes file with no header, but in plink format
    for GWAS
    '''

    # get the sample list from PARAMS
    # for chr10-9 + chr1, estimated memory usage was 4GB per process
    # failing on 4G memory per node
    job_memory = "3G"
    job_threads = 300
    job_queue = "mpi.q"
    cluster_parallel_environment = " mpi "

    out_pattern = ".".join(outfile.split(".")[:-1])
    statement = '''
    mpirun  --np 1
    dissect
    --bfile-list %(infile)s
    --make-grm
    --grm-join-method 0
    --out %(out_pattern)s
    > dissect_test.log
    '''

    P.run()


@follows(test_dissectGrmManyFiles,
         mkdir("pca.dir"))
@transform("grm.dir/*.dat",
           regex("grm.dir/(.+).dat"),
           r"pca.dir/\1.pca.eigenvalues")
def dissectPCA(infile, outfile):
    '''
    Test DISSECT PCA on grms
    '''
    job_memory = "2G"
    infiles = ",".join(infiles)
    job_threads = 128
    job_options = "-l h=!andromeda,h=!gandalf,h=!saruman"
    job_queue = "all.q"
    cluster_parallel_environment = " mpi "
    statement = '''
    mpirun
    dissect
    --pca
    --grm grm.dir/wholegenome
    --out pca.dir/wholegenome
    --num-eval 20
    > dissect_test.log
    '''

    P.run()

# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Pipeline targets


@follows(runFilteredPCA,
         plotPcaResults,
         excludeInbred,
         excludeRelated,
         plotIbdHistogram,
         mergeExclusions)
def QC():
    pass


@follows(QC,
         mergeCovariates,
         pcAdjustedAssociation,
         plotGenomeManhattan)
def GWAS():
    pass


@follows(QC,
         GWAS,
         getConditionalRegions,
         conditionalAssociation)
def Conditional():
    pass


@follows(Conditional,
         dissectPCA)
def ParallelMLM():
    pass


@follows(QC)
def Evo():
    pass


@follows(QC,
         GWAS,
         Conditional,
         ParallelMLM,
         Evo)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
