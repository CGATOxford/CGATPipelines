"""===========================
Pipeline GWAS
===========================


.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_gwas.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* gcta >= 1.25.0
* plink >= 1.9
* bolt-lmm >= 2.1
* smartpca >= 1.0
* goshifter >= 1.0
* gcta >= 1.25

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *
from ruffus.combinatorics import *

import sys
import os
import re
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh

# ---------------------------------------------------
# load the UKBiobank phenotype data into an SQLite DB
# use this as the main accessor of phenotype data
# for the report and non-genetic analyses


@follows(mkdir("phenotypes.dir"),
         mkdir("%s" % PARAMS['plots_dir']))
@transform("%s/%s*.tab" % (PARAMS['data_dir'],
                           PARAMS['data_prefix']),
           regex("%s/(.+).tab" % PARAMS['data_dir']),
           r"phenotypes.dir/\1.tsv")
def formatPhenotypeData(infiles, outfile):
    '''
    Use the UKBiobank encoding dictionary/R script to
    set the factor levels for phenotype data.
    Output is in plink covariate file format
    '''

    pheno_file = infiles

    statement = '''
    cgat pheno2pheno
    --task=plink_format
    --id-variable=%(data_id_var)s
    --log=%(outfile)s.log
    %(pheno_file)s
    > %(outfile)s
    '''

    P.run()


@follows(formatPhenotypeData)
@transform(formatPhenotypeData,
           suffix(".tsv"),
           ".load")
def loadPhenotypes(infile, outfile):
    '''
    load all phenotype data in to an SQLite DB
    '''

    P.load(infile, outfile)


@follows(loadPhenotypes)
@transform(formatPhenotypeData,
           regex("phenotypes.dir/(.+).tsv"),
           r"phenotypes.dir/\1_British.tsv")
def selectBritish(infile, outfile):
    '''
    Select only those individuals with a white British
    ethnicity
    '''

    statement = '''
    cgat pheno2pheno
    --task=select_ethnicity
    --ethnicity-id=%(format_ethnicity_var)s
    --ethnicity-label=%(format_ethnicity)s
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(selectBritish)
@transform(selectBritish,
           suffix("_British.tsv"),
           ".keep")
def makeKeepFile(infile, outfile):
    '''
    make a samples.keep file for filtering
    on individuals
    '''

    statement = '''
    cat %(infile)s | awk '{if(NR > 1) {printf("%%s\\t%%s\\n", $1, $2)}}'
    > %(outfile)s
    '''

    P.run()


@follows(loadPhenotypes,
         selectBritish)
@transform(selectBritish,
           regex("phenotypes.dir/(.+)_British.tsv"),
           r"phenotypes.dir/\1.pheno")
def dichotimisePhenotype(infile, outfile):
    '''
    Dichotomise a phenotype for association testing
    '''

    statement = '''
    cgat pheno2pheno
    --task=dichotimise_phenotype
    --pheno-id=%(data_dichot_var)s
    --reference-variable=%(data_reference_value)s
    --missing-var-label=%(data_missing_label)s
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(loadPhenotypes,
         mkdir("plots.dir"))
@transform(formatPhenotypeData,
           regex("phenotypes.dir/(.+).tsv"),
           r"plots.dir/1\_phenotype.png")
def plotPhenotypeData(infile, outfile):
    '''
    Generare plots of phenotype distributions
    for CGATReport document
    '''


@follows(loadPhenotypes,
         plotPhenotypeData)
@transform(formatPhenotypeData,
           regex("phenotypes.dir/(.+).tsv"),
           r"plots.dir/\1_map_%s.png" % PARAMS['phenotype_map_overlay'])
def plotPhenotypeMap(infile, outfile):
    '''
    Plot an overlay of a phenotype by geographical distribution
    onto a map of the UK
    '''

    job_memory = "4G"

    statement = '''
    cgat pheno2plot
    --plot-type=map
    -x %(phenotype_map_overlay)s
    --coordinate-file=%(phenotype_coord_file)s
    --coords-id-col=%(phenotype_id_coords)s
    --lattitude-column=%(phenotype_lat)s
    --longitude-column=%(phenotype_long)s
    --xvar-labels=%(phenotype_xlabels)s
    --reference-value=%(phenotype_ref_value)s
    --var-type=categorical
    --log=%(outfile)s.log
    --output-file=%(outfile)s
    %(infile)s
    '''

    P.run()

# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Specific pipeline tasks
# need to allow for extra characters after chromsome ID


@follows(mkdir("plink.dir"),
         dichotimisePhenotype)
@collate("%s/*.%s" % (PARAMS['data_dir'],
                      PARAMS['data_suffix']),
         regex("%s/(.+)(\d+)(.+)\.%s" % (PARAMS['data_dir'],
                                         PARAMS['data_suffix'])),
         add_inputs(r"%s/\1\2\3.%s" % (PARAMS['data_dir'],
                                       PARAMS['data_aux'])),
         r"plink.dir/\1\2.bed")
def convertToPlink(infiles, outfiles):
    '''
    Convert from other format
    to Plink binary format.  One bed file
    per chromosome - keep the fam files the same
    '''

    job_memory = "60G"
    infiles = ",".join([x for x in infiles[0]])

    log_out = ".".join(outfiles.split(".")[:-1])
    out_pattern = ".".join(outfiles.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=%(data_format)s
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --update-sample-attribute=gender
    --format-parameter=%(format_gender)s
    --method=format
    --memory="60G"
    --format-method=change_format
    --reformat-type=plink_binary
    --output-file-pattern=%(out_pattern)s
    --log=%(log_out)s.log
    %(infiles)s
    '''

    P.run()

# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# some variants have missing ID names - change these with the following
# structure:
# chr_bp_A1_A2


@follows(convertToPlink)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim"]),
           r"plink.dir/\1.exclude")
def nameVariants(infiles, outfile):
    '''
    Some variants have missing file convert these to
    have ID structure: chr_bp_A1_A2 instead - update the
    relevant bim file and generate a list of variants
    to exclude - triallelic, duplicates and overlapping
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    # temporary file name
    temp_file = P.getTempFilename(shared=True)

    job_memory = "2G"

    # use awk on the .bim file to generate replacement IDs

    state0 = '''
    cat %(bim_file)s | awk '{if($2 == ".") {printf("%%s\\t%%s_%%s_%%s_%%s\\t%%s\\t%%s\\t%%s\\t%%s\\n",
    $1,$1,$4,$5,$6,$3,$4,$5,$6)} else{print $0}}' > %(temp_file)s.bim;
    mv %(temp_file)s.bim %(bim_file)s
    '''

    # create files to remove triallelic variants, overlapping variants
    # and duplicates
    state1 = '''
    cgat geno2geno
    --task=detect_duplicates
    --outfile-pattern=%(temp_file)s
    --log=%(outfile)s.log
    %(bim_file)s;
    cat %(temp_file)s.triallelic %(temp_file)s.duplicates
    %(temp_file)s.overlapping | sort | uniq >> %(outfile)s
    '''

    state2 = '''
    touch %(outfile)s
    '''

    statement = ";".join([state0, state1, state2])
    P.run()

# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# QC tasks on genotype and samples

# genotype filtering tasks:
# snp genotyping rate
# individual missingness
# individual heterozygosity
# individual relatedness - IBD
# gender check - reported vs genetic gender
# PCA on set of LD pruned SNPs - compare to self-reported ethnicity
# perform each task independently and create exclusion lists. Remove
# all individuals and SNPs at the end.

# GRM will give relatedness and inbreeding at the same time

# First step is to produce a complete LD pruned list of SNPs
# this is slow for lots of SNPs and samples!


@follows(mkdir("QC.dir"),
         nameVariants)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude"]),
           r"QC.dir/\1_round1.prune.in")
def ldPruneSNPsRound1(infiles, outfile):
    '''
    LD prune SNPs to create a list of independent SNPs
    genome-wide.  To be used for PCA, GRM, inbreeding,
    and heterozygosity estimation
    '''

    # this needs to be performed multiple times to get
    # a small enough set of SNPS ~ 10k-50k

    # this selects entirely INDELS if not MAF cut-off
    # is used.  Evidently LD is low between indels,
    # but not between SNPs and indels.  How many
    # INDELs are on the Affy array, and how many
    # are imputed?   What is their imputation accuracy?
    # this is causing relatedness to be massively
    # overestimated!!
    # devel version allows --keep and --remove
    # use plinkdev for development version

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    exclude_file = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "6G"
    job_threads = 10

    statement = '''
    cgat geno2qc
    --program=plinkdev
    --input-file-format=plink_binary
    --exclude-snps=%(exclude_file)s
    --method=ld_prune
    --keep=%(gwas_keep)s
    --use-kb
    --memory=60G
    --ignore-indels
    --genotype-rate=0.1
    --hardy-weinberg=1e-5
    --min-allele-frequency=0.01
    --prune-method=%(ld_prune_method)s
    --step-size=%(ld_prune_step)s
    --window-size=%(ld_prune_window)s
    --threshold=%(ld_prune_threshold)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mkdir("QC.dir"),
         nameVariants,
         ldPruneSNPsRound1)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude",
                       r"QC.dir/\1_round1.prune.in"]),
           r"QC.dir/\1_round2.prune.in")
def ldPruneSNPsRound2(infiles, outfile):
    '''
    LD prune SNPs to create a list of independent SNPs
    genome-wide.  To be used for PCA, GRM, inbreeding,
    and heterozygosity estimation
    '''

    # this needs to be performed multiple times to get
    # a small enough set of SNPS ~ 10k-50k

    # this selects entirely INDELS if not MAF cut-off
    # is used.  Evidently LD is low between indels,
    # but not between SNPs and indels.  How many
    # INDELs are on the Affy array, and how many
    # are imputed?   What is their imputation accuracy?
    # this is causing relatedness to be massively
    # overestimated!!
    # devel version allows --keep and --remove
    # use plinkdev for development version

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    exclude_file = infiles[1][2]
    include_file = infiles[1][3]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "6G"
    job_threads = 10
    prune_threshold = PARAMS['ld_prune_threshold'] - 0.10

    statement = '''
    cgat geno2qc
    --program=plinkdev
    --input-file-format=plink_binary
    --exclude-snps=%(exclude_file)s
    --extract-snps=%(include_file)s
    --method=ld_prune
    --keep=%(gwas_keep)s
    --use-kb
    --memory=60G
    --ignore-indels
    --min-allele-frequency=0.01
    --genotype-rate=0.1
    --hardy-weinberg=1e-5
    --prune-method=%(ld_prune_method)s
    --step-size=%(ld_prune_step)s
    --window-size=%(ld_prune_window)s
    --threshold=%(prune_threshold)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mkdir("QC.dir"),
         nameVariants,
         ldPruneSNPsRound2)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude",
                       r"QC.dir/\1_round2.prune.in"]),
           r"QC.dir/\1.prune.in")
def ldPruneSNPsRound3(infiles, outfile):
    '''
    LD prune SNPs to create a list of independent SNPs
    genome-wide.  To be used for PCA, GRM, inbreeding,
    and heterozygosity estimation
    '''

    # this needs to be performed multiple times to get
    # a small enough set of SNPS ~ 10k-50k

    # this selects entirely INDELS if not MAF cut-off
    # is used.  Evidently LD is low between indels,
    # but not between SNPs and indels.  How many
    # INDELs are on the Affy array, and how many
    # are imputed?   What is their imputation accuracy?
    # this is causing relatedness to be massively
    # overestimated!!
    # devel version allows --keep and --remove
    # use plinkdev for development version

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    exclude_file = infiles[1][2]
    include_file = infiles[1][3]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "6G"
    job_threads = 10
    prune_threshold = PARAMS['ld_prune_threshold'] - 0.18

    statement = '''
    cgat geno2qc
    --program=plinkdev
    --input-file-format=plink_binary
    --exclude-snps=%(exclude_file)s
    --extract-snps=%(include_file)s
    --method=ld_prune
    --keep=%(gwas_keep)s
    --use-kb
    --memory=60G
    --ignore-indels
    --min-allele-frequency=0.01
    --genotype-rate=0.1
    --hardy-weinberg=1e-5
    --prune-method=%(ld_prune_method)s
    --step-size=%(ld_prune_step)s
    --window-size=%(ld_prune_window)s
    --threshold=%(prune_threshold)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(ldPruneSNPsRound3,
         mkdir("genome.dir"))
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"QC.dir/\1.prune.in"]),
           r"genome.dir/\1_sparse.bed")
def makeTrimmedData(infiles, outfile):
    '''
    Filter on the LD pruned set of SNPs to
    create smaller, sparser genotype files
    Remove duplicates first
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    snps_file = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    outpattern = ".".join(outfile.split(".")[:-1])
    job_memory = "60G"
    job_threads = 1

    tmpfile = P.getTempFilename(shared=True)

    # find duplicates
    state1 = '''
    cgat geno2geno
    --log=%(outfile)s.exclude.log
    --task=detect_duplicates
    --outfile-pattern=%(tmpfile)s
    %(bim_file)s
    '''

    exclude_file = tmpfile + ".exclude"

    state2 = '''
    cat %(tmpfile)s.triallelic %(tmpfile)s.duplicates
    %(tmpfile)s.overlapping | sort | uniq >> %(exclude_file)s
    '''

    state3 = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=format
    --format-method=change_format
    --reformat-type=plink_binary
    --extract-snps=%(snps_file)s
    --exclude-snps=%(exclude_file)s
    --memory=%(job_memory)s
    --log=%(outfile)s.log
    --output-file-pattern=%(outpattern)s
    --threads=%(job_threads)i
    %(plink_files)s
    '''

    statement = ";".join([state1, state2, state3])

    P.run()


@follows(makeTrimmedData)
@collate(makeTrimmedData,
         regex("genome.dir/(.+)_sparse.bed"),
         add_inputs([r"genome.dir/\1_sparse.fam",
                     r"genome.dir/\1_sparse.bim"]),
         r"genome.dir/WholeGenome.bed")
def mergePlinkFiles(infiles, outfile):
    '''
    Merge all of the LD pruned files together
    to form a set of LD-independent genome-wide
    genotyping files for downstream sample QC
    '''

    # generate text file that contains the names of the files
    # to be merged
    temp_file = P.getTempFilename(shared=True)

    # not all multi-allelic SNPs have been removed properly
    # include the *.missnp file as an exclusion

    outpattern = ".".join(outfile.split(".")[:-1])
    job_memory = "64G"
    job_threads = 1
    with open(temp_file, "w") as ofile:
        for ifile in infiles:
            ifile_bed = ifile[0]
            ifile_fam = ifile[1][0]
            ifile_bim = ifile[1][1]
            ofile.write("%s\t%s\t%s\n" % (ifile_bed, ifile_bim, ifile_fam))

    statement = '''
    plink2
    --merge-list %(temp_file)s
    --threads %(job_threads)i
    --out %(outpattern)s;
    rm -rf %(temp_file)s
    '''

    P.run()


@follows(mergePlinkFiles)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"QC.dir/\1.het.gz")
def calcInbreeding(infiles, outfile):
    '''
    Detect individuals with excess of heterozygosity - indicative
    of population outbreeding/admixture and/or genotyping
    errors
    Also relevant to inbreeding, i.e. depletion of heterozygosity
    indicates individuals more related to themselves than expected
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    temp_file = P.getTempFilename(shared=True)

    job_memory = "4G"
    statement = '''
    cgat geno2qc
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --keep-individuals=%(gwas_keep)s
    --summary-method=inbreeding
    --summary-parameter=gz
    --output-file-pattern=%(temp_file)s
    --log=%(outfile)s.log
    %(plink_files)s;
    zcat %(temp_file)s.het.gz | tr -s ' ' '\\t' |
    sed -E 's/^[[:space:]]|[[:space:]]$//g' | gzip > %(outfile)s
    '''

    P.run()


@follows(calcInbreeding)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"QC.dir/\1.ibc")
def findExcessHomozygotes(infiles, outfile):
    '''
    Use the Plink2/GCTA to calculate inbreeding coefficients
    across all individuals expected, i.e. inbred
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-1])

    job_memory = "4G"
    statement = '''
    cgat geno2qc
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --summary-method=inbreeding_coef
    --keep-individuals=%(gwas_keep)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    '''

    P.run()


@follows(mkdir("QC.dir"))
@transform("%s/chrX*" % PARAMS['data_dir'],
           regex("%s/(.+).bed" % PARAMS['data_dir']),
           add_inputs([r"%s/\1.fam" % PARAMS['data_dir'],
                       r"%s/\1.bim" % PARAMS['data_dir'],
                       r"%s" % PARAMS['qc_pseudo_autosomal']]),
           r"QC.dir/\1.sexcheck")
def genderChecker(infiles, outfile):
    '''
    Check self-reported gender against X-chromosome
    inferred gender.

    Input data are plink binary format with the XY
    pseudoautosomal region removed.  This is required
    for the Plink1.9 gender checking function to work
    properly.

    Output a list of discordant individuals
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    pseudoautosome = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-1])
    job_memory = "6G"
    job_threads = 1

    tmp_file = P.getTempFilename(shared=True)

    state1 = '''
    cgat geno2qc
    --program=plink2
    --input-file-format=plink_binary
    --method=check_gender
    --memory=%(job_memory)s
    --exclude-snps=%(pseudoautosome)s
    --keep-individuals=%(gwas_keep)s
    --output-file-pattern=%(tmp_file)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink2.log
    '''

    # swap all spaces for a single tab and remove
    # leading and trailing spaces
    state2 = '''
    cat %(tmp_file)s.sexcheck | tr -s ' ' '\t' |
    sed 's/^[[:space:]]*//g' | sed 's/*[[:space:]]$//g'
    > %(outfile)s
    '''

    statement = ";".join([state1, state2])

    P.run()


@follows(mergePlinkFiles,
         mkdir("grm.dir"))
@transform("genome.dir/WholeGenome.*",
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"grm.dir/\1.grm.N.bin")
def makeGRM(infiles, outfiles):
    '''
    Calculate the realised GRM across all LD trimmed
    variants
    Use parallelisation
    '''

    job_threads = PARAMS['grm_threads']
    # memory per thread
    job_memory = "12G"

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfiles.split(".")[:-3])

    # why does GCTA keep throwing memory errors??
    statement = '''
    cgat geno2assoc
    --program=plink2
    --parallel=%(job_threads)s
    --input-file-format=plink_binary
    --keep-individuals=%(gwas_keep)s
    --memory="120G"
    --method=matrix
    --matrix-compression=bin
    --matrix-form=grm
    --output-file-pattern=%(out_pattern)s
    --log=%(outfiles)s.log
    %(plink_files)s
    > %(outfiles)s.gcta.log
    '''

    P.run()


@follows(mergePlinkFiles)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"QC.dir/\1.genome.gz")
def calculateIdentityByDescent(infiles, outfile):
    '''
    Calculate the pair-wise estimates of IBS/IBD
    between individuals.  This will be used to
    flag related individuals
    '''

    job_memory = "20G"
    job_threads = 12

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2qc
    --program=plink2
    --input-file-format=plink_binary
    --keep-individuals=%(gwas_keep)s
    --parallel=%(job_threads)s
    --memory="240G"
    --threads=%(job_threads)s
    --method=IBD
    --IBD-parameter=norm
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mergePlinkFiles,
         mkdir("pca.dir"))
@transform("genome.dir/WholeGenome.*",
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.bim",
                       r"genome.dir/\1.fam"]),
           r"pca.dir/\1_naive.pcs")
def runNaivePCA(infiles, outfile):
    '''
    flashPCA breaks with too many SNPs.
    '''

    job_threads = PARAMS['pca_threads']
    job_memory = PARAMS['pca_memory']

    bed_file = infiles[0]
    bim_file = infiles[1][0]
    fam_file = infiles[1][1]
    plink_root = ".".join(bed_file.split(".")[:-1])
    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    flashpca --numthreads %(job_threads)i
             --bfile %(plink_root)s
             --ndim 20
             --mem low
             --outpc %(out_pattern)s.pcs
             --outvec %(out_pattern)s.eigenvec
             --outload %(out_pattern)s.loadings
             --outval %(out_pattern)s.eigenval
             --outpve %(out_pattern)s.pve
    '''

    P.run()


@follows(runNaivePCA,
         mkdir("pca.dir"))
@transform("genome.dir/WholeGenome.*",
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"pca.dir/\1_filtered.pcs")
def runFilteredPCA(infiles, outfile):
    '''
    Pre-filter samples for ethnicity.

    flashPCA must be in the PATH variable
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])
    temp_out = P.getTempFilename(shared=True)

    statement1 = '''
    cgat geno2assoc
    --program=plinkdev
    --input-file-format=plink_binary
    --method=format
    --keep-individuals=%(gwas_keep)s
    --format-method=change_format
    --reformat-type=plink_binary
    --output-file-pattern=%(temp_out)s
    --log=%(outfile)s.log
    %(plink_files)s
    '''

    out_pattern = ".".join(outfile.split(".")[:-1])
    job_threads = PARAMS['pca_threads']

    statement2 = '''
    flashpca --numthreads %(job_threads)i
             --bfile %(temp_out)s
             --ndim 20
             --mem low
             --outpc %(out_pattern)s.pcs
             --outvec %(out_pattern)s.eigenvec
             --outload %(out_pattern)s.loadings
             --outval %(out_pattern)s.eigenval
             --outpve %(out_pattern)s.pve
    '''

    statement = " ; ".join([statement1,
                            statement2])

    P.run()


@follows(runNaivePCA,
         runFilteredPCA)
@transform([runNaivePCA,
            runFilteredPCA],
           regex("pca.dir/(.+)_(.+).pcs"),
           add_inputs([PARAMS['data_pheno_all'],
                       r"genome.dir/\1.fam"]),
           r"plots.dir/\1_\2-PC1vsPC2.png")
def plotPcaResults(infiles, outfile):
    '''
    Generate pairwise plot of the first
    2 principal components
    '''

    pcs_file = infiles[0]
    phenotypes = infiles[1][0]
    fam_file = infiles[1][1]

    statement = '''
    cgat pheno2plot
    --plot-type=pca
    --plot-n-pc=2
    --metadata-file=%(phenotypes)s
    --fam-file=%(fam_file)s
    --group-labels=%(format_ethnicity_var)s
    --log=%(outfile)s.log
    --output-file=%(outfile)s
    %(pcs_file)s
    '''

    P.run()


###############################################
# Parse QC files to get a list of individuals #
# to exclude from analyses                    #
###############################################


@follows(genderChecker,
         mkdir("exclusions.dir"))
@transform(genderChecker,
           regex("QC.dir/(.+).sexcheck"),
           r"exclusions.dir/gender_exclusion.txt")
def excludeDiscordantGender(infile, outfile):
    '''
    Make a list of individuals with discordant gender
    from X chromsome data vs. self-reported gender
    '''

    job_memory = "2G"
    statement = '''
    cgat qcs2qc
    --task=discordant_gender
    --gender-check-file=%(infile)s
    --plotting-path=%(plots_dir)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(findExcessHomozygotes,
         excludeDiscordantGender)
@transform(findExcessHomozygotes,
           regex("QC.dir/(.+).ibc"),
           r"exclusions.dir/\1.inbred")
def excludeInbred(infile, outfile):
    '''
    Flag up individuals with high inbreeding
    coefficients based on a threshold.

    Most human populations have F < 0.05, but
    this may be greatly affect by Ne.

    Use GCTA's Fhat3 as an unbiased estimator
    of F.
    '''

    job_memory = "2G"
    statement = '''
    cgat qcs2qc
    --task=find_inbreds
    --inbreeding-coef-file=%(infile)s
    --inbreeding-coefficient=Fhat3
    --inbred-cutoff=%(qc_inbreed_threshold)0.3f
    --plotting-path=%(plots_dir)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(calcInbreeding)
@transform(calcInbreeding,
           regex("QC.dir/(.+).het.gz"),
           r"exclusions.dir/\1.het_exclude")
def findExcessHeterozygotes(infile, outfile):
    '''
    Calculate the heterozygosity rate and flag individuals
    with excess heterozygosity indicative of population
    admixture or genotyping errors/contamination.

    Also flag individuals with high inbreeding coefficient.
    Plot these - if there are multiple clusters - indicates
    additional populations of individuals - see UKBiobank
    documentation for details
    '''

    job_memory = "2G"
    statement = '''
    cgat qcs2qc
    --task=flag_hets
    --heterozygotes-file=%(infile)s
    --plotting-path=%(plots_dir)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(mergePlinkFiles)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"exclusions.dir/\1.rel.id")
def excludeRelated(infiles, outfile):
    '''
    Find and exclude related individuals with IBD
    >= a threshold. Recommend IBD <= 3rd cousins
    (IBD >= 0.03125) - this can be parallelised

    Plink2 recommend using the binary genotype
    files as input, not the previously computed
    GRM. See here for details:
    https://www.cog-genomics.org/plink2/distance#rel_cutoff

    # Plink seems to be over doing the trimming of
    # individuals based on --rel-cutoff <- needs
    # further investigation.
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    job_memory = "10G"
    job_threads = 24
    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])

    statement = '''
    cgat geno2qc
    --program=plink2
    --input-file-format=plink_binary
    --method=remove_relations
    --threshold=%(relationship_cutoff)s
    --memory=240G
    --threads=%(job_threads)s
    --keep-individuals=%(gwas_keep)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(excludeRelated)
@transform(excludeRelated,
           regex("exclusions.dir/(.+).rel.id"),
           add_inputs(r"genome.dir/\1.fam"),
           r"exclusions.dir/\1.related")
def flagRelated(infiles, outfile):
    '''
    Diff the input .fam and the unrelated individuals
    from the --rel-cutoff to get a set of related
    individuals to exclude
    '''

    unrelated = infiles[0]
    fam_file = infiles[1]

    job_memory = "1G"
    to_cluster = False
    statement = '''
    cat %(fam_file)s | tr -s ' ' '\\t' | cut -f 1,2
    | diff %(unrelated)s - | grep ">" | sed 's/>//g'
    > %(outfile)s
    '''

    P.run()


@follows(calculateIdentityByDescent)
@transform(calculateIdentityByDescent,
           regex("QC.dir/(.+).genome.gz"),
           r"plots.dir/IBD-hist.png")
def plotIbdHistogram(infile, outfile):
    '''
    plot the distribution of IBD estimates
    '''

    job_memory = "300G"

    statement = '''
    cgat qcs2qc
    --task=flag_relations
    --relationship-file=%(infile)s
    --ibs-cutoff=%(relationship_cutoff)s
    --plotting-path=plots.dir
    --log=%(outfile)s.log
    '''

    P.run()

# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Association testing and other statistical testing tasks


if PARAMS['candidate_region']:
    @follows(convertToPlink,
             mkdir("candidate.dir"))
    @transform(convertToPlink,
               regex("plink.dir/%s(.+).bed" % PARAMS['candidate_chromosome']),
               add_inputs([r"plink.dir/%s\1.fam" % PARAMS['candidate_chromosome'],
                           r"plink.dir/%s\1.bim" % PARAMS[
                               'candidate_chromosome'],
                           r"plink.dir/%s\1.exclude" % PARAMS[
                               'candidate_chromosome'],
                           r"exclusions.dir/WholeGenome.gwas_exclude"]),
               r"candidate.dir/%s\1-candidate_region.bed" % PARAMS['candidate_chromosome'])
    def getCandidateRegion(infiles, outfile):
        '''
        Pull out genotyping data on individuals over a
        candidate region for testing.
        '''

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        exclude = infiles[1][2]
        gwas_exclude = infiles[1][3]
        region = ",".join(PARAMS['candidate_region'].split(":")[-1].split("-"))
        out_pattern = ".".join(outfile.split(".")[:-1])

        statement = '''
        cgat geno2assoc
        --program=plink2
        --input-file-format=plink_binary
        --method=format
        --keep-individuals=%(gwas_keep)s
        --remove-individuals=%(gwas_exclude)s
        --restrict-chromosome=%(candidate_chromosome)s
        --snp-range=%(region)s
        --exclude-snps=%(exclude)s
        --format-method=change_format
        --format-parameter=%(format_gender)s
        --update-sample-attribute=gender
        --reformat-type=plink_binary
        --output-file-pattern=%(out_pattern)s
        --log=%(outfile)s.log
        %(plink_files)s
        '''

        P.run()

    # make a GRM from the candidate region for MLM analysis
    # need to remove duplicates first

    @follows(getCandidateRegion)
    @transform("candidate.dir/*.bed",
               regex("candidate.dir/(.+).bed"),
               add_inputs([r"candidate.dir/\1.fam",
                           r"candidate.dir/\1.bim"]),
               r"grm.dir/\1.grm.N.bin")
    def makeCandidateGRM(infiles, outfiles):
        '''
        Calculate the realised GRM across all candidate region
        variants.   Use parallelisation.
        '''

        job_threads = PARAMS['grm_threads']
        # memory per thread
        job_memory = "12G"

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]

        plink_files = ",".join([bed_file, fam_file, bim_file])
        out_pattern = ".".join(outfiles.split(".")[:-3])

        statement = '''
        cgat geno2assoc
        --program=gcta
        --threads=%(job_threads)s
        --input-file-format=plink_binary
        --keep-individuals=%(gwas_keep)s
        --method=matrix
        --matrix-compression=bin
        --matrix-form=grm
        --output-file-pattern=%(out_pattern)s
        --log=%(outfiles)s.log
        %(plink_files)s
        > %(outfiles)s.gcta.log
        '''

        P.run()

    @follows(getCandidateRegion)
    @transform(getCandidateRegion,
               regex("candidate.dir/(.+).bed"),
               add_inputs([r"candidate.dir/\1.fam",
                           r"candidate.dir/\1.bim"]),
               r"candidate.dir/\1_assoc.assoc")
    def testCandidateRegion(infiles, outfile):
        '''
        Test the candidate region for association using
        Plink basic association - i.e. not a model-specific
        analysis
        '''

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        out_pattern = ".".join(outfile.split(".")[:-1])

        job_threads = PARAMS['candidate_threads']
        job_memory = PARAMS['candidate_memory']

        statement = '''
        cgat geno2assoc
        --program=plink2
        --input-file-format=plink_binary
        --method=association
        --keep-individuals=%(gwas_keep)s
        --association-method=assoc
        --genotype-rate=0.01
        --indiv-missing=0.01
        --hardy-weinberg=0.0001
        --min-allele-frequency=0.001
        --output-file-pattern=%(out_pattern)s
        --threads=%(candidate_threads)s
        --log=%(outfile)s.log
        -v 5
        %(plink_files)s
        '''

        P.run()

    @follows(testCandidateRegion)
    @transform(getCandidateRegion,
               regex("candidate.dir/(.+).bed"),
               add_inputs([r"candidate.dir/\1.fam",
                           r"candidate.dir/\1.bim"]),
               r"candidate.dir/\1_conditional.%s" % PARAMS['conditional_model'])
    def conditionalTestRegion(infiles, outfile):
        '''
        Perform association analysis conditional on
        top SNP from previous analysis
        '''

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        out_pattern = ".".join(outfile.split(".")[:-1])

        statement = '''
        cgat geno2assoc
        --program=plink2
        --input-file-format=plink_binary
        --method=association
        --association-method=logistic
        --keep-individuals=%(gwas_keep)s
        --genotype-rate=0.1
        --indiv-missing=0.1
        --hardy-weinberg=0.0001
        --conditional-snp=rs12924101
        --min-allele-frequency=0.01
        --output-file-pattern=%(out_pattern)s
        --threads=%(pca_threads)s
        --log=%(outfile)s.log
        -v 5
        %(plink_files)s
        '''

        P.run()
else:
    pass


@follows(excludeDiscordantGender,
         findExcessHeterozygotes,
         findExcessHomozygotes,
         excludeInbred,
         flagRelated)
@transform("exclusions.dir/*.het_exclude",
           regex("exclusions.dir/(.+).het_exclude"),
           add_inputs([r"exclusions.dir/\1.inbred",
                       r"exclusions.dir/\1.related",
                       excludeDiscordantGender]),
           r"exclusions.dir/\1.gwas_exclude")
def mergeExclusions(infiles, outfile):
    '''
    Merge together files and drop duplicates
    for individuals that fail QC steps
    '''

    hets = infiles[0]
    inbreds = infiles[1][0]
    related = infiles[1][1]
    gender = infiles[1][2]

    statement = '''
    cgat qcs2qc
    --task=merge_exclusions
    --gender-check-file=%(gender)s
    --relationship-file=%(related)s
    --inbreeding-coef-file=%(inbreds)s
    --heterozygotes-file=%(hets)s
    --auxillary-file=%(gwas_remove)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()

if PARAMS['gwas_model'] == "linear":
    @follows(convertToPlink,
             mergeExclusions,
             mkdir("gwas.dir"))
    @transform("plink.dir/chr*",
               regex("plink.dir/(.+).bed"),
               add_inputs([r"plink.dir/\1.fam",
                           r"plink.dir/\1.bim",
                           r"exclusions.dir/WholeGenome.gwas_exclude"]),
               r"gwas.dir/\1_assoc.qassoc")
    def unadjustedAssociation(infiles, outfile):
        '''
        Run an unadjusted association analysis on SNPs MAF >= 1%
        Need to condition on array batch - field 22000
        '''

        job_memory = "36G"

        mem = int(job_memory.strip("G"))
        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]

        gwas_exclude = infiles[1][2]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        out_pattern = ".".join(outfile.split(".")[:-1])

        statement = '''
        cgat geno2assoc
        --program=plink2
        --input-file-format=plink_binary
        --phenotypes-file=%(data_phenotypes)s
        --pheno=%(format_pheno)s
        --method=association
        --keep=%(gwas_keep)s
        --remove-individuals=%(gwas_exclude)s
        --association-method=assoc
        --genotype-rate=0.1
        --hardy-weinberg=0.000000000000001
        --min-allele-frequency=0.001
        --output-file-pattern=%(out_pattern)s
        --memory=%(mem)s
        -v 5
        %(plink_files)s
        > %(outfile)s.plink.log
        '''

        P.run()

else:
    @follows(convertToPlink,
             mergeExclusions,
             mkdir("gwas.dir"))
    @transform("plink.dir/chr*",
               regex("plink.dir/(.+).bed"),
               add_inputs([r"plink.dir/\1.fam",
                           r"plink.dir/\1.bim",
                           r"exclusions.dir/WholeGenome.gwas_exclude"]),
               r"gwas.dir/\1_assoc.assoc")
    def unadjustedAssociation(infiles, outfile):
        '''
        Run an unadjusted association analysis on SNPs MAF >= 1%
        Need to condition on array batch - field 22000
        '''

        job_memory = "36G"

        mem = int(job_memory.strip("G"))
        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]

        gwas_exclude = infiles[1][2]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        out_pattern = ".".join(outfile.split(".")[:-1])

        statement = '''
        cgat geno2assoc
        --program=plink2
        --input-file-format=plink_binary
        --phenotypes-file=%(data_phenotypes)s
        --pheno=%(format_pheno)s
        --method=association
        --keep=%(gwas_keep)s
        --remove-individuals=%(gwas_exclude)s
        --association-method=assoc
        --genotype-rate=%(gwas_geno)s
        --hardy-weinberg=%(gwas_hwe)s
        --min-allele-frequency=%(gwas_maf)s
        --indiv-missing=%(gwas_mind)s
        --output-file-pattern=%(out_pattern)s
        --memory=%(mem)s
        --subset-filter=%(gwas_filter)s
        -v 5
        %(plink_files)s
        > %(outfile)s.plink.log
        '''

        P.run()


@follows(mkdir("covariates.dir"),
         mergeExclusions,
         unadjustedAssociation)
@transform("covariates.dir/*",
           regex("covariates.dir/(.+).batch"),
           r"covariates.dir/\1.covar")
def mergeCovariates(infiles, outfile):
    '''
    Merge together all covariates for inclusion
    in the adjusted GWA
    '''

    covar_file = infiles
    pca_file = PARAMS['gwas_pca']
    covars = ",".join([covar_file, pca_file])
    job_memory = "4G"

    statement = '''
    cgat pheno2pheno
    --task=merge_covariates
    --covariate-file=%(covars)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(convertToPlink,
         mergeCovariates,
         mergeExclusions,
         mkdir("gwas.dir"))
@transform("plink.dir/chr*",
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"exclusions.dir/WholeGenome.gwas_exclude",
                       mergeCovariates]),
           r"gwas.dir/\1_adj.assoc.%s" % PARAMS['gwas_model'])
def pcAdjustedAssociation(infiles, outfile):
    '''
    Run an association analysis on SNPs MAF >= 1%
    adjusted for principal components
    Need to condition on array batch - field 22000
    '''

    job_memory = "36G"

    mem = int(job_memory.strip("G"))
    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    remove = infiles[1][2]
    covariate_file = infiles[1][3]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-2])

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=association
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --covariates-file=%(covariate_file)s
    --covariate-column=%(gwas_covars)s
    --keep=%(gwas_keep)s
    --remove-individuals=%(remove)s
    --association-method=%(gwas_model)s
    --genotype-rate=%(gwas_geno)s
    --hardy-weinberg=%(gwas_hwe)s
    --min-allele-frequency=%(gwas_maf)s
    --indiv-missing=%(gwas_mind)s
    --output-file-pattern=%(out_pattern)s
    --memory=%(mem)s
    --subset-filter=%(gwas_filter)s
    -v 5
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mkdir("plots.dir"),
         unadjustedAssociation)
@collate(unadjustedAssociation,
         regex("gwas.dir/(.+)_assoc.(.+)"),
         r"plots.dir/WholeGenome_manhattan.png")
def plotUnadjustedManhattan(infiles, outfile):
    '''
    Generate a manhattan plot for the unadjusted
    association analysis
    '''
    job_memory = "16G"

    res_files = ",".join(infiles)
    out_file = outfile.split("/")[-1]
    out_file = out_file.split("-")[0]
    out_file = "gwas.dir/" + out_file + ".results"

    statement = '''
    cgat assoc2plot
    --plot-type=manhattan
    --resolution=genome_wide
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(res_files)s
    > %(out_file)s
    '''

    P.run()


@follows(mkdir("plots.dir"),
         unadjustedAssociation,
         plotUnadjustedManhattan,
         pcAdjustedAssociation)
@collate(pcAdjustedAssociation,
         regex("gwas.dir/(.+)_adj.assoc.%s" % PARAMS['gwas_model']),
         r"plots.dir/WholeGenome_adj-manhattan.png")
def plotGenomeManhattan(infiles, outfile):
    '''
    Generate a manhattan plot across all chromosomes
    from the covariate adjusted analysis
    '''

    job_memory = "16G"

    res_files = ",".join(infiles)
    out_file = outfile.split("/")[-1]
    out_file = out_file.split("-")[0]
    out_file = "gwas.dir/" + out_file + ".results"
    statement = '''
    cgat assoc2plot
    --plot-type=manhattan
    --resolution=genome_wide
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(res_files)s
    > %(out_file)s
    '''

    P.run()


@follows(mkdir("plots.dir"),
         unadjustedAssociation,
         pcAdjustedAssociation,
         plotUnadjustedManhattan,
         plotGenomeManhattan)
@collate(pcAdjustedAssociation,
         regex("gwas.dir/(.+)_adj.assoc.%s" % PARAMS['gwas_model']),
         r"plots.dir/WholeGenome_adj-qqplot.png")
def plotGenomeQQ(infiles, outfile):
    '''
    Generate a QQ plot across all chromosome
    from the covariate adjusted analysis
    '''

    job_memory = "32G"

    res_files = ",".join(infiles)
    statement = '''
    cgat assoc2plot
    --plot-type=qqplot
    --resolution=genome_wide
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(res_files)s
    '''

    P.run()


# testing conditional analysis of multiple regions
# make a load of dummy files first
@follows(pcAdjustedAssociation,
         mkdir("conditional.dir"))
@transform(PARAMS['gwas_hit_regions'],
           regex("(.+)/(.+)-(.+).tsv"),
           r"conditional.dir/\2.tsv")
def splitRegionsFile(infile, outfile):
    '''
    Split a file containing genome intervals
    to pull out for conditional analyses
    '''

    # I LOVE AWK!!!!

    statement = '''
    cat %(infile)s | awk '{if(NR > 1) {printf("conditional.dir/chr%%i-%%i-%%i_%%s.tsv\\n",
    $1, $2, $3, $4)}}' | awk '{system("touch " $0)}';
    '''

    P.run()
    P.touch(outfile)


@follows(splitRegionsFile)
@transform("conditional.dir/*.tsv",
           regex("conditional.dir/(.+)-(.+)-(.+)_(.+).tsv"),
           add_inputs([r"plink.dir/\1.bed",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.fam",
                       r"plink.dir/\1.exclude"]),
           r"conditional.dir/\1-\2-\3_\4.bed")
def getConditionalRegions(infiles, outfile):
    '''
    Pull out the regions of interest for downstream
    conditional analyses
    '''

    conditional_file = infiles[0]
    bed_file = infiles[1][0]
    bim_file = infiles[1][1]
    fam_file = infiles[1][2]
    exclude_snps = infiles[1][3]

    chrom = conditional_file.split("/")[-1].split("-")[0]
    start = conditional_file.split("/")[-1].split("-")[1]
    end = conditional_file.split("/")[-1].split("-")[2]
    end = end.split("_")[0]

    snp_range = ",".join([start, end])
    plink_files = ",".join([bed_file, bim_file, fam_file])

    out_pattern = outfile.strip(".bed")

    job_memory = "40G"
    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=format
    --restrict-chromosome=%(chrom)s
    --snp-range=%(snp_range)s
    --exclude-snps=%(exclude_snps)s
    --format-method=change_format
    --reformat-type=plink_binary
    --keep=%(gwas_keep)s
    --output-file-pattern=%(out_pattern)s
    --memory=%(job_memory)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(getConditionalRegions)
@transform(getConditionalRegions,
           regex("conditional.dir/(.+).bed"),
           add_inputs([r"conditional.dir/\1.fam",
                       r"conditional.dir/\1.bim",
                       r"exclusions.dir/WholeGenome.gwas_exclude",
                       mergeCovariates]),
           r"conditional.dir/\1_conditional.assoc.%s" % PARAMS['conditional_model'])
def conditionalAssociation(infiles, outfile):
    '''
    Perform association analysis conditional on
    top SNP from previous analysis
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    condition_snp = bed_file.split("/")[-1].split("_")[-1]
    conditional_snp = condition_snp.rstrip(".bed")
    remove = infiles[1][2]
    covariate_file = infiles[1][3]

    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "40G"

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=association
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --association-method=%(conditional_model)s
    --covariates-file=%(covariate_file)s
    --covariate-column=%(gwas_covars)s
    --keep-individuals=%(gwas_keep)s
    --remove-individuals=%(remove)s
    --genotype-rate=0.1
    --hardy-weinberg=1e-50
    --memory=%(job_memory)s
    --conditional-snp=%(conditional_snp)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    -v 5
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# test for epistasis between GWAS regions on the phenotype of interest.  Use gwas hit
# regions


@follows(getConditionalRegions,
         mkdir("epistasis.dir"))
@collate(getConditionalRegions,
         regex("conditional.dir/(.+).bed"),
         add_inputs([r"conditional.dir/\1.fam",
                     r"conditional.dir/\1.bim"]),
         r"epistasis.dir/GwasHits.bed")
def mergeGwasHits(infiles, outfile):
    '''
    Take the GWAS hit regions and merge
    into a single set of Plink files to test
    epistatic interactions
    '''

    # generate text file that contains the names of the files
    # to be merged
    temp_file = P.getTempFilename(shared=True)
    outpattern = ".".join(outfile.split(".")[:-1])
    job_memory = "3G"
    job_threads = 10

    with open(temp_file, "w") as ofile:
        for ifile in infiles:
            ifile_bed = ifile[0]
            ifile_fam = ifile[1][0]
            ifile_bim = ifile[1][1]
            ofile.write("%s\t%s\t%s\n" % (ifile_bed, ifile_bim, ifile_fam))

    statement = '''
    plink2
    --merge-list %(temp_file)s
    --threads %(job_threads)i
    --out %(outpattern)s;
    rm -rf %(temp_file)s
    '''

    P.run()


@follows(mergeGwasHits)
@transform(mergeGwasHits,
           regex("epistasis.dir/(.+).bed"),
           add_inputs([r"epistasis.dir/\1.fam",
                       r"epistasis.dir/\1.bim"]),
           r"epistasis.dir/\1_VsRegion.epi.cc")
def testEpistasisVsRegion(infiles, outfile):
    '''
    Test for epistasis with a set of specific variants
    of interest provided in a Plink .set file

    Only test SNPs with MAF >= 0.5%
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "1G"
    job_threads = 11

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --method=epistasis
    --epistasis-method=epistasis
    --set-file=%(epistasis_set)s
    --set-method="set-by-all"
    --epistasis-threshold=%(epistasis_threshold)s
    --epistasis-report-threshold=%(epistasis_reporting)s
    --min-allele-freq=0.005
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    --threads=%(job_threads)s
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()

############################################
# We want to test for epistasis explicitly between target region variants
# and all others whilst adjusting for covariates and removing sources of
# type I error.
# This means for each target region variant any SNPs in LD must be removed.
# So first create lists of variants, for each target variant, that are NOT
# in LD.  These will be selected using Plink to generate the relevant
# files for analysis

# get the list of region SNPs first and create a dummy file for each


@follows(testEpistasisVsRegion,
         mkdir("target_snps.dir"))
@subdivide("%s" % PARAMS['epistasis_set'],
           regex("epistasis.dir/(.+).set"),
           r"epistasis.dir/split_snps.text")
def splitTargetVariants(infile, outfile):
    '''
    Generate a dummy file for each variant in the
    set file
    '''

    job_memory = "0.5G"
    statement = '''
    cat %(infile)s | grep -v "END" | awk '{if(NR > 1) {printf("target_snps.dir/%%s.target\\n", $1)}}'
    | awk '{system("touch " $0)}';
    '''

    P.run()
    P.touch(outfile)


@follows(splitTargetVariants)
@transform("target_snps.dir/*",
           regex("target_snps.dir/(.+).target"),
           r"target_snps.dir/\1.exclude")
def excludeLdVariants(infile, outfile):
    '''
    Extract the variants in LD with target variant
    to exclude from epistasis analysis
    '''

    contig = PARAMS['candidate_chromosome'].lstrip("chr")
    ld_dir = os.path.join(os.getcwd(), "ld.dir")
    ld_files = [lf for lf in os.listdir(ld_dir) if re.search(contig, lf)]
    ld_fle = [os.path.join(ld_dir, bg)
              for bg in ld_files if re.search("bgz$", bg)][0]

    snp = infile.split("/")[-1].split(".")[0]
    job_memory = "2G"

    statement = '''tabix %(ld_fle)s %(contig)s:87500000-90354753 |
    grep "%(snp)s" | awk '{ print } END {if (!NR) {print "Empty"} else {if($7 > 0.1) { print }}}'
    | cut -f 3,6 | tr -s '\\t' '\\n' | tr -s ';' '\\n' | sort | uniq | grep -v %(snp)s
    > %(outfile)s
    '''

    P.run()


@follows(splitTargetVariants,
         excludeLdVariants)
@transform(mergeGwasHits,
           regex("epistasis.dir/(.+).bed"),
           add_inputs([r"epistasis.dir/\1.fam",
                       r"epistasis.dir/\1.bim",
                       r"exclusions.dir/WholeGenome.gwas_exclude"]),
           r"epistasis.dir/\1.raw")
def convertToRawFormat(infiles, outfile):
    '''
    Extract the target SNP genotypes in raw format
    to merge with the covariates file
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])
    exclusions = infiles[1][2]

    out_pattern = ".".join(outfile.split(".")[:-1])
    job_memory = "30G"

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=format
    --format-method=change_format
    --reformat-type=raw
    --keep-individuals=%(gwas_keep)s
    --remove-individuals=%(exclusions)s
    --extract-snps=%(epistasis_set)s
    --output-file-pattern=%(out_pattern)s
    --memory=%(job_memory)s
    %(plink_files)s
    '''

    P.run()


@follows(convertToRawFormat)
@transform(convertToRawFormat,
           regex("epistasis.dir/(.+).raw"),
           add_inputs("covariates.dir/WholeGenome.covar"),
           r"epistasis.dir/\1.covar")
def mergeGenotypeAndCovariates(infiles, outfile):
    '''
    Merge covariates and target SNPs into a single
    file
    '''

    snp_file = infiles[0]
    covar_file = infiles[1]
    covars = ",".join([covar_file, snp_file])
    job_memory = "4G"

    statement = '''
    cgat pheno2pheno
    --task=merge_covariates
    --adjustment=snp
    --covariate-file=%(covars)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(excludeLdVariants)
@transform(excludeLdVariants,
           regex("target_snps.dir/(.+).exclude"),
           add_inputs([r"epistasis.dir/GwasHits.bed",
                       r"epistasis.dir/GwasHits.fam",
                       r"epistasis.dir/GwasHits.bim"]),
           r"epistasis.dir/single_\1.epi.cc")
def ldExcludedEpistasisVsGwasLead(infiles, outfile):
    '''
    Test for epistasis between a given variant derived
    from a set file, and only the lead SNPs from
    a genome-wide analysis
    '''

    job_memory = "40G"
    job_threads = 1

    snp = infiles[0].split("/")[-1].split(".")[0]
    ld_exclude = infiles[0]

    bed_file = infiles[1][0]
    fam_file = infiles[1][1]
    bim_file = infiles[1][2]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-2])
    out_pattern = out_pattern

    # write the SNP id to a dummy set file
    tmpf = P.getTempFilename(shared=True)
    with open(tmpf, "w") as tfile:
        tfile.write("VAR\n{}\nEND".format(snp))

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --method=epistasis
    --exclude-snps=%(ld_exclude)s
    --epistasis-method=epistasis
    --extract-snps=%(epistasis_hit_region)s
    --set-file=%(tmpf)s
    --set-method="set-by-all"
    --epistasis-threshold=%(epistasis_threshold)s
    --epistasis-report-threshold=%(epistasis_reporting)s
    --min-allele-freq=0.001
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    --threads=%(job_threads)s
    --memory=%(job_memory)s
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()

    statement = '''rm -rf %(tmpf)s'''

    P.run()


@follows(excludeLdVariants,
         ldExcludedEpistasisVsGwasLead)
@transform(excludeLdVariants,
           regex("target_snps.dir/(.+).exclude"),
           add_inputs([r"epistasis.dir/GwasHits.bed",
                       r"epistasis.dir/GwasHits.fam",
                       r"epistasis.dir/GwasHits.bim"]),
           r"epistasis.dir/\1.epi.cc")
def ldExcludedEpistasis(infiles, outfile):
    '''
    Test for epistasis with a given variant derived
    from a set file, exlcuding all variants
    in LD.
    Only test SNPs with MAF >= 0.5%
    '''

    job_memory = "80G"
    job_threads = 1

    snp = infiles[0].split("/")[-1].split(".")[0]
    ld_exclude = infiles[0]

    bed_file = infiles[1][0]
    fam_file = infiles[1][1]
    bim_file = infiles[1][2]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-2])

    # write the SNP id to a dummy set file
    tmpf = P.getTempFilename(shared=True)
    with open(tmpf, "w") as tfile:
        tfile.write("VAR\n{}\nEND".format(snp))

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --method=epistasis
    --exclude-snps=%(ld_exclude)s
    --epistasis-method=epistasis
    --set-file=%(tmpf)s
    --set-method="set-by-all"
    --epistasis-threshold=%(epistasis_threshold)s
    --epistasis-report-threshold=%(epistasis_reporting)s
    --min-allele-freq=0.005
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    --threads=%(job_threads)s
    --memory=%(job_memory)s
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()

    statement = '''rm -rf %(tmpf)s'''

    P.run()


@follows(ldExcludedEpistasis)
@transform(ldExcludedEpistasis,
           regex("epistasis.dir/rs(.+).epi.cc"),
           r"plots.dir/rs\1-naive_epistasis_manhattan.png")
def plotLdExcludedEpistasis(infile, outfile):
    '''
    Generate a manhattan plot and QQplot
    of the adjusted epistasis analysis
    '''

    job_memory = "8G"

    plot_path = "_".join(outfile.split("_")[:-1])

    statement = '''
    cgat assoc2plot
    --plot-type=epistasis
    --resolution=chromosome
    --log=%(outfile)s.log
    --save-path=%(plot_path)s
    %(infile)s
    > %(outfile)s
    '''

    P.run()


# @jobs_limit(6)
# @follows(mergeGenotypeAndCovariates,
#          excludeLdVariants,
#          plotLdExcludedEpistasis)
# @transform(excludeLdVariants,
#            regex("target_snps.dir/(.+).exclude"),
#            add_inputs([r"epistasis.dir/GwasHits.bed",
#                        r"epistasis.dir/GwasHits.fam",
#                        r"epistasis.dir/GwasHits.bim",
#                        mergeGenotypeAndCovariates,
#                        r"exclusions.dir/WholeGenome.gwas_exclude"]),
#            r"epistasis.dir/\1.auto.R")
# def adjustedEpistasis(infiles, outfile):
#     '''
#     Test for epistasis between target SNPs
#     and SNPs of interest, whilst adjusting for
#     covariates.
#     '''

#     job_memory = "75G"
#     job_threads = 1

#     snp = infiles[0].split("/")[-1].split(".")[0]
#     ld_exclude = infiles[0]

#     bed_file = infiles[1][0]
#     fam_file = infiles[1][1]
#     bim_file = infiles[1][2]
#     plink_files = ",".join([bed_file, fam_file, bim_file])

#     covar_file = infiles[1][3]
#     covars = PARAMS['gwas_covars']
#     all_covars = ",".join([covars, snp])
#     remove = infiles[1][4]

#     out_pattern = ".".join(outfile.split(".")[:-2])

#     statement = '''
#     R CMD Rserve --vanilla; checkpoint;
#     cgat geno2assoc
#     --program=plink2
#     --input-file-format=plink_binary
#     --phenotypes-file=%(data_phenotypes)s
#     --pheno=%(format_pheno)s
#     --method=epistasis
#     --epistasis-method=adjusted
#     --epistasis-parameter=%(epistasis_plugin)s
#     --covariates-file=%(covar_file)s
#     --covariate-column=%(all_covars)s
#     --exclude-snps=%(ld_exclude)s
#     --remove-individuals=%(remove)s
#     --min-allele-freq=0.005
#     --output-file-pattern=%(out_pattern)s
#     --log=%(outfile)s.log
#     --memory=%(job_memory)s
#     %(plink_files)s
#     > %(outfile)s.plink.log
#     '''

#     P.run()


# @follows(adjustedEpistasis)
# @transform(adjustedEpistasis,
#            regex("epistasis.dir/(.+).auto.R"),
#            r"plots.dir/\1-epistasis_manhattan.png")
# def plotAdjustedEpistasis(infile, outfile):
#     '''
#     Generate a manhattan plot and QQplot
#     of the adjusted epistasis analysis
#     '''

#     job_memory = "4G"

#     plot_path = "_".join(outfile.split("_")[:-1])

#     statement = '''
#     cgat assoc2plot
#     --plot-type=epistasis
#     --resolution=chromosome
#     --log=%(outfile)s.log
#     --save-path=%(plot_path)s
#     %(infile)s
#     > %(outfile)s
#     '''

#     P.run()


#######################################################
# Plink R interface for epistasis testing
# unknown problem with this, can't figure it out
# not clear if it is the R code or plink being weird
# use cassi for adjusted/unadjusted epistasis testing
# instead - uses LR test instead of Wald
#######################################################

@follows(convertToRawFormat)
@collate(excludeLdVariants,
         regex("target_snps.dir/(.+).exclude"),
         r"target_snps.dir/all_snps.exclude")
def mergeLdExclusions(infiles, outfile):
    '''
    Merge together all of the LD exclusions for
    epistasis testing with cassi,
    need to exclude the epistasis target snps
    '''

    snps = [fx.split("/")[-1].split(".")[0] for fx in infiles]
    snp_grep = "|".join(['{}'.format(snp) for snp in snps])
    exclude = " ".join(infiles)

    statement = '''
    cat %(exclude)s | grep -P  -v "(%(snp_grep)s)" >> %(outfile)s
    '''

    P.run()


@follows(convertToRawFormat,
         mergeLdExclusions)
@transform("%s.set" % PARAMS['epistasis_set'],
           regex("epistasis.dir/(.+).set"),
           add_inputs([r"epistasis.dir/GwasHits.bed",
                       r"covariates.dir/WholeGenome.covar",
                       r"exclusions.dir/WholeGenome.gwas_exclude",
                       mergeLdExclusions]),
           r"epistasis.dir/\1.cov")
def makeCassiFiles(infiles, outfile):
    '''
    Create two sets of files for cassi, the first input
    bed,bim,fam files containing all GWAS hit region
    variants, and the second with the appropriate
    individuals covariates.  cassi does not do the
    inplace filtering and joining that plink does
    so files need to be matched exactly for individuals
    '''

    job_memory = "60G"

    snp_file = infiles[0]
    bed_file = infiles[1][0].rstrip(".bed")
    covar_file = infiles[1][1]
    exclude = infiles[1][2]
    ld_snps = infiles[1][3]

    out_pattern = outfile.rstrip(".cov")

    statement = '''
    plink2
    --bfile %(bed_file)s
    --covar %(covar_file)s
    --remove %(exclude)s
    --extract %(snp_file)s
    --exclude %(ld_snps)s
    --make-bed
    --out %(out_pattern)s
    '''

    P.run()


@follows(convertToRawFormat,
         makeCassiFiles)
@subdivide("epistasis.dir/GwasHits.bed",
           regex("epistasis.dir/(.+).bed"),
           add_inputs([r"epistasis.dir/\1.bim",
                       mergeLdExclusions]),
           r"epistasis.dir/split_chromosomes.txt")
def splitByChromosome(infiles, outfile):
    '''
    Split cassi plink format files by chromosome
    to make parallelisation easier.
    '''

    bim_file = infiles[1][0]
    plink_prefix = infiles[0].rstrip(".bed")
    exclude = infiles[1][1]

    statement = '''
    for chr in `awk '{print $1}' %(bim_file)s | uniq` ;
    do plink2 --bfile %(plink_prefix)s --chr $chr --exclude %(exclude)s  --make-bed --out epistasis.dir/chr$chr\.epi_cassi ;
    done;
    touch %(outfile)s
    '''

    P.run()


@follows(convertToRawFormat,
         makeCassiFiles,
         splitByChromosome)
@transform("epistasis.dir/*.epi_cassi.bed",
           regex("epistasis.dir/chr(.+).epi_cassi.bed"),
           add_inputs([r"epistasis.dir/chr\1.epi_cassi.bed",
                       r"covariates.dir/WholeGenome.covar",
                       r"exclusions.dir/WholeGenome.gwas_exclude"]),
           r"epistasis.dir/chr\1-cassi.bed")
def makeOtherCassiFiles(infiles, outfile):
    '''
    Create two sets of files for cassi, the first input
    bed,bim,fam files containing all GWAS hit region
    variants, and the second with the appropriate
    individuals covariates.  cassi does not do the
    inplace filtering and joining that plink does
    so files need to be matched exactly for individuals
    Need to break this down by chromosome.
    '''

    job_memory = "60G"

    bed_file = infiles[0].rstrip(".bed")
    covar = infiles[1][1]
    exclude = infiles[1][2]

    out_pattern = outfile.rstrip(".bed")

    statement = '''
    plink2
    --bfile %(bed_file)s
    --remove %(exclude)s
    --make-bed
    --maf 0.005
    --covar %(covar)s
    --out %(out_pattern)s
    '''

    P.run()


@follows(mergeGenotypeAndCovariates,
         excludeLdVariants,
         makeCassiFiles,
         makeOtherCassiFiles,
         mkdir("unadjusted_epistasis.dir"))
@transform(makeOtherCassiFiles,
           regex("epistasis.dir/(.+)-cassi.bed"),
           add_inputs([makeCassiFiles,
                       r"%s.bed" % PARAMS['epistasis_set']]),
           r"epistasis.dir/\1.cassi.epi")
def testAdjustedEpistasis(infiles, outfile):
    '''
    Use cassi to test for epistasis between a set of input target
    variants of interest, and all other input variants.

    Epistasis testing is by a likelihood ratio test of the full
    model with interaction terms vs. the reduced model with only
    the main effects
    '''
    # make cassi is in your $PATH
    # use the direct cassi interface for now,
    # maybe wrap up into geno2assoc.py at a later date

    job_memory = "150G"
    job_threads = 1

    bed2_file = infiles[0]
    bed1_file = infiles[1][1]

    covar_file = infiles[1][0]

    statement = '''
    cassi
    -i %(bed1_file)s
    -i2 %(bed2_file)s
    -mem1
    -lr
    -lr-covar %(covar_file)s
    -lr-covar-name %(gwas_covars)s
    -rsq
    -dprime
    -lr-th 1.0
    -max 30000000
    -o %(outfile)s
    '''

    P.run()


@follows(testAdjustedEpistasis)
@collate(testAdjustedEpistasis,
         regex("epistasis.dir/(.+)\.(.+).epi"),
         r"epistasis.dir/split_\2.txt")
def aggregateCassiResults(infiles, outfile):
    '''
    The output for cassi is split over chromosomes,
    and each file contains one chromosome but many target SNPs.
    Therefore results need to be aggregated by target SNP.
    '''

    join_files = ",".join(infiles)
    out_dir = "/".join(outfile.split("/")[:-1])

    statement = '''
    cgat assoc2assoc
    --task=process_epi
    --file-format=cassi
    --output-directory=%(out_dir)s
    --log=%(outfile)s.log
    %(join_files)s
    > %(outfile)s
    '''

    P.run()


@follows(aggregateCassiResults)
@transform("epistasis.dir/*_cassi.epi",
           regex("epistasis.dir/(.+)_cassi.epi"),
           r"plots.dir/\1-cassi_epistasis_manhattan.png")
def plotAdjustedEpistasis(infile, outfile):
    '''
    Generate a manhattan plot and QQplot
    of the adjusted epistasis analysis
    '''

    job_memory = "4G"

    plot_path = "_".join(outfile.split("_")[:-1])

    statement = '''
    cgat assoc2plot
    --plot-type=epistasis
    --file-format=cassi_covar
    --resolution=chromosome
    --log=%(outfile)s.log
    --save-path=%(plot_path)s
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(mergeGenotypeAndCovariates,
         excludeLdVariants,
         makeCassiFiles,
         makeOtherCassiFiles,
         testAdjustedEpistasis,
         mkdir("unadjusted_epistasis.dir"))
@transform(makeOtherCassiFiles,
           regex("epistasis.dir/(.+)-cassi.bed"),
           add_inputs([makeCassiFiles,
                       r"%s.bed" % PARAMS['epistasis_set']]),
           r"epistasis.dir/\1_unadjusted.cassi.epi")
def testUnadjustedEpistasis(infiles, outfile):
    '''
    Use cassi to test for epistasis between a set of input target
    variants of interest, and all other input variants,
    without principal component covariates.

    Epistasis testing is by a likelihood ratio test of the full
    model with interaction terms vs. the reduced model with only
    the main effects
    '''
    # make cassi is in your $PATH
    # use the direct cassi interface for now,
    # maybe wrap up into geno2assoc.py at a later date

    job_memory = "150G"
    job_threads = 1

    bed2_file = infiles[0]
    bed1_file = infiles[1][1]

    bed1_file = infiles[1][0]
    bed2_file = infiles[1][1]

    statement = '''
    cassi
    -i %(bed1_file)s
    -i2 %(bed2_file)s
    -mem2
    -lr
    -rsq
    -dprime
    -lr-th 1.0
    -max 30000000
    -o %(outfile)s
    '''

    P.run()

# @jobs_limit(6)
# @follows(mergeGenotypeAndCovariates,
#          excludeLdVariants,
#          plotLdExcludedEpistasis,
#          plotAdjustedEpistasis,
#          mkdir("unadjusted_epistasis.dir"))
# @transform(excludeLdVariants,
#            regex("target_snps.dir/(.+).exclude"),
#            add_inputs([r"epistasis.dir/GwasHits.bed",
#                        r"epistasis.dir/GwasHits.fam",
#                        r"epistasis.dir/GwasHits.bim",
#                        mergeGenotypeAndCovariates,
#                        r"exclusions.dir/WholeGenome.gwas_exclude"]),
#            r"unadjusted_epistasis.dir/\1.auto.R")
# def unadjustedEpistasis(infiles, outfile):
#     '''
#     Test for epistasis between target SNPs
#     and SNPs of interest, without the population structure
#     adjustment.
#     '''

#     job_memory = "75G"
#     job_threads = 1

#     snp = infiles[0].split("/")[-1].split(".")[0]
#     ld_exclude = infiles[0]

#     bed_file = infiles[1][0]
#     fam_file = infiles[1][1]
#     bim_file = infiles[1][2]
#     plink_files = ",".join([bed_file, fam_file, bim_file])

#     covar_file = infiles[1][3]
#     # remove principal components variables f....
#     covars = ",".join([cx for cx in PARAMS['gwas_covars'].split(",") if not re.search("f", cx)])
#     all_covars = ",".join([covars, snp])
#     remove = infiles[1][4]

#     out_pattern = ".".join(outfile.split(".")[:-2])

#     statement = '''
#     R CMD Rserve --vanilla; checkpoint;
#     cgat geno2assoc
#     --program=plink2
#     --input-file-format=plink_binary
#     --phenotypes-file=%(data_phenotypes)s
#     --pheno=%(format_pheno)s
#     --remove-individuals=%(remove)s
#     --method=epistasis
#     --epistasis-method=adjusted
#     --epistasis-parameter=%(epistasis_plugin)s
#     --covariates-file=%(covar_file)s
#     --covariate-column=%(all_covars)s
#     --exclude-snps=%(ld_exclude)s
#     --min-allele-freq=0.005
#     --output-file-pattern=%(out_pattern)s
#     --log=%(outfile)s.log
#     --memory=%(job_memory)s
#     %(plink_files)s
#     > %(outfile)s.plink.log
#     '''

#     P.run()


# @follows(testUnadjustedEpistasis)
# @transform(testUnadjustedEpistasis,
#            regex("unadjusted_epistasis.dir/(.+).auto.R"),
#            r"plots.dir/\1-unadjusted-epistasis_manhattan.png")
# def plotUnadjustedEpistasis(infile, outfile):
#     '''
#     Generate a manhattan plot and QQplot
#     of the adjusted epistasis analysis
#     '''

#     job_memory = "4G"

#     plot_path = "_".join(outfile.split("_")[:-1])

#     statement = '''
#     cgat assoc2plot
#     --plot-type=epistasis
#     --resolution=chromosome
#     --log=%(outfile)s.log
#     --save-path=%(plot_path)s
#     %(infile)s
#     > %(outfile)s
#     '''

#     P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Analysis of pleiotropy using the Pleiotropy Estimation and Testing method               #
# of Zhang et al Genetic Epidemiology 2014                                                #
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Requires SNPs in raw genotype (human readable) format, phenotypes and
# covariates

@follows(convertToRawFormat,
         mkdir("pleiotropy.dir"))
@transform(convertToRawFormat,
           regex("epistasis.dir/(.+).raw"),
           add_inputs(["covariates.dir/WholeGenome.merged",
                       selectBritish]),
           r"pleiotropy.dir/\1.merged")
def mergeForPleiotropy(infiles, outfile):
    '''
    Merge covariates, phenotype and target SNPs into a single
    file
    '''

    snp_file = infiles[0]
    covar_file = infiles[1][0]
    pheno_file = infiles[1][1]
    covars = ",".join([covar_file, snp_file, pheno_file])
    job_memory = "4G"

    statement = '''
    cgat pheno2pheno
    --task=merge_covariates
    --adjustment=snp
    --covariate-file=%(covars)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(mergeForPleiotropy)
@transform(mergeForPleiotropy,
           suffix(".merged"),
           ".pleiotropy")
def calcPleiotropyTest(infile, outfile):
    '''
    Apply the PET-B method to genotype data for
    two traits.
    '''

    job_memory = "40G"
    job_threads = 1

    statement = '''
    cgat testPleiotropy
    --R-scripts=%(r_scripts)s
    --trait1=%(pleiotropy_trait1)s
    --trait2=%(pleiotropy_trait2)s
    --covariates=%(gwas_covars)s
    --resamples=%(pleiotropy_resamples)i
    --trait1-model=%(pleiotropy_trait1_model)s
    --trait2-model=%(pleiotropy_trait2_model)s
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# mixed model analysis on a subsample of the data. Compute GRM from specific SNP set
# take SNP set across limited region of MC1R??


@follows(convertToPlink,
         mkdir("mlm.dir"))
@transform("%s/%s*" % (PARAMS['mlm_genotypes'],
                       PARAMS['mlm_grm_region'].split("-")[0]),
           regex("%s/(.+).bed" % PARAMS['mlm_genotypes']),
           add_inputs([r"%s/\1.fam" % PARAMS['mlm_genotypes'],
                       r"%s/\1.bim" % PARAMS['mlm_genotypes'],
                       r"%s/\1.exclude" % PARAMS['mlm_genotypes'],
                       mergeExclusions]),
           r"mlm.dir/\1_region.bed")
def getGrmRegion(infiles, outfile):
    '''
    Get a specific region to generate the GRM for
    the linear mixed model
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    exclude = infiles[1][2]
    gwas_exclude = infiles[1][3]
    out_pattern = ".".join(outfile.split(".")[:-1])
    chromosome = PARAMS['mlm_grm_region'].split("-")[0]
    region = ",".join(PARAMS['mlm_grm_region'].split("-")[1:])

    job_memory = "30G"

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=format
    --keep-individuals=%(gwas_keep)s
    --remove-individuals=%(gwas_exclude)s
    --restrict-chromosome=%(chromosome)s
    --snp-range=%(region)s
    --exclude-snps=%(exclude)s
    --format-method=change_format
    --format-parameter=%(format_gender)s
    --update-sample-attribute=gender
    --reformat-type=plink_binary
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    '''

    P.run()


@follows(getGrmRegion)
@transform("mlm.dir/*.fam",
           regex("mlm.dir/(.+).fam"),
           add_inputs("%s" % PARAMS['gwas_keep']),
           r"mlm.dir/\1.keep")
def subSampleIndividuals(infiles, outfile):
    '''
    Subsample from the total population
    to get ~n individuals for linear
    model analysis.
    Output with combined list of ethnically
    selected individuals
    '''

    fam_file = infiles[0]
    keep_file = infiles[1]
    keep_temp = P.getTempFilename(shared=True)
    fam_temp = P.getTempFilename(shared=True)

    statement = '''
    cat %(keep_file)s | cut -f1,2 | sort | grep -v "FID" > %(keep_temp)s;
    cat %(fam_file)s | tr " " "\\t" | cut -f1,2 | sort > %(fam_temp)s;
    comm -12 %(fam_temp)s %(keep_temp)s | shuf -n %(mlm_subsample)s
    > %(outfile)s;
    rm -rf %(keep_temp)s %(fam_temp)s
    '''

    P.run()


@follows(getGrmRegion,
         subSampleIndividuals)
@transform("mlm.dir/*.bed",
           regex("mlm.dir/(.+).bed"),
           add_inputs([r"mlm.dir/\1.fam",
                       r"mlm.dir/\1.bim",
                       subSampleIndividuals]),
           r"mlm.dir/\1.grm.N.bin")
def calcRegionGrm(infiles, outfiles):
    '''
    Calculate the realised GRM across all region
    variants. Use multi-threading, no parallelisation
    required.
    '''

    job_threads = PARAMS['grm_threads']
    # memory per thread
    job_memory = "8G"

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    grm_keep = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfiles.split(".")[:-3])

    statement = '''
    cgat geno2assoc
    --program=gcta
    --threads=%(job_threads)s
    --input-file-format=plink_binary
    --keep-individuals=%(grm_keep)s
    --method=matrix
    --matrix-compression=bin
    --matrix-form=grm
    --output-file-pattern=%(out_pattern)s
    --log=%(outfiles)s.log
    %(plink_files)s
    > %(outfiles)s.gcta.log
    '''

    P.run()


@follows(subSampleIndividuals)
@transform("mlm.dir/*.fam",
           regex("mlm.dir/(.+).fam"),
           r"mlm.dir/\1.pheno")
def subsetPhenotype(infile, outfile):
    '''
    Generate a phenotype file of the subset
    individuals from the .fam file.

    GCTA doesn't like having extraneous phenotype
    data when the sample size is large
    '''

    job_memory = "1G"

    statement = '''
    cat %(infile)s | tr " " "\\t" | cut -f 1,2,6 |
    awk 'BEGIN {printf("FID\\tIID\\tPHENO\\n")} {print $0}'
    > %(outfile)s'''

    P.run()


@follows(subsetPhenotype)
@transform("plink.dir/*.bed",
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       mergeExclusions,
                       subsetPhenotype,
                       r"covariates.dir/WholeGenome.batch"]),
           r"mlm.dir/\1.mlma")
def runMixedModel(infiles, outfile):
    '''
    Run a linear mixed-model association analysis
    on the subset individuals.
    '''

    job_threads = PARAMS['grm_threads']
    # memory per thread
    job_memory = "7G"

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    mlm_exclude = infiles[1][2]
    mlm_pheno = infiles[1][3]
    mlm_covar = infiles[1][4]
    grm_prefix = ".".join(mlm_pheno.split(".")[:-1])

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --program=gcta
    --threads=%(job_threads)s
    --input-file-format=plink_binary
    --method=lmm
    --lmm-method=standard
    --min-allele-frequency=0.01
    --grm-prefix=%(grm_prefix)s
    --remove-individuals=%(mlm_exclude)s
    --phenotypes-file=%(mlm_pheno)s
    --pheno=1
    --covariates-file=%(gwas_pca)s
    --discrete-covariates-file=%(mlm_covar)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.gcta.log
    '''

    P.run()


@follows(mkdir("plots.dir"),
         runMixedModel)
@collate(runMixedModel,
         regex("mlm.dir/(.+).mlma"),
         r"plots.dir/WholeGenome_mlm-manhattan.png")
def plotMixedModelManhattan(infiles, outfile):
    '''
    Generate a manhattan plot across each chromosome
    from the unadjusted analysis
    '''

    job_memory = "16G"

    res_files = ",".join(infiles)
    out_file = outfile.split("/")[-1]
    out_file = out_file.split("-")[0]
    out_file = "gwas.dir/" + out_file + ".results"
    statement = '''
    cgat assoc2plot
    --plot-type=manhattan
    --resolution=genome_wide
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(res_files)s
    > %(out_file)s
    '''

    P.run()


@follows(mkdir("plots.dir"),
         runMixedModel)
@collate(runMixedModel,
         regex("mlm.dir/(.+).mlma"),
         r"plots.dir/WholeGenome_mlm-qqplot.png")
def plotMixedModelQQ(infiles, outfile):
    '''
    Generate a QQ plot across all MLM results
    '''

    job_memory = "16G"

    res_files = ",".join(infiles)
    statement = '''
    cgat assoc2plot
    --plot-type=qqplot
    --resolution=genome_wide
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(res_files)s
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# these next tasks aren't strictly GWA, but they do use genome-wide
# genotying nonetheless


@follows(mergePlinkFiles,
         mkdir("fst.dir"))
@transform("genome.dir/WholeGenome.*",
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"fst.dir/\1.fst")
def getGenomeWideFst(infiles, outfile):
    '''
    Calculate Wright's F-statistic for population
    differentiation, Fst, for all SNPs.

    Use approx. independent SNP set following
    LD pruning.
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])
    job_memory = "20G"

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --summary-method=case_control_fst
    --summary-parameter=case-control
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    -v 5
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mergePlinkFiles,
         getGenomeWideFst,
         mkdir("fst.dir"))
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude"]),
           r"fst.dir/\1.fst")
def getFstByChromosome(infiles, outfile):
    '''
    Calculate Wright's F-statistic for population
    differentiation, Fst, for all SNPs.

    Use all SNPs across all chromosomes, minus
    exclusions
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    exclude_file = infiles[1][2]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --exclude-snps=%(exclude_file)s
    --summary-method=case_control_fst
    --summary-parameter="case-control"
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    -v 5
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Calculate region-wide LD using a reference population


@follows(mkdir("reference.dir"))
@transform("%s" % PARAMS['reference_pops'],
           regex("(.+)/(.+).ALL.panel"),
           r"reference.dir/\2.pop")
def selectRefPopulation(infile, outfile):
    '''
    Select the reference population from
    the panel demographics file
    '''

    job_memory = "1G"

    statement = '''
    cat %(infile)s |
    awk '{if($3 == "%(reference_select)s") {printf("%%s\\t%%s\\n", $1, $1)}}'
    > %(outfile)s
    '''

    P.run()


@follows(convertToPlink,
         mkdir("haplotypes.dir"))
@transform("plink.dir/*.bed",
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"exclusions.dir/WholeGenome.gwas_exclude"]),
           r"haplotypes.dir/\1.blocks.det")
def defineHaplotypeBlocks(infiles, outfile):
    '''
    Assign SNPs to haplotype blocks and get LD
    block positions
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    remove = infiles[1][2]

    out_pattern = ".".join(outfile.split(".")[:-2])

    job_memory = "40G"

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=estimate_haplotypes
    --memory="40G"
    --keep=%(gwas_keep)s
    --remove-individuals=%(remove)s
    --genotype-rate=0.1
    --min-allele-frequency=0.001
    --hardy-weinberg=0.00000001
    --haplotype-frequency=0.001
    --haplotype-size=1000
    --log=%(outfile)s.log
    --output-file-pattern=%(out_pattern)s
    -v 5
    %(plink_files)s
    > %(outfile)s.plink.log
    '''
    P.run()


# LD calculations need to be on defined regions,
# otherwise files are huge and a pain the bumhole to
# manipulate, process and store
# expect file names format - "ALL.chrN.*.vcf.gz"

@follows(mkdir("reference.dir"))
@transform("%s/*.vcf.gz" % PARAMS['reference_vcf'],
           regex("(.+)/ALL_chr(.+)_phase(.+)_(.+)_(.+).vcf.gz"),
           add_inputs(selectRefPopulation),
           r"reference.dir/chr\2_ref.bed")
def convertRefVcf(infiles, outfile):
    '''
    Convert a reference panel VCF file to
    plink format for LD calculation.
    Only keep variants with MAF >= 0.1%
    '''

    job_memory = "60G"

    vcf = infiles[0]
    keep = infiles[1]
    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=vcf
    --method=format
    --memory="60G"
    --format-method=change_format
    --reformat-type=plink_binary
    --min-allele-frequency=0.001
    --keep-individuals=%(keep)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(vcf)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(convertRefVcf,
         mkdir("ld.dir"))
@transform(convertRefVcf,
           regex("reference.dir/chr(.+)_ref.bed"),
           add_inputs([r"reference.dir/chr\1_ref.fam",
                       r"reference.dir/chr\1_ref.bim"]),
           r"ld.dir/chr\1.ld.gz")
def calcLd(infiles, outfile):
    '''
    Calculate LD region wide for SNPs within
    1Mb of eachother from a reference panel;
    recommend 1000 Genomes project.
    This needs to go into a database
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "5G"
    job_threads = 12

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=ld
    --ld-statistic=r2
    --ld-min=0.05
    --ld-format-output=table
    --memory="60G"
    --threads=%(job_threads)s
    --log=%(outfile)s.log
    --output-file-pattern=%(out_pattern)s
    %(plink_files)s
    > %(outfile)s.plink.log;
    '''

    P.run()


@follows(calcLd)
@transform(calcLd,
           suffix(".ld.gz"),
           ".ld.bgz")
def bgzipLdFiles(infile, outfile):
    '''
    Tabix requires block Gzipped files
    to index - zap input files to
    save on storage.
    '''

    job_memory = "1G"
    tmp_file = P.getTempFilename()
    statements = []

    statements.append('''
    zcat %(infile)s | tr -s ' ' '\\t' |
    sed 's/^[[:space:]]*//g' |
    sed 's/*[[:space:]]$//g' |
    bgzip > %(outfile)s
    ''')

    # statements.append('''
    # touch -r %(infile)s %(tmp_file)s
    # ''')

    # statements.append('''
    # touch -r %(tmp_file)s %(infile)s
    # ''')

    # statements.append('''
    # rm -rf %(tmp_file)s
    # ''')

    P.run()


@follows(bgzipLdFiles)
@transform(bgzipLdFiles,
           suffix(".ld.bgz"),
           ".ld.bgz.tbi")
def tabixIndexLd(infile, outfile):
    '''
    Use tabix to index pair-wise LD
    values on BP_B column
    '''

    job_memory = "4G"

    statement = '''
    tabix -S 1 -b 2 -e 2 %(infile)s
    '''

    P.run()


@follows(calcLd)
@transform(calcLd,
           suffix(".ld.gz"),
           ".load")
def loadLd(infile, outfile):
    '''
    Load all LD values into separate tables,
    use BP_A as the index
    '''

    job_memory = "60G"
    # ld output files need whitespace substituting for tabs
    temp_file = P.getTempFilename(shared=True)
    statement = '''
    zcat %(infile)s | tr -s " " "\\t" |
     sed 's/^[[:space:]]*//g' |
     sed 's/[[:space:]]$//g' > %(temp_file)s
    '''

    P.run()

    P.load(temp_file, outfile,
           job_memory=job_memory,
           options="--add-index=BP_A")

# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# SNP prioritisation methods <- trying to identify most likely causal SNPs for each
# association signal

# need a list of SNPs that represent independent signals <- conditional and original
# need: P-value, ORs, SE, LD, functional annotation(?)


@follows(tabixIndexLd,
         plotGenomeManhattan,
         mkdir("hit_regions.dir"))
@subdivide("gwas.dir/*_adj.results",
           regex("gwas.dir/(.+)_adj.results"),
           r"hit_regions.dir/res_sig.tsv")
def splitGwasRegions(infile, outfile):
    '''
    Split top GWAS hits into separate files
    for candidate causal SNP prioritisation
    '''

    job_memory = "5G"
    out_dir = "/".join(outfile.split("/")[:-1])

    statement = '''
    cgat assoc2assoc
    --task=get_hits
    --p-threshold=0.00000005
    --output-directory=%(out_dir)s
    --log=%(outfile)s.log
    %(infile)s
    '''

    P.run()
    P.touch(outfile)


@follows(tabixIndexLd,
         conditionalAssociation,
         splitGwasRegions)
@transform("conditional.dir/*.assoc.logistic",
           regex("conditional.dir/(.+)-(.+)-(.+).assoc.logistic"),
           r"hit_regions.dir/cond_sig.tsv")
def splitConditionalRegions(infile, outfile):
    '''
    Split top conditional analysos hits into separate files
    for candidate causal SNP prioritisation
    '''

    job_memory = "5G"
    out_dir = "/".join(outfile.split("/")[:-1])

    statement = '''
    cgat assoc2assoc
    --task=get_hits
    --p-threshold=0.000001
    --output-directory=%(out_dir)s
    --log=%(outfile)s.log
    %(infile)s
    '''

    P.run()
    P.touch(outfile)


@follows(splitGwasRegions,
         splitConditionalRegions,
         mkdir("locuszoom.dir"))
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/(.+)_(.+)_significant.tsv"),
           r"locuszoom.dir/\1_\2.metal")
def transformMetalForLocuszoom(infile, outfile):
    '''
    Transform regional results file into METAL format
    for plotting in LocusZoom
    '''

    statement = '''
    cat %(infile)s | cut -f 6,7 |
    awk 'BEGIN {printf("MarkerName\\tP\\n")}
    {if(NR > 1)
    {printf("%%s\\t%%s\\n", $2, $1)}}' |
    awk '{if($2 == 0) {printf("%%s\\t2.25074E-308\\n", $1)}
    else {print $0}}'
    > %(outfile)s
    '''

    P.run()


@follows(transformMetalForLocuszoom)
@transform("locuszoom.dir/*.metal",
           regex("locuszoom.dir/(.+)_(.+)_(.+).metal"),
           r"locuszoom.dir/\1_\2_\3.pdf")
def plotLocusZoom(infile, outfile):
    '''
    Generate high-resolution Manhattan plots
    with locuszoom.

    `locuszoom` must be in your PATH variable
    '''

    components = infile.split("/")[-1].split("_")
    snp = components[2].split(".")[0]
    start = components[1]
    chrome = components[0]
    outpattern = "_".join([chrome, start])

    # need the absolute path for locuszoom to read
    # the input file
    inpath = os.path.abspath(infile)

    job_memory = "4G"

    statement = '''
    cd  locuszoom.dir/; checkpoint;
    locuszoom
    --metal %(inpath)s
    --pvalcol P
    --build hg19
    --source 1000G_March2012
    --pop EUR
    --plotonly
    --no-date
    --flank 1500kb
    --refsnp %(snp)s
    --prefix %(outpattern)s
    > %(outpattern)s.log;
    rm -rf ld_cache.db;
    cd ../;
    '''

    P.run()


@follows(splitGwasRegions,
         splitConditionalRegions,
         mkdir("snpsets.dir"))
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/(.+)_(.+)_significant.tsv"),
           r"snpsets.dir/\1_\2.snpset")
def makeSnpSets(infile, outfile):
    '''
    Write file of SNP Ids for downstream task - use
    results files
    '''

    statement = '''
    cat %(infile)s | cut -f 7 > %(outfile)s
    '''

    P.run()


@follows(makeSnpSets,
         mkdir("scores.dir"))
@transform(makeSnpSets,
           regex("snpsets.dir/chr(\d+)_(\d+)_(.+).snpset"),
           add_inputs(r"%s/chr\1.bim" % PARAMS['functional_bim_dir']),
           r"scores.dir/chr\1_\2_\3_scores.tsv")
def getSnpFunctionalScores(infiles, outfile):
    '''
    Retrieve functional scores associated with
    each SNP - used to calculate prior
    probabilities for SNP prioritisation methods
    '''

    snp_set = infiles[0]
    bim_file = infiles[1]

    job_memory = "2G"
    statement = '''
    cgat snpPriority
    --score-method=get_eigen
    --eigen-score-directory=%(functional_score_dir)s
    --snp-set=%(snp_set)s
    --log=%(outfile)s.log
    %(bim_file)s
    > %(outfile)s
    '''

    P.run()


@follows(splitGwasRegions,
         splitConditionalRegions,
         mkdir("candidate_snps.dir"))
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/(.+)_significant.tsv"),
           r"candidate_snps.dir/\1_PICS.tsv")
def calcPicsScores(infiles, outfile):
    '''
    Calculate the probabilisitc inference of causal SNPs
    score for all association signals

    Output SNPs which explain ~99% of posterior probability
    of P(B^causal | A^lead)
    '''

    snp_file = infiles
    chrome = snp_file.split("/")[-1].split("_")[0]
    chrome = chrome.lstrip("chr")

    statement = '''
    cgat snpPriority
    --score-method=PICS
    --chromosome=%(chrome)s
    --distribution=normal
    --flat-prior
    --ld-dir=%(ld_dir)s
    --ld-threshold=0.5
    --log=%(outfile)s.log
    %(snp_file)s
    > %(outfile)s
    '''

    P.run()


@follows(calcPicsScores,
         mkdir("credible_sets.dir"))
@transform(calcPicsScores,
           regex("candidate_snps.dir/(.+)_PICS.tsv"),
           r"credible_sets.dir/\1_PICS.tsv")
def makePicsCredibleSet(infile, outfile):
    '''
    Create an 80% credible set from the PICS
    posterior probabilities
    '''

    job_memory = "1G"

    statement = '''
    cgat snpPriority
    --score-method=credible_set
    --credible-interval=0.95
    --lead-snp-id=2
    --filename-separator="_"
    --snp-column=0
    --probability-column=1
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(makePicsCredibleSet)
@collate(makePicsCredibleSet,
         regex("credible_sets.dir/(.+)_PICS.tsv"),
         r"credible_sets.dir/PICS.table")
def summarisePicsResults(infiles, outfile):
    '''
    Summarise and tabulate the results from
    PICS prioritisation
    '''

    infiles = ",".join(infiles)

    statement = '''
    cgat snpPriority
    --score-method=summarise
    --log=%(outfile)s.log
    %(infiles)s
    > %(outfile)s
    '''

    P.run()


###########################
# Take this out for the moment - it isn't mature yet

# @follows(splitGwasRegions,
#          splitConditionalRegions,
#          calcPicsScores)
# @transform("hit_regions.dir/*_significant.tsv",
#            regex("hit_regions.dir/(.+)_significant.tsv"),
#            r"candidate_snps.dir/\1_LDscore.tsv")
# def calcLdScores(infile, outfile):
#     '''
#     Calculate the SE weighted effects * LDscore for all
#     association signals.

#     Select top 1% ranked variants
#     '''

#     table = outfile.split("/")[-1].split("_")[0]
#     chrome = table.strip("chr")

#     statement = '''
#     python /ifs/devel/projects/proj045/gwas_pipeline/snpPriority.py
#     --score-method=LDscore
#     --ld-dir=%(ld_dir)s
#     --ld-threshold=0.5
#     --chromosome=%(chrome)s
#     --log=%(outfile)s.log
#     %(infile)s
#     > %(outfile)s
#     '''

#     P.run()


@follows(splitGwasRegions,
         splitConditionalRegions,
         calcPicsScores)
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/(.+)_significant.tsv"),
           r"candidate_snps.dir/\1_LDranks-1pc.tsv")
def calcTop1pcLdRanks(infile, outfile):
    '''
    Take the top 1% SNPs in LD with the index
    SNP with r2 > 0.8
    '''

    table = outfile.split("/")[-1].split("_")[0]
    chrome = table.strip("chr")

    statement = '''
    cgat snpPriority
    --score-method=R2_rank
    --ld-dir=%(ld_dir)s
    --chromosome=%(chrome)s
    --rank-threshold=0.01
    --ld-threshold=0.8
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(splitGwasRegions,
         splitConditionalRegions,
         calcPicsScores,
         calcTop1pcLdRanks)
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/(.+)_significant.tsv"),
           r"candidate_snps.dir/\1_ABF.tsv")
def calcApproxBayesFactorScore(infile, outfile):
    '''
    Calculate the approximate Bayes Factor for fine-mapped
    region SNPs - returns the
    '''

    table = outfile.split("/")[-1].split("_")[0]
    chrome = table.strip("chr")

    statement = '''
    cgat snpPriority
    --score-method=ABF
    --chromosome=%(chrome)s
    --flat-prior
    --prior-variance=0.04
    --fine-map-window=1500000
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(calcApproxBayesFactorScore,
         mkdir("credible_sets.dir"))
@transform(calcApproxBayesFactorScore,
           regex("candidate_snps.dir/(.+)_ABF.tsv"),
           r"credible_sets.dir/\1_ABF.tsv")
def makeAbfCredibleSet(infile, outfile):
    '''
    Create an 95% credible set from the ABF
    posterior probabilities
    '''

    job_memory = "1G"

    statement = '''
    cgat snpPriority
    --score-method=credible_set
    --credible-interval=0.95
    --lead-snp-id=2
    --filename-separator="_"
    --snp-column=0
    --probability-column=2
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(makeAbfCredibleSet,
         summarisePicsResults)
@collate(makeAbfCredibleSet,
         regex("credible_sets.dir/(.+)_ABF.tsv"),
         r"credible_sets.dir/ABF.table")
def summariseAbfResults(infiles, outfile):
    '''
    Summarise and tabulate the results from
    approximate Bayes factor prioritisation
    '''

    infiles = ",".join(infiles)

    statement = '''
    cgat snpPriority
    --score-method=summarise
    --log=%(outfile)s.log
    %(infiles)s
    > %(outfile)s
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Perform colocalization testing between association summary statistics and additional
# summary statistics from an eQTL analysis e.g. GTex
# inputs needed: gwas summary, MAF file, trait summary
@follows(splitConditionalRegions,
         mkdir("coloc.dir"))
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/chr([0-9]+)_(\d+)_(\S+)_significant.tsv"),
           add_inputs(r"%s/chr\1_genes.tsv" % PARAMS['eqtl_dir']),
           r"coloc.dir/chr\1_\2_genes.tsv")
def selectRegionGenes(infiles, outfile):
    '''
    Select the gene symbols that lie within
    each association region to be tested for
    colocalisation.

    The input gene file needs to contain start and end
    co-ordinates.

    Genes are selected who's TSS and TSE lie within the
    boundary of the fine mapping interval, generally
    1.5Mb +/- lead SNP position.  The gene file should be
    in BED4 format, with column 4 containing gene symbols
    '''

    resfile = infiles[0]
    gene_file = infiles[1]
    start = resfile.split("/")[-1].split("_")[1]

    statement = '''
    cat %(gene_file)s |
    awk '{if(($2 >= %(start)s - 1500000) && ($3 <= %(start)s + 1500000))
    {print $0}}' | cut -f 4 | sort | uniq > %(outfile)s
    '''

    P.run()


# trait summary statistic results files need to contain
# just the variants in LD with each other, say r^2 >=0.2
# there needs to be balance so that some SNPs are still
# retained for analysis
@follows(selectRegionGenes)
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/chr([0-9]+)_(.+)_(.+)significant.tsv"),
           add_inputs(r"ld.dir/chr\1.ld.bgz"),
           r"coloc.dir/chr\1_\2_\3ldextract.tsv")
def ldExtractResults(infile, outfile):
    '''
    Extract only those variants in LD r^2 >= 0.5
    to make sure only independent association
    signals are used in the colocalisation
    analysis
    '''

    components = infile[0].split("/")[-1].rstrip("_significant.tsv")
    lead_snp = "_".join(components.split("_")[2:])
    chrome = int(components.split("_")[0].strip("chr"))
    ld_file = infile[1]
    snpos = int(components.split("_")[1])
    start = snpos - 1500000
    end = snpos + 1500000
    if start < 1:
        start = 1
    else:
        pass

    statement = '''
    tabix %(ld_file)s %(chrome)i:%(start)i-%(end)i
    | awk '{if($2 == %(snpos)i || $5 == %(snpos)i) {print $0}}'
    | awk '$7 >= %(coloc_ldthresh)s {print $0}'
    | cut -f 3,6 | sed 's/\\t/\\n/' | sort | uniq
    > %(outfile)s
    '''

    P.run()


@follows(selectRegionGenes,
         ldExtractResults)
@transform("hit_regions.dir/*_significant.tsv",
           regex("hit_regions.dir/chr([0-9]+)_(\d+)_(\S+)_significant.tsv"),
           add_inputs([r"%s/chr\1_eQTL.txt.gz" % PARAMS['eqtl_dir'],
                       r"coloc.dir/chr\1_\2_genes.tsv",
                       r"coloc.dir/chr\1_\2_\3_ldextract.tsv",
                       r"%s/chr\1.frq" % PARAMS['coloc_mafdir']]),
           r"coloc.dir/chr\1_\2_\3.coloc")
def colocTesteQTL(infiles, outfile):
    '''
    Perform a colocalisation test between trait summary statistics
    and eQTL result summary statistics
    '''

    trait1_results = infiles[0]
    trait2_results = infiles[1][0]
    gene_list = infiles[1][1]
    ld_extract = infiles[1][2]
    maf_table = infiles[1][3]

    job_memory = "8G"

    statement = '''
    cgat assoc2coloc
    --trait1-results=%(trait1_results)s
    --trait2-results=%(trait2_results)s
    --trait2-p-column=%(eqtl_pcol)s
    --trait1-prevalence=%(coloc_prevalence)s
    --trait2-size=%(eqtl_nsize)i
    --trait1-snplist=%(ld_extract)s
    --R-script=%(r_scripts)s
    --gene-list=%(gene_list)s
    --maf-table=%(maf_table)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Variance components analysis on GWAS hit regions and observed epistatic interactions
# Detect genetic overlap with additional traits by estimating h2 additive from GWAS and
# epistasis regions.
# get the top SNPs first


@follows(mergeGwasHits,
         plotGenomeManhattan,
         splitGwasRegions,
         mkdir("reml.dir"))
@collate("hit_regions.dir/*significant.tsv",
         regex("hit_regions.dir/(.+)_(.+)_(.+)_significant.tsv"),
         r"reml.dir/GwasHits.snps")
def getGwasTopHits(infiles, outfile):
    '''
    Take the top SNPs with association p-values
    below a given threshold.
    '''

    job_memory = "10G"
    infiles = " ".join(infiles)

    statement = '''
    cat %(infiles)s | grep -v "SNP" | cut -f 7
    > %(outfile)s
    '''

    P.run()


@follows(getGrmRegion)
@transform("epistasis.dir/*.fam",
           regex("epistasis.dir/(.+).fam"),
           add_inputs("%s" % PARAMS['gwas_keep']),
           r"reml.dir/\1.keep")
def subSampleIndividualsForReml(infiles, outfile):
    '''
    Subsample from the total population
    to get ~n individuals for linear
    model analysis.
    Output with combined list of ethnically
    selected individuals
    '''

    fam_file = infiles[0]
    keep_file = infiles[1]
    keep_temp = P.getTempFilename(shared=True)
    fam_temp = P.getTempFilename(shared=True)

    statement = '''
    cat %(keep_file)s | tr -s " " "\\t" | cut -f1,2 | sort | grep -v "FID" > %(keep_temp)s;
    cat %(fam_file)s | tr " " "\\t" | cut -f1,2 | sort > %(fam_temp)s;
    comm -12 %(fam_temp)s %(keep_temp)s | shuf -n %(mlm_subsample)s
    > %(outfile)s;
    rm -rf %(keep_temp)s %(fam_temp)s
    '''

    P.run()


@follows(mergeGwasHits,
         getGwasTopHits,
         subSampleIndividualsForReml,
         mkdir("reml.dir"))
@transform(mergeGwasHits,
           regex("epistasis.dir/(.+).bed"),
           add_inputs([r"epistasis.dir/\1.fam",
                       r"epistasis.dir/\1.bim",
                       subSampleIndividualsForReml,
                       getGwasTopHits]),
           r"reml.dir/REML_Gwas.bed")
def subsetCohortForReml(infiles, outfile):
    '''
    Subset the data prior to GRM calculation
    and REML estimation of variance components
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    keep_file = infiles[1][2]
    snp_file = infiles[1][3]
    job_memory = "40G"

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --program=plink2
    --input-file-format=plink_binary
    --method=format
    --keep-individuals=%(keep_file)s
    --extract-snps=%(snp_file)s
    --min-allele-frequency=0.001
    --format-method=change_format
    --format-parameter=%(format_gender)s
    --update-sample-attribute=gender
    --reformat-type=plink_binary
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(subsetCohortForReml)
@transform(subsetCohortForReml,
           regex("reml.dir/(.+).bed"),
           add_inputs([r"reml.dir/\1.fam",
                       r"reml.dir/\1.bim"]),
           r"reml.dir/\1.grm.N.bin")
def makeRemlGrm(infiles, outfile):
    '''
    Generate the GRM across GWAS regions for
    variance components analysis
    '''

    job_threads = PARAMS['grm_threads']
    # memory per thread
    job_memory = "10G"

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-3])

    statement = '''
    cgat geno2assoc
    --program=gcta
    --threads=%(job_threads)s
    --input-file-format=plink_binary
    --method=matrix
    --matrix-compression=bin
    --matrix-form=grm
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.gcta.log
    '''

    P.run()


@follows(makeRemlGrm)
@transform("reml.dir/*.fam",
           regex("reml.dir/(.+).fam"),
           r"reml.dir/\1.pheno")
def subsetRemlPhenotype(infile, outfile):
    '''
    Generate a phenotype file from the subset
    .fam file
    '''

    job_memory = "1G"

    statement = '''
    cat %(infile)s | tr " " "\\t" | cut -f1,2,6
    > %(outfile)s
    '''

    P.run()


@follows(makeRemlGrm,
         subsetRemlPhenotype)
@transform(makeRemlGrm,
           regex("reml.dir/(.+).grm.N.bin"),
           add_inputs([r"reml.dir/\1.grm.bin",
                       r"reml.dir/\1.grm.id",
                       r"reml.dir/\1.pheno"]),
           r"reml.dir/\1.hsq")
def calcHeritabilityReml(infiles, outfile):
    '''
    Use REML to perform variance components analysis
    and estimate genetic contribution to trait
    of interest
    '''

    n_file = infiles[0]
    bin_file = infiles[1][0]
    id_file = infiles[1][1]
    grm_files = ",".join([n_file, bin_file, id_file])

    pheno_file = infiles[1][2]

    job_threads = PARAMS['grm_threads']
    job_memory = "10G"
    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --input-file-format=GRM_binary
    --program=gcta
    --phenotypes-file=%(pheno_file)s
    --pheno=1
    --covariates-file=%(mlm_cont_covarfile)s
    --discrete-covariates-file=%(mlm_discrete_covarfile)s
    --method=reml
    --reml-method=BLUP_EBV
    --prevalence=%(reml_prevalence)f
    --threads=%(job_threads)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(grm_files)s
    > %(outfile)s.gcta.log
    '''

    P.run()


@follows(calcHeritabilityReml)
@transform("reml.dir/*.indi.blp",
           regex("reml.dir/(.+).indi.blp"),
           add_inputs([r"reml.dir/\1.bed",
                       r"reml.dir/\1.fam",
                       r"reml.dir/\1.bim"]),
           r"reml.dir/\1.snp.blp")
def snpBlup(infiles, outfile):
    '''
    SNP BLUP <- phenotypic variance explained
    by each SNP that went into the GRM
    '''

    blup_file = infiles[0]
    bed_file = infiles[1][0]
    fam_file = infiles[1][1]
    bim_file = infiles[1][2]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-2])

    job_memory = "20G"

    statement = '''
    cgat geno2assoc
    --program=gcta
    --input-file-format=plink_binary
    --method=reml
    --reml-method=snpBLUP
    --reml-parameters=%(blup_file)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.gcta.log
    '''

    P.run()


@follows(makeRemlGrm)
@transform("reml.dir/*.fam",
           regex("reml.dir/(.+).fam"),
           add_inputs("%s" % PARAMS['reml_phenos']),
           r"reml.dir/\1_All.pheno")
def subsetAllPhenotypes(infile, outfile):
    '''
    Subset the phenotypes file based
    on the Plink .fam file
    '''

    job_memory = "5G"

    fam_file = infiles[0]
    pheno_file = infiles[1]

    statement = '''
    cgat pheno2pheno
    --task=subset_phenotypes
    --fam-file=%(fam_file)s
    --log=%(outfile)s.log
    %(pheno_file)s
    > %(outfile)s
    '''

    P.run()


@follows(makeRemlGrm,
         subsetAllPhenotypes)
@transform(makeRemlGrm,
           regex("reml.dir/(.+).grm.N.bin"),
           add_inputs([r"reml.dir/\1.grm.bin",
                       r"reml.dir/\1.grm.id",
                       "%s" % PARAMS['reml_phenos']]),
           r"reml.dir/\1.hsq")
def calcBivariateReml(infiles, outfile):
    '''
    Use REML to perform variance components analysis
    and estimate genetic contribution to two
    traits of interest
    '''

    n_file = infiles[0]
    bin_file = infiles[1][0]
    id_file = infiles[1][1]
    grm_files = ",".join([n_file, bin_file, id_file])

    pheno_file = infiles[1][2]

    job_threads = PARAMS['grm_threads']
    job_memory = "10G"
    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    cgat geno2assoc
    --input-file-format=GRM_binary
    --program=gcta
    --phenotypes-file=%(pheno_file)s
    --pheno=1
    --covariates-file=%(mlm_cont_covarfile)s
    --discrete-covariates-file=%(mlm_discrete_covarfile)s
    --method=reml
    --reml-method=BLUP_EBV
    --prevalence=0.05
    --threads=%(job_threads)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(grm_files)s
    > %(outfile)s.gcta.log
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
############################
# Joint analysis of traits #
############################
# Need to finish this or put it in a separate pipeline
@follows(plotGenomeManhattan,
         mkdir("joint_analysis.dir"))
@transform("gwas.dir/*.results",
           regex("gwas.dir/(.+)_adj.results"),
           r"joint_analysis.dir/\1.cojo")
def transformGwasToCojo(infile, outfile):
    '''
    Process output from Plink GWAS into the required format
    for a conditional and joint analysis using GCTA
    '''

    job_memory = "75G"
    job_threads = 1

    statement = '''
    cgat assoc2assoc
    --task=merge_freq
    --frequency-directory=%(joint_freq_dir)s
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#

# DISSECT testing tasks


@follows(mkdir("dissect.dir"),
         mkdir("grm.dir"))
@transform("data.dir/subsample_small.bed",
           regex("data.dir/(.+)_small.bed"),
           r"grm.dir/\1.dat")
def test_dissectGrmSingleFile(infiles, outfile):
    '''
    Test DISSECT for parallelised analysis
    Make a GRM, then do PCA
    MUST have a separate phenotypes file with no header, but in plink format
    for GWAS
    '''

    # memory errors with 3G - bump it up to 12G
    # using more processes may also relieve some of the
    # MEMORY ERRORS WITH 12G!!! WTF?? - more processes?? > 300?
    # memory requirements - try it with 192
    # if memory requirements are too high, jobs fail
    # increase the number of processes to spread the job out
    job_memory = "8G"
    infiles = ",".join(infiles)
    job_threads = 128
    # job_queue = "mpi.q"

    job_queue = ",".join(["all.q@cgat001", "all.q@cgat002", "all.q@cgat003", "all.q@cgat004",
                          "all.q@cgat005", "all.q@cgat006", "all.q@cgat007", "all.q@cgat008",
                          "all.q@cgat009", "all.q@cgat010", "all.q@cgat011", "all.q@cgat012",
                          "all.q@cgat013", "all.q@cgat014", "all.q@cgat101", "all.q@cgat102",
                          "all.q@cgat103", "all.q@cgat104", "all.q@cgat105", "all.q@cgat106",
                          "all.q@cgat107", "all.q@cgat108", "all.q@cgat109", "all.q@cgat110",
                          "all.q@cgat111", "all.q@cgat112", "all.q@cgat113", "all.q@cgat114",
                          "all.q@cgat115", "all.q@cgat116", "all.q@cgatgpu1", "all.q@cgatsmp1"])

    cluster_parallel_environment = " mpi "
    statement = '''
    mpirun
    dissect
    --bfile data.dir/subsample
    --make-grm
    --out grm.dir/subsample
    > dissect_test.log
    '''

    P.run()


@follows(mkdir("dissect.dir"),
         mkdir("grm.dir"))
@transform("data.dir/subsample_list.tsv",
           regex("data.dir/(.+)_list.tsv"),
           r"grm.dir/\1.dat")
def test_dissectGrmManyFiles(infile, outfile):
    '''
    Test DISSECT for parallelised analysis
    Make a GRM, then do PCA
    MUST have a separate phenotypes file with no header, but in plink format
    for GWAS
    '''

    # get the sample list from PARAMS
    # for chr10-9 + chr1, estimated memory usage was 4GB per process
    # failing on 4G memory per node
    job_memory = "3G"
    job_threads = 300
    job_queue = "mpi.q"
    cluster_parallel_environment = " mpi "

    out_pattern = ".".join(outfile.split(".")[:-1])
    statement = '''
    mpirun  --np 1
    dissect
    --bfile-list %(infile)s
    --make-grm
    --grm-join-method 0
    --out %(out_pattern)s
    > dissect_test.log
    '''

    P.run()


@follows(test_dissectGrmManyFiles,
         mkdir("pca.dir"))
@transform("grm.dir/*.dat",
           regex("grm.dir/(.+).dat"),
           r"pca.dir/\1.pca.eigenvalues")
def dissectPCA(infile, outfile):
    '''
    Test DISSECT PCA on grms
    '''
    job_memory = "2G"
    infiles = ",".join(infiles)
    job_threads = 128
    job_options = "-l h=!andromeda,h=!gandalf,h=!saruman"
    job_queue = "all.q"
    cluster_parallel_environment = " mpi "
    statement = '''
    mpirun
    dissect
    --pca
    --grm grm.dir/wholegenome
    --out pca.dir/wholegenome
    --num-eval 20
    > dissect_test.log
    '''

    P.run()

# ----------------------------------------------------------------------------------------#
# ----------------------------------------------------------------------------------------#
# Pipeline targets


@follows(runFilteredPCA,
         plotPcaResults,
         excludeInbred,
         excludeRelated,
         plotIbdHistogram,
         mergeExclusions)
def QC():
    pass


@follows(QC,
         mergeCovariates,
         pcAdjustedAssociation,
         plotGenomeManhattan)
def GWAS():
    pass


@follows(QC,
         GWAS,
         getConditionalRegions,
         conditionalAssociation)
def Conditional():
    pass


@follows(Conditional,
         dissectPCA)
def ParallelMLM():
    pass


@follows(QC)
def Evo():
    pass


@follows(QC,
         GWAS,
         Conditional,
         ParallelMLM,
         Evo)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
